<!-- Generated by agents-md: DO NOT EDIT DIRECTLY. Edit *.agents.md fragments instead. Higher priority fragments appear first and win conflicts. -->

<!-- source: documentation/docs/api-reference.md priority=0 -->
{% include 'copy-page-script.md' %}
{% include 'copy-page-button.md' %}

# Amazon Aurora DSQL API Reference

## Overview

In addition to the AWS Console and the AWS Command Line Interface (CLI), Aurora DSQL also provides an API interface. You can use the API operations to manage your resources in Aurora DSQL.

## API Documentation Links

### Actions
For an alphabetical list of API operations, see [Actions](https://docs.aws.amazon.com/aurora-dsql/latest/APIReference/API_Operations.html).

### Data Types
For an alphabetical list of data types, see [Data types](https://docs.aws.amazon.com/aurora-dsql/latest/APIReference/API_Types.html).

### Common Parameters
For a list of common query parameters, see [Common parameters](https://docs.aws.amazon.com/aurora-dsql/latest/APIReference/CommonParameters.html).

### Error Codes
For descriptions of the error codes, see [Common errors](https://docs.aws.amazon.com/aurora-dsql/latest/APIReference/CommonErrors.html).

## CLI Reference

For more information about the AWS CLI, see AWS Command Line Interface reference for Aurora DSQL.

## Related Documentation

- **Authentication Guide**: [Auth & Access Overview](authentication-and-authorization.md)
- **Getting Started**: [Getting Started](guides/getting-started/quickstart.md)
- **Troubleshooting**: [Troubleshooting Overview](troubleshooting.md)
<!-- /source: documentation/docs/api-reference.md -->

<!-- source: documentation/docs/authentication-and-authorization.md priority=0 -->
{% include 'copy-page-script.md' %}
{% include 'copy-page-button.md' %}

# Authentication and Authorization for Amazon Aurora DSQL

## Overview

Aurora DSQL uses IAM roles and policies for cluster authorization. You associate IAM roles with [PostgreSQL database roles](https://www.postgresql.org/docs/current/user-manag.html) for database authorization. This approach combines [benefits from IAM](https://docs.aws.amazon.com/IAM/latest/UserGuide/intro-iam-features.html) with [PostgreSQL privileges](https://www.postgresql.org/docs/current/user-manag.html). Aurora DSQL uses these features to provide a comprehensive authorization and access policy for your cluster, database, and data.

## Managing Your Cluster Using IAM

To manage your cluster, use IAM for authentication and authorization:

### IAM Authentication

To authenticate your IAM identity when you manage Aurora DSQL clusters, you must use IAM. You can provide authentication using:
- [AWS Management Console](https://docs.aws.amazon.com/signin/latest/userguide/how-to-sign-in.html)
- [AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html)
- [AWS SDK](https://docs.aws.amazon.com/sdkref/latest/guide/access.html)

### IAM Authorization

To manage Aurora DSQL clusters, grant authorization using IAM actions for Aurora DSQL. For example, to describe a cluster, make sure that your IAM identity has permissions for the IAM action `dsql:GetCluster`, as in the following sample policy action:

```json
{
  "Effect": "Allow",
  "Action": "dsql:GetCluster",
  "Resource": "arn:aws:dsql:us-east-1:123456789012:cluster/my-cluster"
}
```

## Connecting to Your Cluster Using IAM

To connect to your cluster, use IAM for authentication and authorization:

### IAM Authentication

Generate a temporary authentication token using an IAM identity with authorization to connect to your cluster. To learn more, see [Generating an authentication token in Amazon Aurora DSQL](https://docs.aws.amazon.com/aurora-dsql/latest/userguide/SECTION_authentication-token.html).

### IAM Authorization

Grant the following IAM policy actions to the IAM identity you're using to establish the connection to your cluster's endpoint:

#### Admin Role Connection

Use `dsql:DbConnectAdmin` if you're using the `admin` role. Aurora DSQL creates and manages this role for you. The following sample IAM policy action permits `admin` to connect to `my-cluster`:

```json
{
  "Effect": "Allow",
  "Action": "dsql:DbConnectAdmin",
  "Resource": "arn:aws:dsql:us-east-1:123456789012:cluster/my-cluster"
}
```

#### Custom Database Role Connection

Use `dsql:DbConnect` if you're using a custom database role. You create and manage this role by using SQL commands in your database. The following sample IAM policy action permits a custom database role to connect to `my-cluster` for up to one hour:

```json
{
  "Effect": "Allow",
  "Action": "dsql:DbConnect",
  "Resource": "arn:aws:dsql:us-east-1:123456789012:cluster/my-cluster"
}
```

**Important**: After you establish a connection, your role is authorized for up to one hour for the connection.

## PostgreSQL Database Roles and IAM Roles

### Database Role Management

PostgreSQL manages database access permissions using the concept of roles. A role can be thought of as either a database user, or a group of database users, depending on how the role is set up. You create PostgreSQL roles using SQL commands. To manage database-level authorization, grant PostgreSQL permissions to your PostgreSQL database roles.

Aurora DSQL supports two types of database roles:
- **Admin role**: Aurora DSQL automatically creates a predefined `admin` role for you in your Aurora DSQL cluster. You can't modify the `admin` role.
- **Custom roles**: When you connect to your database as `admin`, you can issue SQL to create new database-level roles to associate with your IAM roles.

To let IAM roles connect to your database, associate your custom database roles with your IAM roles.

### Authentication Process

Use the `admin` role to connect to your cluster. After you connect your database, use the command `AWS IAM GRANT` to associate a custom database role with the IAM identity authorized to connect to the cluster:

```sql
AWS IAM GRANT custom-db-role TO 'arn:aws:iam::account-id:role/iam-role-name';
```

To learn more, see [Authorizing database roles to connect to your cluster](https://docs.aws.amazon.com/aurora-dsql/latest/userguide/using-database-and-iam-roles.html#using-database-and-iam-roles-custom-database-roles).

### Authorization Process

Use the `admin` role to connect to your cluster. Run SQL commands to set up custom database roles and grant permissions. To learn more, see:
- [PostgreSQL database roles](https://www.postgresql.org/docs/current/user-manag.html)
- [PostgreSQL privileges](https://www.postgresql.org/docs/current/ddl-priv.html)

## Using IAM Policy Actions with Aurora DSQL

The IAM policy action you use depends on the role you use to connect to your cluster: either `admin` or a custom database role. The policy also depends on the IAM actions required for this role.

### Using IAM Policy Actions to Connect to Clusters

#### Admin Role Connection
When you connect to your cluster with the default database role of `admin`, use an IAM identity with authorization to perform the following IAM policy action:

```
"dsql:DbConnectAdmin"
```

#### Custom Database Role Connection
When you connect to your cluster with a custom database role, first associate the IAM role with the database role. The IAM identity you use to connect to your cluster must have authorization to perform the following IAM policy action:

```
"dsql:DbConnect"
```

To learn more about custom database roles, see [Using database roles and IAM authentication](https://docs.aws.amazon.com/aurora-dsql/latest/userguide/using-database-and-iam-roles.html).

### Using IAM Policy Actions to Manage Clusters

When managing your Aurora DSQL clusters, specify policy actions only for the actions that your role needs to perform. For example, if your role only needs to get cluster information, you might limit role permissions to only the `GetCluster` and `ListClusters` permissions, as in the following sample policy:

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "dsql:GetCluster",
        "dsql:ListClusters"
      ],
      "Resource": "arn:aws:dsql:us-east-1:123456789012:cluster/my-cluster"
    }
  ]
}
```

#### All Available IAM Policy Actions for Managing Clusters

The following example policy shows all available IAM policy actions for managing clusters:

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "dsql:CreateCluster",
        "dsql:GetCluster",
        "dsql:UpdateCluster",
        "dsql:DeleteCluster",
        "dsql:ListClusters",
        "dsql:TagResource",
        "dsql:ListTagsForResource",
        "dsql:UntagResource"
      ],
      "Resource": "*"
    }
  ]
}
```

## Revoking Authorization Using IAM and PostgreSQL

You can revoke permissions for your IAM roles to access your database-level roles:

### Revoking Admin Authorization to Connect to Clusters

To revoke authorization to connect to your cluster with the `admin` role:
1. Revoke the IAM identity's access to `dsql:DbConnectAdmin`
2. Either edit the IAM policy or detach the policy from the identity

After revoking connection authorization from the IAM identity, Aurora DSQL rejects all new connection attempts from that IAM identity. Any active connections that use the IAM identity might stay authorized for the duration of the connection. For more information on connection durations, see [Quotas and limits](https://docs.aws.amazon.com/aurora-dsql/latest/userguide/CHAP_quotas.html).

### Revoking Custom Role Authorization to Connect to Clusters

To revoke access to database roles other than `admin`:
1. Revoke the IAM identity's access to `dsql:DbConnect`
2. Either edit the IAM policy or detach the policy from the identity

You can also remove the association between the database role and IAM by using the command `AWS IAM REVOKE` in your database. To learn more about revoking access from database roles, see [Revoking database authorization from an IAM role](https://docs.aws.amazon.com/aurora-dsql/latest/userguide/using-database-and-iam-roles.html#using-database-and-iam-roles-revoke).

### Important Notes on Permission Management

- You can't manage permissions of the predefined `admin` database role
- To learn how to manage permissions for custom database roles, see [PostgreSQL privileges](https://www.postgresql.org/docs/current/ddl-priv.html)
- Modifications to privileges take effect on the next transaction after Aurora DSQL successfully commits the modification transaction
<!-- /source: documentation/docs/authentication-and-authorization.md -->

<!-- source: documentation/docs/backup-and-restore.md priority=0 -->
{% include 'copy-page-script.md' %}
{% include 'copy-page-button.md' %}

# Backup and restore for Amazon Aurora DSQL

Amazon Aurora DSQL helps you meet your regulatory compliance and business continuity requirements through integration with AWS Backup, a fully managed data protection service that makes it easy to centralize and automate backups across AWS services, in the cloud, and on premises. The service streamlines backup creation, management, and restoration for both single-Region and multi-Region Aurora DSQL clusters.

Key features include the following:

- Centralized backup management through the AWS Management Console, SDK, or AWS CLI
- Full cluster backups
- Automated backup schedules and retention policies
- Cross-Region and cross-account capabilities
- WORM (write-once, read-many) configuration for all the backups you store

For more information on the features of AWS Backup Vault Lock and an extensive list of available AWS Backup features for Aurora DSQL, see [Vault lock benefits](https://docs.aws.amazon.com/aws-backup/latest/devguide/vault-lock.html#backup-vault-lock-benefits) and [AWS Backup feature availability](https://docs.aws.amazon.com/aws-backup/latest/devguide/backup-feature-availability.html) in the _AWS Backup Developer Guide_.

## Getting started with AWS Backup

AWS Backup creates complete copies of your Aurora DSQL clusters. You can get started using AWS Backup for Aurora DSQL by following the steps in [Getting started with AWS Backup](https://docs.aws.amazon.com/aws-backup/latest/devguide/getting-started.html):

1. Create on-demand backups for immediate protection.
2. Establish backup plans for automated, scheduled backups.
3. Configure retention periods and cross-Region copying.
4. Set up monitoring and notifications for backup activities.

## Restoring your backups

When you restore Aurora DSQL clusters, AWS Backup always creates new clusters to preserve your source data.

### Restoring single-Region clusters

To restore an Aurora DSQL single-Region cluster, use the [AWS Backup console](https://console.aws.amazon.com/backup) or CLI to select the recovery point (backup) you wish to restore. Configure the settings for the new cluster that will be created from your backup. For detailed instructions, see [Restore a single-Region Aurora DSQL cluster](https://docs.aws.amazon.com/aws-backup/latest/devguide/restore-auroradsql.html#restore-auroradsql-singleregion).

### Restoring multi-Region clusters

Restoring an Aurora DSQL multi-Region cluster is supported through both the [AWS Backup console](https://console.aws.amazon.com/backup) and the AWS CLI. For detailed instructions, see [Restore a multi-Region Aurora DSQL cluster](https://docs.aws.amazon.com/aws-backup/latest/devguide/restore-auroradsql.html#restore-auroradsql-multiregion).

To restore to a multi-Region Aurora DSQL cluster, you can use a backup taken in a single AWS Region. However, before you initiate the restore process, you must ensure there is an identical copy of your backup in all AWS Regions for your multi-Region clusters. If you don't yet have those copies, you must first copy the backup to another AWS Region that supports multi-Region clusters.

We recommend creating backup copies in key AWS Regions to enable robust disaster recovery options and meet compliance requirements. To view available AWS Regions for Aurora DSQL, see [Region Availability for Amazon Aurora DSQL](what-is-amazon-aurora-dsql.md#region-availability-for-amazon-aurora-dsql).

For detailed instructions on these steps, see [Amazon Aurora DSQL restore](https://docs.aws.amazon.com/aws-backup/latest/devguide/restore-auroradsql.html) documentation.

## Monitoring and compliance

AWS Backup provides comprehensive visibility into backup and restore operations with the following resources.

- A centralized dashboard for tracking backup and restore jobs
- Integration with CloudWatch and CloudTrail.
- [AWS Backup Audit Manager](https://docs.aws.amazon.com/aws-backup/latest/devguide/aws-backup-audit-manager.html) for compliance reporting and auditing.

See [Logging Aurora DSQL operations using AWS CloudTrail](https://docs.aws.amazon.com/aurora-dsql/latest/userguide/logging-using-cloudtrail.html) to learn more about logging records of actions taken by a user, role, or an AWS service while using Aurora DSQL.

## Additional resources

To learn more about AWS Backup features and and using it in tandem with Aurora DSQL, see the following resources:

- [Managed policies for AWS Backup](https://docs.aws.amazon.com/aws-backup/latest/devguide/security-iam-awsmanpol.html#AWSBackupOperatorAccess)
- [Amazon Aurora DSQL restore](https://docs.aws.amazon.com/aws-backup/latest/devguide/restore-auroradsql.html)
- [Supported services by AWS Region](https://docs.aws.amazon.com/aws-backup/latest/devguide/backup-feature-availability.html#supported-services-by-region)
- [Encryption for backups in AWS Backup](https://docs.aws.amazon.com/aws-backup/latest/devguide/encryption.html)

By using AWS Backup for Aurora DSQL, you implement a robust, compliant, and automated backup strategy that protects your critical database resources while minimizing administrative overhead. Whether you manage a single cluster or a complex multi-Region deployment, AWS Backup provides the tools you need to ensure your data remains secure and recoverable.
<!-- /source: documentation/docs/backup-and-restore.md -->

<!-- source: documentation/docs/connectors-overview.md priority=0 -->
{% include 'copy-page-script.md' %}
{% include 'copy-page-button.md' %}

# Connectors for Amazon Aurora DSQL

## Overview

Aurora DSQL provides specialized connectors that extend existing database drivers to enable seamless IAM authentication and integration with AWS services. These connectors are designed to work with popular programming languages and frameworks while maintaining compatibility with existing PostgreSQL workflows.

## Available Connectors

### Java JDBC Connector

**Purpose**: Extends PostgreSQL JDBC driver functionality for Aurora DSQL

**Key Features**:
- Seamless IAM authentication integration
- Automatic token generation and refresh
- Compatible with existing JDBC workflows
- Built on top of PostgreSQL JDBC driver

**Use Cases**:
- Java applications using JDBC
- Enterprise Java applications
- Spring Boot applications
- Hibernate ORM integration

**Repository**: [Aurora DSQL JDBC Connector](https://github.com/awslabs/aurora-dsql-jdbc-connector)

### Python Connector

**Purpose**: Extends Python PostgreSQL drivers for Aurora DSQL

**Key Features**:
- Works with Psycopg, Psycopg2, and asyncpg
- Automatic IAM token handling
- Seamless integration with existing Python workflows
- Support for both synchronous and asynchronous operations

**Use Cases**:
- Python applications using PostgreSQL drivers
- Django web applications
- SQLAlchemy ORM integration
- Data science and analytics applications

**Supported Libraries**:
- Psycopg (latest version)
- Psycopg2 (legacy support)
- asyncpg (asynchronous operations)

**Repository**: [Aurora DSQL Python Connector](https://github.com/awslabs/aurora-dsql-python-connector)

### Node.js Connectors

**Purpose**: Extends Node.js PostgreSQL drivers for Aurora DSQL

**Key Features**:
- Support for node-postgres and Postgres.js
- Automatic IAM authentication handling
- Compatible with existing Node.js PostgreSQL workflows
- TypeScript support available

**Use Cases**:
- Node.js web applications
- Express.js applications
- TypeScript applications
- Serverless functions (Lambda)

**Supported Libraries**:
- node-postgres (pg)
- Postgres.js

**Repository**: [Aurora DSQL Node.js Connectors](https://github.com/awslabs/aurora-dsql-nodejs-connectors)

## Connector Benefits

### Simplified Authentication
- **Automatic token generation**: Connectors handle IAM token creation and refresh
- **Seamless integration**: No changes required to existing application code
- **Security best practices**: Built-in support for AWS security standards

### Framework Compatibility
- **Existing workflows**: Maintain compatibility with current PostgreSQL applications
- **ORM support**: Works with popular ORMs like Hibernate, SQLAlchemy, Django
- **Migration friendly**: Easy transition from standard PostgreSQL to Aurora DSQL

### AWS Integration
- **IAM authentication**: Native support for AWS IAM roles and policies
- **AWS SDK integration**: Leverages existing AWS SDK configurations
- **Service integration**: Designed for AWS service ecosystem

## Getting Started with Connectors

### Installation Process
1. **Install connector package** for your programming language
2. **Configure AWS credentials** (IAM roles, access keys, or profiles)
3. **Update connection strings** to use Aurora DSQL endpoints
4. **Test connectivity** with your Aurora DSQL cluster

### Configuration Requirements
- **AWS credentials**: Valid IAM credentials with Aurora DSQL permissions
- **Cluster endpoint**: Aurora DSQL cluster endpoint URL
- **Database role**: Admin role or custom database role
- **SSL configuration**: SSL/TLS encryption enabled

### Best Practices
- **Use IAM roles** when possible for enhanced security
- **Configure connection pooling** appropriately for your workload
- **Handle token expiration** gracefully in your applications
- **Monitor connection health** and implement retry logic

## Future Connector Releases

Additional connectors are planned for future releases. For the latest information on connector availability, see the [Aurora DSQL samples repository](https://github.com/aws-samples/aurora-dsql-samples).

### Planned Languages and Frameworks
- Additional ORM integrations
- More programming language support
- Enhanced framework-specific features
- Improved performance optimizations

## Related Documentation

- **Database Drivers**: [Programming with DSQL Overview](programming-with-dsql.md#database-drivers)
- **Authentication**: [Generate Authentication Token](generate-authentication-token.md)
- **Database Roles**: [Database Roles and IAM Authentication](database-roles-iam-authentication.md)
- **Sample Code**: [Aurora DSQL Samples Repository](https://github.com/aws-samples/aurora-dsql-samples)
- **Getting Started**: [Getting Started](guides/getting-started/quickstart.md)
<!-- /source: documentation/docs/connectors-overview.md -->

<!-- source: documentation/docs/considerations.md priority=0 -->
{% include 'copy-page-script.md' %}
{% include 'copy-page-button.md' %}

# Considerations for Working with Amazon Aurora DSQL

## Overview

Consider the following behaviors when you work with Amazon Aurora DSQL. For more information about PostgreSQL compatibility and support, see the compatibility documentation. For quotas and limits, see the quotas and limits guide.

## Key Considerations

### COUNT(*) Operations on Large Tables

**Behavior**: Aurora DSQL doesn't complete `COUNT(*)` operations before transaction timeout for large tables.

**Recommendation**: To retrieve table row count from the system catalog, use the systems tables and commands available in Aurora DSQL.

**Related Documentation**: [Using systems tables and commands in Aurora DSQL](https://docs.aws.amazon.com/aurora-dsql/latest/userguide/working-with-systems-tables.html)

### Prepared Statements Behavior

**Behavior**: Drivers calling `PG_PREPARED_STATEMENTS` might provide an inconsistent view of cached prepared statements for the cluster.

**Technical Details**: 
- You might see more than the expected number of prepared statements per connection for the same cluster and IAM role
- Aurora DSQL doesn't preserve statement names that you prepare

**Impact**: This affects statement caching behavior but doesn't impact functionality

### Multi-Region Cluster Recovery

**Behavior**: In rare multi-Region linked-cluster impairment scenarios, it might take longer than expected for transaction commit availability to resume.

**Technical Details**:
- Automated cluster recovery operations can result in transient concurrency control or connection errors
- In most cases, you will only see the effects for a percentage of your workload

**Recommendation**: When you see these transient errors, retry your transaction or reconnect with your client.

### SQL Client Schema Display

**Behavior**: Some SQL clients, such as DataGrip, make expansive calls to system metadata to populate schema information.

**Technical Details**:
- Aurora DSQL doesn't support all of this metadata information and returns errors
- This issue doesn't affect SQL query functionality
- It might affect schema display in certain clients

**Impact**: Query functionality remains unaffected, only visual schema display may be impacted

### Admin Role Permissions

**Behavior**: The admin role has a set of permissions related to database management tasks.

**Technical Details**:
- By default, these permissions don't extend to objects that other users create
- The admin role can't grant or revoke permissions on user-created objects to other users
- The admin user can grant itself any other role to get the necessary permissions on these objects

**Recommendation**: Use role-based access control for managing permissions on user-created objects

## General Behavioral Considerations

### Transaction Behavior
- **Timeout Limits**: Large operations may timeout before completion
- **Retry Logic**: Implement retry mechanisms for transient errors
- **Connection Management**: Plan for connection lifecycle and recovery scenarios

### Client Compatibility
- **Version Requirements**: Use supported PostgreSQL client versions
- **Feature Support**: Not all PostgreSQL features are available
- **Error Handling**: Implement proper error handling for unsupported operations

### Performance Considerations
- **Large Table Operations**: Consider alternative approaches for operations on large datasets
- **Index Usage**: Optimize queries to use available indexes effectively
- **Connection Pooling**: Implement appropriate connection pooling strategies

### Security Considerations
- **Role Management**: Understand admin role limitations with user-created objects
- **Permission Inheritance**: Plan role hierarchy for proper access control
- **IAM Integration**: Leverage IAM roles for secure database access

## Best Practices

### Application Design
1. **Implement retry logic** for transient errors during cluster recovery
2. **Use appropriate timeouts** for large table operations
3. **Plan for client compatibility** requirements
4. **Design role hierarchy** to handle admin role limitations

### Operational Practices
1. **Monitor cluster health** and recovery operations
2. **Test client compatibility** before production deployment
3. **Implement proper error handling** for unsupported features
4. **Plan capacity** based on quotas and limits

### Development Practices
1. **Use supported PostgreSQL features** only
2. **Test with realistic data volumes** to understand timeout behavior
3. **Implement proper connection management** in applications
4. **Design for eventual consistency** during recovery scenarios
<!-- /source: documentation/docs/considerations.md -->

<!-- source: documentation/docs/copy-page-button.md priority=0 -->
<div style="text-align: right; margin-bottom: 20px;">
  <button onclick="copyPageContent(event)" style="background: #1976d2; color: white; padding: 8px 16px; border: none; border-radius: 4px; cursor: pointer; font-size: 14px;">
    ðŸ“‹ Copy Page
  </button>
</div>
<!-- /source: documentation/docs/copy-page-button.md -->

<!-- source: documentation/docs/copy-page-script.md priority=0 -->
<script>
function copyPageContent(event) {
  const btn = event.target;
  const content = document.querySelector('.md-content__inner');
  if (!content) {
    alert('Could not find page content');
    return;
  }
  
  const text = content.innerText || content.textContent;
  
  // Try modern clipboard API first
  if (navigator.clipboard && navigator.clipboard.writeText) {
    navigator.clipboard.writeText(text).then(() => {
      const originalText = btn.innerHTML;
      btn.innerHTML = 'âœ“ Copied!';
      btn.style.background = '#4caf50';
      setTimeout(() => {
        btn.innerHTML = originalText;
        btn.style.background = '#1976d2';
      }, 2000);
    }).catch(err => {
      // Fallback to old method
      fallbackCopy(text, btn);
    });
  } else {
    // Use fallback for older browsers
    fallbackCopy(text, btn);
  }
}

function fallbackCopy(text, btn) {
  const textArea = document.createElement('textarea');
  textArea.value = text;
  textArea.style.position = 'fixed';
  textArea.style.left = '-999999px';
  textArea.style.top = '0';
  document.body.appendChild(textArea);
  textArea.focus();
  textArea.select();
  
  try {
    const successful = document.execCommand('copy');
    if (successful) {
      const originalText = btn.innerHTML;
      btn.innerHTML = 'âœ“ Copied!';
      btn.style.background = '#4caf50';
      setTimeout(() => {
        btn.innerHTML = originalText;
        btn.style.background = '#1976d2';
      }, 2000);
    } else {
      alert('Failed to copy content. Please copy manually.');
    }
  } catch (err) {
    alert('Failed to copy content. Please copy manually.');
  } finally {
    document.body.removeChild(textArea);
  }
}
</script>
<!-- /source: documentation/docs/copy-page-script.md -->

<!-- source: documentation/docs/database-roles-iam-authentication.md priority=0 -->
{% include 'copy-page-script.md' %}
{% include 'copy-page-button.md' %}

# Database Roles and IAM Authentication

## Overview

Amazon Aurora DSQL supports authentication using both IAM roles and IAM users. You can use either method to authenticate and access Aurora DSQL databases.

## IAM Identity Types

### IAM Roles

An IAM role is an identity within your AWS account that has specific permissions but is not associated with a specific person. Using IAM roles provide temporary security credentials. You can temporarily assume an IAM role in several ways:

- By switching roles in the AWS Console
- By calling a CLI or AWS API operation
- By using a custom URL

After assuming a role, you can access Aurora DSQL using the role's temporary credentials. For more information about methods for using roles, see [IAM Identities](https://docs.aws.amazon.com/IAM/latest/UserGuide/id.html) in the IAM user guide.

### IAM Users

An IAM user is an identity within your AWS account that has specific permissions and is associated with a single person or application. IAM users have long-term credentials such as passwords and access keys that can be used to access Aurora DSQL.

**Note**: To run SQL commands with IAM authentication, you can use either IAM role ARNs or IAM user ARNs in the examples below.

## Database Role Types

Aurora DSQL supports two types of database roles:

### Admin Role
- Aurora DSQL automatically creates a predefined `admin` role for you in your Aurora DSQL cluster
- You can't modify the `admin` role
- When you connect to your database as `admin`, you can issue SQL to create new database-level roles

### Custom Roles
- You create and manage custom roles using SQL commands in your database
- Custom roles must be associated with your IAM roles to allow IAM identities to connect to your database

## Working with Custom Database Roles

### Authorizing Database Roles to Connect to Your Cluster

1. **Create an IAM role** and grant connection authorization with the IAM policy action: `dsql:DbConnect`

2. **Grant cluster access permissions** - The IAM policy must also grant permission to access the cluster resources. Use a wildcard (`*`) or follow the instructions for using IAM condition keys with Aurora DSQL.

### Authorizing Database Roles to Use SQL in Your Database

You must use an IAM role with authorization to connect to your cluster.

**Step-by-step process:**

1. **Connect to your Aurora DSQL cluster** using a SQL utility with the `admin` database role and an IAM identity that is authorized for IAM action `dsql:DbConnectAdmin`

2. **Create a new database role** with the `WITH LOGIN` option:
   ```sql
   CREATE ROLE example WITH LOGIN;
   ```

3. **Associate the database role** with the IAM role ARN:
   ```sql
   AWS IAM GRANT example TO 'arn:aws:iam::012345678912:role/example';
   ```

4. **Grant database-level permissions** to the database role:
   ```sql
   GRANT USAGE ON SCHEMA myschema TO example;
   GRANT SELECT, INSERT, UPDATE ON ALL TABLES IN SCHEMA myschema TO example;
   ```

For more information, see [PostgreSQL GRANT](https://www.postgresql.org/docs/current/sql-grant.html) and [PostgreSQL Privileges](https://www.postgresql.org/docs/current/ddl-priv.html) in the PostgreSQL documentation.

## Viewing IAM to Database Role Mappings

To view the mappings between IAM roles and database roles, query the `sys.iam_pg_role_mappings` system table:

```sql
SELECT * FROM sys.iam_pg_role_mappings;
```

**Example output:**
```
 iam_oid |                  arn                   | pg_role_oid | pg_role_name | grantor_pg_role_oid | grantor_pg_role_name
---------+----------------------------------------+-------------+--------------+---------------------+----------------------
   26398 | arn:aws:iam::012345678912:role/example |       26396 | example      |               15579 | admin
(1 row)
```

This table shows all the mappings between IAM roles (identified by their ARN) and PostgreSQL database roles.

## Revoking Database Authorization from an IAM Role

To revoke database authorization, use the `AWS IAM REVOKE` operation:

```sql
AWS IAM REVOKE example FROM 'arn:aws:iam::012345678912:role/example';
```

## Authentication Process Flow

### For Admin Role
1. **IAM Authentication**: Generate temporary authentication token using IAM identity with `dsql:DbConnectAdmin` permission
2. **Database Connection**: Use token as password to connect with `admin` role
3. **Database Operations**: Perform administrative tasks, create custom roles

### For Custom Role
1. **Role Creation**: Admin creates custom database role with `CREATE ROLE example WITH LOGIN`
2. **IAM Association**: Admin associates role with IAM identity using `AWS IAM GRANT`
3. **Permission Granting**: Admin grants specific database permissions to custom role
4. **IAM Authentication**: User generates token using IAM identity with `dsql:DbConnect` permission
5. **Database Connection**: User connects using custom role and token

## Authorization Process Flow

### Database-Level Authorization
1. **Connect as admin**: Use admin role to connect to cluster
2. **Create custom roles**: Issue SQL commands to create new database-level roles
3. **Grant permissions**: Use PostgreSQL GRANT commands to assign specific permissions
4. **Associate with IAM**: Use `AWS IAM GRANT` to link database roles with IAM identities

### Cluster-Level Authorization
- Managed through IAM policies
- Use `dsql:DbConnectAdmin` for admin role access
- Use `dsql:DbConnect` for custom role access

## Best Practices

### Security Recommendations
- Use custom database roles for production applications instead of admin role
- Grant minimal necessary permissions to custom roles
- Regularly review and audit role mappings using `sys.iam_pg_role_mappings`
- Use temporary IAM roles when possible for enhanced security

### Permission Management
- You can't manage permissions of the predefined `admin` database role
- Modifications to privileges take effect on the next transaction after Aurora DSQL successfully commits the modification transaction
- Use PostgreSQL standard commands for managing custom role permissions
<!-- /source: documentation/docs/database-roles-iam-authentication.md -->

<!-- source: documentation/docs/generate-authentication-token.md priority=0 -->
{% include 'copy-page-script.md' %}
{% include 'copy-page-button.md' %}

# Generating an Authentication Token in Amazon Aurora DSQL

## Overview

To connect to Amazon Aurora DSQL with a SQL client, generate an authentication token to use as the password. This token is used only for authenticating the connection. After the connection is established, the connection remains valid even if the authentication token expires.

## Token Expiration and Duration

- **AWS Console**: Token automatically expires in one hour by default
- **CLI or SDKs**: Default is 15 minutes
- **Maximum duration**: 604,800 seconds (one week)

To connect to Aurora DSQL from your client again, you can use the same authentication token if it hasn't expired, or you can generate a new one.

## Prerequisites

To get started with generating a token:
1. [Create an IAM policy](https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_create-console.html)
2. [Create a cluster in Aurora DSQL](https://docs.aws.amazon.com/aurora-dsql/latest/userguide/getting-started.html#getting-started-quickstart)

At a minimum, you must have the IAM permissions for connecting to clusters, depending on which database role you use to connect.

## Using the AWS Console to Generate an Authentication Token

Aurora DSQL authenticates users with a token rather than a password. You can generate the token from the console.

**To generate an authentication token:**

1. Sign in to the AWS Console and open the Aurora DSQL console at [https://console.aws.amazon.com/dsql](https://console.aws.amazon.com/dsql)

2. Choose the cluster ID of the cluster for which you want to generate an authentication token

3. Choose **Connect** and then select **Get Token**

4. Choose whether you want to connect as an `admin` or with a custom database role

5. Copy the generated authentication token and use it for connecting with SQL clients

## Using AWS CloudShell to Generate an Authentication Token

Before you can generate an authentication token using CloudShell, make sure that you have created an Aurora DSQL cluster.

**To generate an authentication token using CloudShell:**

1. Sign in to the AWS Console and open the Aurora DSQL console at [https://console.aws.amazon.com/dsql](https://console.aws.amazon.com/dsql)

2. At the bottom left of the AWS console, choose **CloudShell**

3. Run the following command to generate an authentication token for the `admin` role:

```bash
--8<-- "samples/cli/authentication/generate_token.sh:cloudshell-admin-token"
```

**Note**: If you're not connecting as `admin`, use `generate-db-connect-auth-token` instead.

4. Use the following command to use `psql` to start a connection to your cluster:

```bash
--8<-- "samples/cli/authentication/generate_token.sh:cloudshell-psql-connection"
```

5. When prompted for a password, paste the generated token

6. Press **Enter** to see the PostgreSQL prompt: `postgres=>`

## Using the AWS CLI to Generate an Authentication Token

When your cluster is `ACTIVE`, you can generate an authentication token using the `aws dsql` command:

- **Admin role**: Use `generate-db-connect-admin-auth-token`
- **Custom database role**: Use `generate-db-connect-auth-token`

The following example uses these attributes to generate an authentication token for the `admin` role:
- **your_cluster_endpoint**: The endpoint of the cluster (format: `your_cluster_identifier.dsql.region.on.aws`)
- **region**: The AWS Region, such as `us-east-2` or `us-east-1`

**Linux and macOS:**
```bash
--8<-- "samples/cli/authentication/generate_token.sh:cli-linux-macos"
```

**Windows:**
```bash
--8<-- "samples/cli/authentication/generate_token.sh:cli-windows"
```

## Using the SDKs to Generate a Token

You can generate an authentication token for your cluster when it is in `ACTIVE` status. The SDK examples use the following attributes to generate an authentication token for the `admin` role:

- **your_cluster_endpoint**: The endpoint of your Aurora DSQL cluster (format: `your_cluster_identifier.dsql.region.on.aws`)
- **region**: The AWS Region in which your cluster is located

### Python SDK

You can generate the token in the following ways:
- **Admin role**: Use `generate_db_connect_admin_auth_token`
- **Custom database role**: Use `generate_connect_auth_token`

```python
--8<-- "samples/python/authentication/src/generate_token.py:python-generate-token"
```

### C++ SDK

You can generate the token in the following ways:
- **Admin role**: Use `GenerateDBConnectAdminAuthToken`
- **Custom database role**: Use `GenerateDBConnectAuthToken`

```cpp
--8<-- "samples/cpp/authentication/src/GenerateToken.cpp:cpp-generate-token"
```

### JavaScript SDK

You can generate the token in the following ways:
- **Admin role**: Use `getDbConnectAdminAuthToken`
- **Custom database role**: Use `getDbConnectAuthToken`

```javascript
--8<-- "samples/javascript/authentication/src/generate_token.js:javascript-generate-token"
```

### Java SDK

You can generate the token in the following ways:
- **Admin role**: Use `generateDbConnectAdminAuthToken`
- **Custom database role**: Use `generateDbConnectAuthToken`

```java
--8<-- "samples/java/authentication/src/main/java/org/example/GenerateToken.java:java-generate-token"
```

### Rust SDK

You can generate the token in the following ways:
- **Admin role**: Use `db_connect_admin_auth_token`
- **Custom database role**: Use `db_connect_auth_token`

```rust
--8<-- "samples/rust/authentication/src/bin/generate_token.rs:rust-generate-token"
```

### Ruby SDK

You can generate the token in the following ways:
- **Admin role**: Use `generate_db_connect_admin_auth_token`
- **Custom database role**: Use `generate_db_connect_auth_token`

```ruby
--8<-- "samples/ruby/authentication/lib/generate_token.rb:ruby-generate-token"
```

### .NET SDK

**Note**: The official SDK for .NET doesn't include a built-in API call to generate an authentication token for Aurora DSQL. Instead, you must use `DSQLAuthTokenGenerator`, which is a utility class.

You can generate the token in the following ways:
- **Admin role**: Use `DbConnectAdmin`
- **Custom database role**: Use `DbConnect`

```csharp
--8<-- "samples/dotnet/authentication/examples/GenerateToken/GenerateToken.cs:dotnet-generate-token"
```

### Golang SDK

**Note**: The Golang SDK doesn't provide a built-in method for generating a pre-signed token. You must manually construct the signed request.

In the following code example, specify the `action` based on the PostgreSQL user:
- **Admin role**: Use the `DbConnectAdmin` action
- **Custom database role**: Use the `DbConnect` action

```go
--8<-- "samples/go/authentication/cmd/generate_token/generate_token.go:go-generate-token"
```
<!-- /source: documentation/docs/generate-authentication-token.md -->

<!-- source: documentation/docs/generative-ai/jupyterlab.md priority=0 -->
{% include 'copy-page-script.md' %}
{% include 'copy-page-button.md' %}

# Query Editors: Using JupyterLab with Aurora DSQL

This guide provides step-by-step instructions on how to connect and query Amazon Aurora DSQL using JupyterLab with Python. JupyterLab is a popular interactive computing environment that combines code, text, and visualizations in a single document. It's widely used for data science and research applications.

The instructions below will cover the basics of Aurora DSQL usage in both a local installation of JupyterLab as well as using Amazon SageMaker AI, a fully-managed machine learning service that provides a hosted environment with a UI for data workflows.

## Getting started

### Requirements

- An Aurora DSQL cluster
- AWS credentials configured (local installation only)
- Python version 3.9 or greater (local installation only)

### Using local JupyterLab

To get started with JupyterLab, users must first install the application using Python's **pip**:

```bash
pip install jupyterlab
```

JupyterLab can then be opened by running `jupyter lab`. This will open the JupyterLab application at localhost:8888, accessible in a browser. Ensure you have AWS credentials configured in your local environment before proceeding.

### Using Amazon SageMaker AI

In the AWS console, proceed to the Amazon SageMaker AI console page and then to the **Notebooks** section under **Applications and IDEs**. From there you can select **Create notebook instance** to begin creating a SageMaker environment. Select an instance type and platform before clicking **Create notebook instance**.

See [Amazon SageMaker AI setup documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/gs-setup-working-env.html) for more information on setup and instance options.

!!! note
    Warning: Using Amazon SageMaker AI may result in charges to your AWS account.

Once the SageMaker instance becomes active, you can open it from the **Notebook instances** section with **Open JupyterLab**. Before getting started with Aurora DSQL in your notebook you must provide access to your DSQL cluster in the SageMaker instance's IAM role. The simplest way to do so is to follow the link to the IAM role in the notebook instance page. From there you can edit the Policies attached to your SageMaker IAM role. See [Authentication and authorization](../authentication-and-authorization.md) for more information on configuring an IAM policy to allow access to Aurora DSQL.

### Connecting to Aurora DSQL using JupyterLab

After you have set up a JupyterLab instance, the steps to connect to Aurora DSQL are the same locally and in SageMaker AI. Create an empty Python 3 notebook, in which you can add cells with Python code.

In a Python cell, download the Amazon root certificate from the official trust store:

```python
import urllib.request
urllib.request.urlretrieve('https://www.amazontrust.com/repository/AmazonRootCA1.pem', 'root.pem')
```

To connect to Aurora DSQL, first install the [Aurora DSQL Connector for Python](https://github.com/awslabs/aurora-dsql-python-connector) and the Psycopg driver in a Python cell, and then import it:

```bash
pip install aurora_dsql_python_connector psycopg
```

```python
import aurora_dsql_psycopg as dsql
```

With the connector imported, you can then create a DSQL configuration and connect. The Aurora DSQL Python Connector will automatically handle creation of an authentication token on each connection.

```python
config = {
    'host': "your-cluster.dsql.us-east-1.on.aws",
    'region': "us-east-1",
    'user': "admin"
}

conn = dsql.connect(**config)
```

Upon running your code you should now have a Psycopg connection to Aurora DSQL. You can then run queries using the Psycopg cursor and providing your SQL query. See the [Psycopg documentation](https://www.psycopg.org/psycopg3/docs/) for more information on using Psycopg with a Postgres-compatible database. This query will result in a list of tuples in `results_list`.

```python
with conn:
    with conn.cursor() as cur:
        cur.execute("SELECT * FROM table")
        results_list = cur.fetchall()
```

You can then use Python frameworks like [Pandas](https://pandas.pydata.org/) to analyze or visualize your query results, for example:

```bash
pip install pandas
```

```python
import pandas as pd

df = pd.DataFrame(tuples_list)
print(df)
print(f"Total records: {len(df)}")
```

## Example notebook

[A sample notebook using Aurora DSQL is available in the Aurora DSQL samples repository.](https://github.com/aws-samples/aurora-dsql-samples/tree/main/python/jupyter/sample.ipynb)

## Further reading

- [Amazon SageMaker AI setup documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/gs-setup-working-env.html)
- [Aurora DSQL Connector for Python](https://github.com/awslabs/aurora-dsql-python-connector)
- [Pandas documentation](https://pandas.pydata.org/docs/user_guide/index.html)
<!-- /source: documentation/docs/generative-ai/jupyterlab.md -->

<!-- source: documentation/docs/generative-ai/mcp-server.md priority=0 -->
{% include 'copy-page-script.md' %}
{% include 'copy-page-button.md' %}

# AWS Labs Aurora DSQL MCP Server

An AWS Labs Model Context Protocol (MCP) server for Aurora DSQL.

## Features

- Converting human-readable questions and commands into structured Postgres-compatible SQL queries and executing them against the configured Aurora DSQL database.
- Read-only by default, transactions enabled with `--allow-writes`
- Connection reuse between requests for improved performance
- Built-in access to Aurora DSQL documentation, search, and best practice recommendations

## Available Tools

### Database Operations

- **readonly_query** - Execute read-only SQL queries against your DSQL cluster
- **transact** - Execute write operations in a transaction (requires `--allow-writes`)
- **get_schema** - Retrieve table schema information

### Documentation and Recommendations

- **dsql_search_documentation** - Search Aurora DSQL documentation
  - Parameters: `search_phrase` (required), `limit` (optional)
- **dsql_read_documentation** - Read specific DSQL documentation pages
  - Parameters: `url` (required), `start_index` (optional), `max_length` (optional)
- **dsql_recommend** - Get recommendations for DSQL best practices
  - Parameters: `url` (required)

## Prerequisites

1. An AWS account with an [Aurora DSQL Cluster](https://docs.aws.amazon.com/aurora-dsql/latest/userguide/getting-started.html)
2. This MCP server can only be run locally on the same host as your LLM client.
3. Set up AWS credentials with access to AWS services
   - You need an AWS account with a role including these permissions:
     - `dsql:DbConnectAdmin` - Connect to DSQL clusters as the admin user
     - `dsql:DbConnect` - Connect to DSQL clusters with custom database roles (only needed if using non-admin users)
   - Configure AWS credentials with `aws configure` or environment variables

## Installation

For most tools, updating the configuration by following the [Default Installation](#default-installation-updating-the-relevant-mcp-config-file) instructions should be sufficient. 

Separate instructions are outlined for [Claude Code](#claude-code) and [Codex](#codex). 

### Default Installation: Updating the Relevant MCP Config File

#### Using `uv`

1. Install `uv` from [Astral](https://docs.astral.sh/uv/getting-started/installation/) or the [GitHub README](https://github.com/astral-sh/uv#installation)
2. Install Python using `uv python install 3.10`

Configure the MCP server in your MCP client configuration ([Finding the MCP Config File](#finding-the-mcp-client-configuration-file))

```json
{
  "mcpServers": {
    "awslabs.aurora-dsql-mcp-server": {
      "command": "uvx",
      "args": [
        "awslabs.aurora-dsql-mcp-server@latest",
        "--cluster_endpoint",
        "[your dsql cluster endpoint, e.g. abcdefghijklmnopqrst234567.dsql.us-east-1.on.aws]",
        "--region",
        "[your dsql cluster region, e.g. us-east-1]",
        "--database_user",
        "[your dsql username, e.g. admin]",
        "--profile",
        "default"
      ],
      "env": {
        "FASTMCP_LOG_LEVEL": "ERROR"
      },
      "disabled": false,
      "autoApprove": []
    }
  }
}
```

#### Windows Installation

For Windows users, the MCP server configuration format is slightly different:

```json
{
  "mcpServers": {
    "awslabs.aurora-dsql-mcp-server": {
      "disabled": false,
      "timeout": 60,
      "type": "stdio",
      "command": "uv",
      "args": [
        "tool",
        "run",
        "--from",
        "awslabs.aurora-dsql-mcp-server@latest",
        "awslabs.aurora-dsql-mcp-server.exe"
      ],
      "env": {
        "FASTMCP_LOG_LEVEL": "ERROR",
        "AWS_PROFILE": "your-aws-profile",
        "AWS_REGION": "us-east-1"
      }
    }
  }
}
```

#### Finding the MCP Client Configuration File
For some of the most common Agentic development tools, you can find your MCP client configurations 
at the following file paths:

- Kiro:
  - User Config: `~/.kiro/settings/mcp.json`
  - Workspace Config: `/path/to/workspace/.kiro/settings/mcp.json`
- Claude Code: Refer to [Claude Code Installation](#claude-code) for detailed setup help
  - User Config: `~/.claude.json` in `"mcpServers"`
  - Project Config: `/path/to/project/.mcp.json`
  - Local Config: `~/.claude.json` in `"projects" -> "path/to/project" -> "mcpServers"`
- Cursor:
  - Global: `~/.cursor/mcp.json`
  - Project: `/path/to/project/.cursor/mcp.json`
- Codex: `~/.codex/config.toml`
  - Each MCP server is configured with a [mcp_servers.<server-name>] table in the config file. Refer to
    the [Custom Codex Installation Instructions](#codex)
- Warp:
  - File Editing: `~/.warp/mcp_settings.json`
  - Application Editor: `Settings > AI > Manage MCP Servers` and paste json
- Amazon Q Developer CLI: `~/.aws/amazonq/mcp.json`
- Cline: Usually a nested VS Code path - `~/.vscode-server/path/to/cline_mcp_settings.json` 

### Claude Code

#### Prerequisites

**Important:** MCP server management is only available through the Claude Code CLI terminal experience, not the VS Code native panel mode.

Install the Claude Code CLI first by following Claudeâ€™s [native installation recommended process](https://code.claude.com/docs/en/setup#native-install-recommended). 

#### Choosing the Right Scope

Claude Code offers 3 different scopes: local (default), project, and user and details which scope to choose based on credential sensitivity and need to share. Refer to the Claude Code documentation on [MCP Installation Scopes](https://code.claude.com/docs/en/mcp#mcp-installation-scopes)for more details.  

1. **Local-scoped** servers represent the default configuration level and are stored in `~/.claude.json` under your projectâ€™s path. Theyâ€™re **both** private to you and only accessible within the current project directory. This is the default `scope` when creating MCP servers. 
2. **Project-scoped** servers **enable team collaboration** while still only being accessible in a project directory. Project-scoped servers add a `.mcp.json` file at your projectâ€™s root directory. This file is designed to be checked into version control, ensuring all team members have access to the same MCP tools and services. When you add a project-scoped server, Claude Code automatically creates or updates this file with the appropriate configuration structure.
3. **User-scoped** servers are stored in `~/.claude.json` and **provide cross-project accessibility**, making them available across all projects on your machine while remaining **private to your user account.** 

#### Using the Claude CLI (recommended)

Using an interactive `claude` CLI session enables an improved troubleshooting experience, 
so this is the recommended path. 

```
claude mcp add amazon-aurora-dsql \
  --scope [one of local, project, or user] \
  --env FASTMCP_LOG_LEVEL="ERROR" \
  -- uvx "awslabs.aurora-dsql-mcp-server@latest" \
  --cluster_endpoint "[dsql-cluster-id].dsql.[region].on.aws" \
  --region "[dsql cluster region, eg. us-east-1]" \
  --database_user "[your-username]"
```

##### **Troubleshooting: Using Claude Code with Bedrock on a different AWS Account**

If you've configured Claude Code with a Bedrock AWS account or profile that is
distinct from the profile needed to connect to your dsql cluster, you'll need to 
provide additional environment arguments:

```
  --env AWS_PROFILE="[dsql profile, eg. default]" \
  --env AWS_REGION="[dsql cluster region, eg. us-east-1]" \
```

#### Direct Modification in the Configuration File 
Claude Code Requires alphanumeric naming, so we recommend naming your server:
`aurora-dsql-mcp-server`. 

##### Local-Scope
Update `~/.claude.json` within the project-specific `mcpServers` field:

```json
{
  "projects": {
    "/path/to/project": {
      "mcpServers": {}
    }
  }
}
```

##### Project-Scope
Update `/path/to/project/root/.mcp.json` in the `mcpServers` field:

```json
{
  "mcpServers": {}
}
```

##### User-Scope
Update `~/.claude.json` within the project-specific `mcpServers` field:

```json
{
  "mcpServers": {}
}
```

### Codex

#### Option 1: Codex CLI
If you have the Codex CLI installed, you can use the codex mcp command to configure your MCP servers.

```bash
codex mcp add amazon-aurora-dsql \
  --env FASTMCP_LOG_LEVEL="ERROR" \
  -- uvx "awslabs.aurora-dsql-mcp-server@latest" \
  --cluster_endpoint "[dsql-cluster-id].dsql.[region].on.aws" \
  --region "[dsql cluster region, eg. us-east-1]" \
  --database_user "[your-username]"
```

#### Option 2: config.toml
For more fine grained control over MCP server options, you can manually edit the ~/.codex/config.toml configuration file. Each MCP server is configured with a `[mcp_servers.<server-name>]` table in the config file.

```toml 
[mcp_servers.amazon-aurora-dsql]
command = "uvx"
args = [
  "awslabs.aurora-dsql-mcp-server@latest",
  "--cluster_endpoint", "<DSQL_CLUSTER_ID>.dsql.<AWS_REGION>.on.aws",
  "--region", "<AWS_REGION>",
  "--database_user", "<DATABASE_USERNAME>"
]

[mcp_servers.amazon-aurora-dsql.env]
FASTMCP_LOG_LEVEL = "ERROR"
```

### Verifying Installation

For Amazon Q Developer CLI, Kiro CLI, Claude CLI/TUI, or Codex CLI/TUI, run `/mcp` to see the status 
of the MCP server.

For the Kiro IDE, you can also navigate to the Kiro Panel's `MCP SERVERS` tab which shows 
all configured MCP servers and their connection status indicators. 


## Server Configuration Options

### `--allow-writes`

By default, the dsql mcp server does not allow write operations ("read-only mode"). Any invocations of transact tool will fail in this mode. To use transact tool, allow writes by passing `--allow-writes` parameter.

We recommend using least-privilege access when connecting to DSQL. For example, users should use a role that is read-only when possible. The read-only mode has a best-effort client-side enforcement to reject mutations.

### `--cluster_endpoint`

This is mandatory parameter to specify the cluster to connect to. This should be the full endpoint of your cluster, e.g., `01abc2ldefg3hijklmnopqurstu.dsql.us-east-1.on.aws`

### `--database_user`

This is a mandatory parameter to specify the user to connect as. For example `admin`, or `my_user`. Note that the AWS credentials you are using must have permission to login as that user. For more information on setting up and using database roles in DSQL, see [Using database roles with IAM roles](https://docs.aws.amazon.com/aurora-dsql/latest/userguide/using-database-and-iam-roles.html).

### `--profile`

You can specify the aws profile to use for your credentials. Note that this is not supported for docker installation.

Using the `AWS_PROFILE` environment variable in your MCP configuration is also supported:

```json
"env": {
  "AWS_PROFILE": "your-aws-profile"
}
```

If neither is provided, the MCP server defaults to using the "default" profile in your AWS configuration file.

### `--region`

This is a mandatory parameter to specify the region of your DSQL database.

### `--knowledge-server`

Optional parameter to specify the remote MCP server endpoint for DSQL knowledge tools (documentation search, reading, and recommendations). By default it is pre-configured.

Example:

```bash
--knowledge-server https://custom-knowledge-server.example.com
```

**Note:** For security, only use trusted knowledge server endpoints. The server should be an HTTPS endpoint.

### `--knowledge-timeout`

Optional parameter to specify the timeout in seconds for requests to the knowledge server.

Default: `30.0`

Example:

```bash
--knowledge-timeout 60.0
```

Increase this value if you experience timeouts when accessing documentation on slow networks.
<!-- /source: documentation/docs/generative-ai/mcp-server.md -->

<!-- source: documentation/docs/generative-ai/query-editor.md priority=0 -->
{% include 'copy-page-script.md' %}
{% include 'copy-page-button.md' %}

# Get started with the Aurora DSQL Query Editor

With the Aurora DSQL Query Editor, you can securely connect to your Aurora DSQL clusters and run SQL queries directly from the AWS Management Console without installing or configuring external clients. It provides an intuitive workspace with built-in syntax highlighting, auto-completion, and intelligent code assistance. You can quickly explore schema objects, develop and execute SQL queries, and view results, all within a single interface.

This topic walks you through the steps to connect to a cluster, run queries, view results, and explore advanced capabilities such as execution plans.

!!! note
    The Query Editor is available in all Regions where Aurora DSQL is supported. For more information, see [AWS Regional Services](https://aws.amazon.com/about-aws/global-infrastructure/regional-product-services/).

## Prerequisites

Before you begin, ensure that you meet the following requirements:

- You have at least one Aurora DSQL cluster available. For more information, see [Step 1: Create a Single-Region Cluster](../guides/getting-started/quickstart.md#step-1-create-a-single-region-cluster).
- Your cluster endpoint is publicly accessible. The Query Editor does not currently support clusters that have public access blocked by resource-based policies or clusters managed through VPC endpoints. For more information, see [Blocking public access with resource-based policies in Aurora DSQL](https://docs.aws.amazon.com/aurora-dsql/latest/userguide/rbp-block-public-access.html) and [Managing and connecting to Amazon Aurora DSQL clusters using AWS PrivateLink](https://docs.aws.amazon.com/aurora-dsql/latest/userguide/privatelink-managing-clusters.html).
- Your IAM user or role has the required permissions to access and connect to the cluster. For more information, see [Using database roles and IAM authentication](../database-roles-iam-authentication.md).

## Working with the Query Editor

### Open the Query Editor

**To open the Query Editor:**

1. Open the [Aurora DSQL console](https://console.aws.amazon.com/dsql).
2. In the navigation pane, choose **Query Editor**.

Alternatively, from the **Clusters** page, select the cluster you want to query and choose **Connect with Query editor** to launch the editor directly.

!!! note
    Work and connection state are not saved. If you navigate away from the Aurora DSQL console, close the browser tab, or sign out, your connections, query text, and results are lost.

### Connect to a cluster

**To connect to a cluster:**

1. If no cluster connection exists, the editor displays **No cluster has been connected**. Choose **Connect** or select **+** (Add) in the **Cluster Explorer** pane to connect to an existing cluster.
2. (Optional) Connect to multiple clusters or to the same cluster using different roles.

### Explore cluster objects

The Cluster Explorer displays all available cluster connections and lets you browse objects such as databases, schemas, tables, and views. It also provides common actions like **Refresh**, **Create table**, and other context-specific options.

### Run queries

**To run a query:**

1. In the query editor tab pane, enter your SQL statement. For example:
   ```sql
   SELECT * FROM public.orders LIMIT 10;
   ```

2. Verify the **Active Cluster Context** displayed on the upper right of the query tab. This indicates the cluster connection associated with the current query tab.

3. (Optional) Use the **connection** dropdown to review all available connections or switch to a different cluster. Changing the connection updates where your queries in that tab are executed.

4. Choose **Run** to execute the query.

!!! note
    Each query can return up to 10,000 rows in the results pane. For larger datasets, refine your query with filters or limits.

### Review results and execution plans

After the query runs, review the output in the **Results panel** at the bottom of the editor. By default, each query execution displays the **Results (Table)** tab, showing tabular query output.

To get the query execution plan, run `EXPLAIN ANALYZE` or `EXPLAIN ANALYZE VERBOSE` to get additional insights into query performance. For more information, see [Reading Aurora DSQL EXPLAIN plans](https://docs.aws.amazon.com/aurora-dsql/latest/userguide/reading-dsql-explain-plans.html).

!!! tip
    The `EXPLAIN ANALYZE VERBOSE` command surfaces DPU usage estimates, including Compute, Read, Write, and Total DPU values, providing immediate visibility into the resources consumed by individual SQL statements.
<!-- /source: documentation/docs/generative-ai/query-editor.md -->

<!-- source: documentation/docs/guides/getting-started/quickstart.md priority=0 -->
{% include 'copy-page-script.md' %}
{% include 'copy-page-button.md' %}

# Getting Started with Aurora DSQL

Learn how to create an Aurora DSQL cluster, connect to it, and run your first queries. This guide walks you through creating single-Region and multi-Region Aurora DSQL clusters, connecting to them, and running sample SQL commands using the AWS Console and PostgreSQL-compatible tools.

## Prerequisites

Before you begin using Aurora DSQL, ensure you meet the following prerequisites:

- Your IAM identity must have permission to [sign in to the console](https://docs.aws.amazon.com/signin/latest/userguide/console-sign-in-tutorials.html)
- Your IAM identity must meet the following criteria:
  - Access to perform any action on any resource in your AWS account
  - `AmazonAuroraDSQLConsoleFullAccess` AWS managed policy is [attached](https://docs.aws.amazon.com/aws-managed-policy/latest/reference/AmazonAuroraDSQLConsoleFullAccess.html)

## Step 1: Create a Single-Region Cluster

The basic unit of Aurora DSQL is the cluster, which is where you store your data. In this step, you create a cluster in a single AWS Region.

### Create a single-Region cluster

1. Sign in to the AWS Console and open the Aurora DSQL console at [https://console.aws.amazon.com/dsql](https://console.aws.amazon.com/dsql)

2. Choose **Create cluster** and then **Single-Region**

3. (Optional) Change the value of the default **Name** tag

4. (Optional) Add additional **Tags** for this cluster

5. (Optional) In **Cluster settings**, select any of the following options:
   - Select **Customize encryption settings (advanced)** to choose or create an AWS KMS key
   - Select **Enable deletion protection** to prevent a delete operation from removing your cluster. By default, deletion protection is selected
   - Select **Resource-based policy (advanced)** to specify access control policies for this cluster

6. Choose **Create cluster**

7. The console returns you to the **Clusters** page. A notification banner appears indicating that the cluster is being created. Select the **Cluster ID** to open the cluster details view

## Step 2: Connect to Your Cluster

Aurora DSQL supports multiple ways to connect to your cluster, including the DSQL Query Editor, AWS CloudShell, the local psql client, and other PostgreSQL-compatible tools. In this step, you connect using the [Aurora DSQL Query Editor](https://docs.aws.amazon.com/aurora-dsql/latest/userguide/getting-started-query-editor.html), which provides a quick way to begin interacting with your new cluster.

### Connect using the Query Editor

1. In the Aurora DSQL Console ([https://console.aws.amazon.com/dsql](https://console.aws.amazon.com/dsql)), open the **Clusters** page and confirm that your cluster creation has completed and its status is Active

2. Select your cluster from the list, or choose the **Cluster ID** to open the Cluster details page

3. Choose **Connect with Query editor**

4. Choose Connect as **admin** for the cluster that was just created
   - Optionally you can connect with a custom role see [Using database roles and IAM authentication](https://docs.aws.amazon.com/aurora-dsql/latest/userguide/using-database-and-iam-roles.html)

## Step 3: Run Sample SQL Commands

Test your Aurora DSQL cluster by running SQL statements. After opening the cluster in the Query Editor, select and run each sample query step by step.

### Create a schema

Create a schema named `test`:

```sql
CREATE SCHEMA IF NOT EXISTS test;
```

### Create a table

Create a hello_world table that uses an automatically generated UUID as the primary key:

```sql
CREATE TABLE IF NOT EXISTS test.hello_world (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    message VARCHAR(255) NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

### Insert sample data

Insert a sample row:

```sql
INSERT INTO test.hello_world (message)
VALUES ('Hello, World!');
```

### Query the data

Read the inserted values:

```sql
SELECT * FROM test.hello_world;
```

### Clean up (Optional)

Optionally clean up the test resources:

```sql
DROP TABLE test.hello_world;
DROP SCHEMA test;
```

## Step 4 (Optional): Create a Multi-Region Cluster

When you create a multi-Region cluster, you specify the following Regions:

### Remote Region

This is the Region in which you create a second cluster. You create a second cluster in this Region and peer it to your initial cluster. Aurora DSQL replicates all writes on the initial cluster to the remote cluster. You can read and write on any cluster.

### Witness Region

This Region receives all data that is written to the multi-Region cluster. However, witness Regions don't host client endpoints and don't provide user data access. A limited window of the encrypted transaction log is maintained in witness Regions. This log facilitates recovery and supports transactional quorum if a Region becomes unavailable.

### Create a multi-Region cluster

Use the following procedure to create an initial cluster, create a second cluster in a different Region, and then peer the two clusters to create a multi-Region cluster. It also demonstrates cross-Region write replication and consistent reads from both Regional endpoints.

1. Sign in to the [Aurora DSQL console](https://console.aws.amazon.com/dsql)

2. In the navigation pane, choose **Clusters**

3. Choose **Create cluster** and then **Multi-Region**

4. (Optional) Change the value of the default **Name** tag

5. (Optional) Add additional **Tags** for this cluster

6. In **Multi-Region settings**, choose the following options for your initial cluster:
   - In **Witness Region**, choose a Region. Currently, only US-based Regions are supported for witness Regions in multi-Region clusters
   - (Optional) In **Remote Region cluster ARN**, enter an ARN for an existing cluster in another Region. If no cluster exists to serve as the second cluster in your multi-Region cluster, complete setup after you create the initial cluster

7. (Optional) In **Cluster settings**, select any of the following options for your initial cluster:
   - Select **Customize encryption settings (advanced)** to choose or create an AWS KMS key
   - Select **Enable deletion protection** to prevent a delete operation from removing your cluster. By default, deletion protection is selected
   - Select **Resource-based policy (advanced)** to specify access control policies for this cluster

8. Choose **Create cluster** to create your initial cluster. If you didn't enter an ARN in the previous step, the console shows the **Cluster setup pending** notification

9. In the **Cluster setup pending** notification, choose **Complete multi-Region cluster setup**. This action initiates creation of a second cluster in another Region

10. Choose one of the following options for your second cluster:
    - **Add remote Region cluster ARN** â€“ Choose this option if a cluster exists, and you want it to be the second cluster in your multi-Region cluster
    - **Create cluster in another Region** â€“ Choose this option to create a second cluster. In **Remote Region**, choose the Region for this second cluster

11. Choose **Create cluster in your-second-region**, where `your-second-region` is the location of your second cluster. The console opens in your second Region

12. (Optional) Choose cluster settings for your second cluster. For example, you can choose an AWS KMS key

13. Choose **Create cluster** to create your second cluster

14. Choose **Peer in initial-cluster-region**, where `initial-cluster-region` is the Region that hosts the first cluster that you created

15. When prompted, choose **Confirm**. This step completes the creation of your multi-Region cluster

### Connect to your second cluster

1. Open the Aurora DSQL console and choose the Region for your second cluster

2. Choose **Clusters**

3. Select the row for the second cluster in your multi-Region cluster

4. Choose **Connect with Query editor**

5. Choose **Connect as admin**

6. Create a sample schema and table, and insert data by following the steps in [Step 3: Run Sample SQL Commands](#step-3-run-sample-sql-commands)

### Query data across Regions

To query data in the second cluster from the Region hosting your initial cluster:

1. In the Aurora DSQL console, choose the Region for your initial cluster

2. Choose **Clusters**

3. Select the row for the second cluster in your multi-Region cluster

4. Choose **Connect with Query editor**

5. Choose **Connect as admin**

6. Query the data that you inserted into the second cluster:

```sql
SELECT * FROM test.hello_world;
```

## Troubleshooting

See the [Troubleshooting](https://docs.aws.amazon.com/aurora-dsql/latest/userguide/troubleshooting.html) section of the Aurora DSQL documentation.

## Next Steps

- Learn about [authentication and authorization](https://docs.aws.amazon.com/aurora-dsql/latest/userguide/authentication.html)
- Explore [programming with Aurora DSQL](https://docs.aws.amazon.com/aurora-dsql/latest/userguide/programming-with.html)
- Understand [multi-Region clusters](https://docs.aws.amazon.com/aurora-dsql/latest/userguide/multi-region-clusters.html)
- Review [security best practices](https://docs.aws.amazon.com/aurora-dsql/latest/userguide/security-best-practices.html)
<!-- /source: documentation/docs/guides/getting-started/quickstart.md -->

<!-- source: documentation/docs/index.md priority=0 -->
{% include 'copy-page-script.md' %}
{% include 'copy-page-button.md' %}

# Aurora DSQL Documentation

Welcome to the Amazon Aurora DSQL documentation. Aurora DSQL is a serverless, distributed relational database optimized for transactional workloads.

## What is Aurora DSQL?

Amazon Aurora DSQL is a serverless, distributed SQL database that provides:

- **Serverless Architecture** - No infrastructure to manage
- **Distributed Design** - Built for high availability and scalability
- **PostgreSQL Compatibility** - Use familiar PostgreSQL tools and syntax
- **Multi-Region Support** - Deploy across multiple AWS regions
- **ACID Compliance** - Full transactional consistency

## Getting Started

Ready to start using Aurora DSQL? Check out our [Quickstart Guide](guides/getting-started/quickstart.md) to:

1. Create your first Aurora DSQL cluster
2. Connect to your cluster
3. Run your first queries
4. Set up multi-region clusters

## Key Features

### Single-Region Clusters
Perfect for getting started or applications that don't require multi-region deployment.

### Multi-Region Clusters
Deploy your database across multiple AWS regions for:
- High availability
- Disaster recovery
- Low-latency access for global users

### Query Editor
Built-in query editor in the AWS Console for quick database interactions.

### IAM Authentication
Secure database access using AWS IAM roles and policies.

## Documentation Structure

- **Getting Started** - Quick guides to get you up and running
- **User Guide** - Detailed documentation on features and capabilities
- **API Reference** - Complete API documentation
- **Examples** - Sample code and use cases

## Need Help?

- [AWS Documentation](https://docs.aws.amazon.com/aurora-dsql/)
- [Troubleshooting Guide](https://docs.aws.amazon.com/aurora-dsql/latest/userguide/troubleshooting.html)
- [AWS Support](https://aws.amazon.com/support/)
<!-- /source: documentation/docs/index.md -->

<!-- source: documentation/docs/managing-clusters.md priority=0 -->
{% include 'copy-page-script.md' %}
{% include 'copy-page-button.md' %}

# Managing Amazon Aurora DSQL Clusters

## Overview

Learn how to set up and optimize performance for your Aurora DSQL deployments. Aurora DSQL provides several configuration options to help you establish the right database infrastructure for your needs.

## Cluster Management Topics

The features and functionality discussed in this guide ensure that your Aurora DSQL environment is more resilient, responsive, and capable of supporting your applications as they grow and evolve.

### Single-Region Clusters

**Purpose**: Deploy Aurora DSQL clusters within a single AWS Region

**Key Features**:
- Simplified deployment and management
- Lower latency for regional applications
- Cost-effective for single-region workloads
- Automatic scaling within the region

**Use Cases**:
- Applications with users in a specific geographic region
- Development and testing environments
- Cost-sensitive workloads
- Applications with strict data residency requirements

### Multi-Region Clusters

**Purpose**: Deploy Aurora DSQL clusters across multiple AWS Regions

**Key Features**:
- High availability across regions
- Disaster recovery capabilities
- Global application support
- Cross-region data replication

**Use Cases**:
- Global applications with worldwide users
- Business continuity and disaster recovery
- Compliance with data sovereignty requirements
- High availability requirements

### CloudFormation Setup

**Purpose**: Infrastructure as Code deployment for Aurora DSQL clusters

**Key Features**:
- Automated cluster provisioning
- Repeatable deployments
- Version-controlled infrastructure
- Integration with AWS CloudFormation

**Use Cases**:
- Automated deployment pipelines
- Consistent environment provisioning
- Infrastructure version control
- Large-scale deployments

### Cluster Lifecycle Management

**Purpose**: Manage Aurora DSQL clusters throughout their operational lifecycle

**Key Features**:
- Cluster creation and deletion
- Configuration updates and modifications
- Monitoring and maintenance
- Performance optimization

**Use Cases**:
- Ongoing cluster maintenance
- Performance tuning and optimization
- Capacity planning and scaling
- Operational monitoring

## Configuration Options

### Deployment Strategies

**Single-Region Deployment**:
- Choose appropriate AWS Region based on user location
- Configure cluster size based on expected workload
- Set up monitoring and alerting
- Plan for backup and recovery

**Multi-Region Deployment**:
- Select primary and secondary regions
- Configure cross-region replication
- Set up failover procedures
- Plan for data consistency requirements

### Performance Optimization

**Cluster Sizing**:
- Assess application requirements
- Plan for peak usage patterns
- Consider growth projections
- Monitor performance metrics

**Connection Management**:
- Implement connection pooling
- Plan for connection limits
- Configure timeout settings
- Monitor connection usage

## Best Practices

### Planning and Design
1. **Assess requirements** before cluster creation
2. **Choose appropriate regions** based on user distribution
3. **Plan for scalability** and future growth
4. **Consider compliance** and data residency requirements

### Operational Excellence
1. **Monitor cluster performance** regularly
2. **Implement automated backups** and recovery procedures
3. **Set up alerting** for critical metrics
4. **Plan for maintenance** windows and updates

### Security and Compliance
1. **Configure appropriate access controls** using IAM
2. **Implement encryption** for data at rest and in transit
3. **Regular security audits** and access reviews
4. **Compliance monitoring** for regulatory requirements

## Related Documentation

- **Getting Started**: [Getting Started](guides/getting-started/quickstart.md)
- **Authentication**: [Auth & Access Overview](authentication-and-authorization.md)
- **Quotas and Limits**: [Quotas and Limits Overview](quotas-and-limits.md)
- **Troubleshooting**: [Troubleshooting Overview](troubleshooting.md)
<!-- /source: documentation/docs/managing-clusters.md -->

<!-- source: documentation/docs/multi-region-clusters.md priority=0 -->
{% include 'copy-page-script.md' %}
{% include 'copy-page-button.md' %}

# Configuring Multi-Region Clusters

## Overview

Learn how to work with clusters that span multiple AWS Regions. Configure and manage clusters across multiple AWS Regions using either the CLI or your preferred programming language including Python, C++, JavaScript, Java, Rust, Ruby, .NET, and Golang.

## Multi-Region Cluster Concepts

### Witness Region
A witness region is a third AWS Region that helps maintain consistency and availability for your multi-region cluster. The witness region doesn't host client endpoints but maintains transaction logs to support quorum decisions.

### Cluster Linking
Multi-region clusters are created by linking two clusters in different regions. This creates a synchronized, distributed database that spans multiple geographic locations.

### High Availability
Multi-region clusters provide enhanced availability and disaster recovery capabilities by maintaining synchronized data across multiple AWS Regions.

## Using AWS SDKs

The AWS SDKs provide programmatic access to Aurora DSQL in your preferred programming language. The following sections show how to perform common multi-region cluster operations.

### Create Multi-Region Cluster

The following examples show how to create a multi-Region cluster using different programming languages.

#### Python SDK

```python
import boto3

def create_multi_region_clusters(region_1, region_2, witness_region):
    try:
        client_1 = boto3.client("dsql", region_name=region_1)
        client_2 = boto3.client("dsql", region_name=region_2)

        # We can only set the witness region for the first cluster
        cluster_1 = client_1.create_cluster(
            deletionProtectionEnabled=True,
            multiRegionProperties={"witnessRegion": witness_region},
            tags={"Name": "Python multi region cluster"}
        )
        print(f"Created {cluster_1['arn']}")

        # For the second cluster we can set witness region and designate cluster_1 as a peer
        cluster_2 = client_2.create_cluster(
            deletionProtectionEnabled=True,
            multiRegionProperties={"witnessRegion": witness_region, "clusters": [cluster_1["arn"]]},
            tags={"Name": "Python multi region cluster"}
        )
        print(f"Created {cluster_2['arn']}")

        # Now that we know the cluster_2 arn we can set it as a peer of cluster_1
        client_1.update_cluster(
            identifier=cluster_1["identifier"],
            multiRegionProperties={"witnessRegion": witness_region, "clusters": [cluster_2["arn"]]}
        )
        print(f"Added {cluster_2['arn']} as a peer of {cluster_1['arn']}")

        # Now that multiRegionProperties is fully defined for both clusters they'll begin the transition to ACTIVE
        print(f"Waiting for {cluster_1['arn']} to become ACTIVE")
        client_1.get_waiter("cluster_active").wait(
            identifier=cluster_1["identifier"],
            WaiterConfig={'Delay': 10, 'MaxAttempts': 30}
        )

        print(f"Waiting for {cluster_2['arn']} to become ACTIVE")
        client_2.get_waiter("cluster_active").wait(
            identifier=cluster_2["identifier"],
            WaiterConfig={'Delay': 10, 'MaxAttempts': 30}
        )

        return (cluster_1, cluster_2)
    except:
        print("Unable to create cluster")
        raise

def main():
    region_1 = "us-east-1"
    region_2 = "us-east-2"
    witness_region = "us-west-2"
    (cluster_1, cluster_2) = create_multi_region_clusters(region_1, region_2, witness_region)
    print("Created multi region clusters:")
    print("Cluster id: " + cluster_1['arn'])
    print("Cluster id: " + cluster_2['arn'])

if __name__ == "__main__":
    main()
```

#### JavaScript SDK

```javascript
import { DSQLClient, CreateClusterCommand, UpdateClusterCommand, waitUntilClusterActive } from "@aws-sdk/client-dsql";

async function createMultiRegionCluster(region1, region2, witnessRegion) {
    const client1 = new DSQLClient({ region: region1 });
    const client2 = new DSQLClient({ region: region2 });

    try {
        // We can only set the witness region for the first cluster
        console.log(`Creating cluster in ${region1}`);
        const createClusterCommand1 = new CreateClusterCommand({
            deletionProtectionEnabled: true,
            tags: { Name: "javascript multi region cluster 1" },
            multiRegionProperties: { witnessRegion: witnessRegion }
        });
        const response1 = await client1.send(createClusterCommand1);
        console.log(`Created ${response1.arn}`);

        // For the second cluster we can set witness region and designate the first cluster as a peer
        console.log(`Creating cluster in ${region2}`);
        const createClusterCommand2 = new CreateClusterCommand({
            deletionProtectionEnabled: true,
            tags: { Name: "javascript multi region cluster 2" },
            multiRegionProperties: {
                witnessRegion: witnessRegion,
                clusters: [response1.arn]
            }
        });
        const response2 = await client2.send(createClusterCommand2);
        console.log(`Created ${response2.arn}`);

        // Now that we know the second cluster arn we can set it as a peer of the first cluster
        const updateClusterCommand = new UpdateClusterCommand({
            identifier: response1.identifier,
            multiRegionProperties: {
                witnessRegion: witnessRegion,
                clusters: [response2.arn]
            }
        });
        await client1.send(updateClusterCommand);
        console.log(`Added ${response2.arn} as a peer of ${response1.arn}`);

        // Now that multiRegionProperties is fully defined for both clusters they'll begin the transition to ACTIVE
        console.log(`Waiting for cluster ${response1.identifier} to become ACTIVE`);
        await waitUntilClusterActive(
            { client: client1, maxWaitTime: 300 },
            { identifier: response1.identifier }
        );
        console.log(`Cluster 1 is now active`);

        console.log(`Waiting for cluster ${response2.identifier} to become ACTIVE`);
        await waitUntilClusterActive(
            { client: client2, maxWaitTime: 300 },
            { identifier: response2.identifier }
        );
        console.log(`Cluster 2 is now active`);
        console.log("The multi region clusters are now active");
        return;
    } catch (error) {
        console.error("Failed to create cluster: ", error.message);
        throw error;
    }
}

async function main() {
    const region1 = "us-east-1";
    const region2 = "us-east-2";
    const witnessRegion = "us-west-2";

    await createMultiRegionCluster(region1, region2, witnessRegion);
}

main();
```

#### Java SDK

```java
import software.amazon.awssdk.auth.credentials.DefaultCredentialsProvider;
import software.amazon.awssdk.regions.Region;
import software.amazon.awssdk.services.dsql.DsqlClient;
import software.amazon.awssdk.services.dsql.DsqlClientBuilder;
import software.amazon.awssdk.services.dsql.model.CreateClusterRequest;
import software.amazon.awssdk.services.dsql.model.CreateClusterResponse;
import software.amazon.awssdk.services.dsql.model.GetClusterResponse;
import software.amazon.awssdk.services.dsql.model.UpdateClusterRequest;

import java.time.Duration;
import java.util.Map;

public class CreateMultiRegionCluster {
    public static void main(String[] args) {
        Region region1 = Region.US_EAST_1;
        Region region2 = Region.US_EAST_2;
        Region witnessRegion = Region.US_WEST_2;

        DsqlClientBuilder clientBuilder = DsqlClient.builder()
                .credentialsProvider(DefaultCredentialsProvider.create());

        try (DsqlClient client1 = clientBuilder.region(region1).build();
             DsqlClient client2 = clientBuilder.region(region2).build()) {
            
            // We can only set the witness region for the first cluster
            System.out.println("Creating cluster in " + region1);
            CreateClusterRequest request1 = CreateClusterRequest.builder()
                    .deletionProtectionEnabled(true)
                    .multiRegionProperties(mrp -> mrp.witnessRegion(witnessRegion.toString()))
                    .tags(Map.of("Name", "java multi region cluster"))
                    .build();
            CreateClusterResponse cluster1 = client1.createCluster(request1);
            System.out.println("Created " + cluster1.arn());

            // For the second cluster we can set the witness region and designate cluster1 as a peer
            System.out.println("Creating cluster in " + region2);
            CreateClusterRequest request2 = CreateClusterRequest.builder()
                    .deletionProtectionEnabled(true)
                    .multiRegionProperties(mrp ->
                            mrp.witnessRegion(witnessRegion.toString()).clusters(cluster1.arn())
                    )
                    .tags(Map.of("Name", "java multi region cluster"))
                    .build();
            CreateClusterResponse cluster2 = client2.createCluster(request2);
            System.out.println("Created " + cluster2.arn());

            // Now that we know the cluster2 ARN we can set it as a peer of cluster1
            UpdateClusterRequest updateReq = UpdateClusterRequest.builder()
                    .identifier(cluster1.identifier())
                    .multiRegionProperties(mrp ->
                            mrp.witnessRegion(witnessRegion.toString()).clusters(cluster2.arn())
                    )
                    .build();
            client1.updateCluster(updateReq);
            System.out.printf("Added %s as a peer of %s%n", cluster2.arn(), cluster1.arn());

            // Wait for both clusters to become ACTIVE
            System.out.printf("Waiting for cluster %s to become ACTIVE%n", cluster1.arn());
            GetClusterResponse activeCluster1 = client1.waiter().waitUntilClusterActive(
                    getCluster -> getCluster.identifier(cluster1.identifier()),
                    config -> config.waitTimeout(Duration.ofMinutes(5))
            ).matched().response().orElseThrow();

            System.out.printf("Waiting for cluster %s to become ACTIVE%n", cluster2.arn());
            GetClusterResponse activeCluster2 = client2.waiter().waitUntilClusterActive(
                    getCluster -> getCluster.identifier(cluster2.identifier()),
                    config -> config.waitTimeout(Duration.ofMinutes(5))
            ).matched().response().orElseThrow();

            System.out.println("Created multi region clusters:");
            System.out.println(activeCluster1);
            System.out.println(activeCluster2);
        }
    }
}
```

#### Ruby SDK

```ruby
require "aws-sdk-dsql"
require "pp"

def create_multi_region_clusters(region_1, region_2, witness_region)
  client_1 = Aws::DSQL::Client.new(region: region_1)
  client_2 = Aws::DSQL::Client.new(region: region_2)

  # We can only set the witness region for the first cluster
  puts "Creating cluster in #{region_1}"
  cluster_1 = client_1.create_cluster(
    deletion_protection_enabled: true,
    multi_region_properties: { witness_region: witness_region },
    tags: { Name: "ruby multi region cluster" }
  )
  puts "Created #{cluster_1.arn}"

  # For the second cluster we can set witness region and designate cluster_1 as a peer
  puts "Creating cluster in #{region_2}"
  cluster_2 = client_2.create_cluster(
    deletion_protection_enabled: true,
    multi_region_properties: {
      witness_region: witness_region,
      clusters: [ cluster_1.arn ]
    },
    tags: { Name: "ruby multi region cluster" }
  )
  puts "Created #{cluster_2.arn}"

  # Now that we know the cluster_2 arn we can set it as a peer of cluster_1
  client_1.update_cluster(
    identifier: cluster_1.identifier,
    multi_region_properties: {
      witness_region: witness_region,
      clusters: [ cluster_2.arn ]
    }
  )
  puts "Added #{cluster_2.arn} as a peer of #{cluster_1.arn}"

  # Now that multi_region_properties is fully defined for both clusters they'll begin the transition to ACTIVE
  puts "Waiting for #{cluster_1.arn} to become ACTIVE"
  cluster_1 = client_1.wait_until(:cluster_active, identifier: cluster_1.identifier) do |w|
    w.max_attempts = 30
    w.delay = 10
  end

  puts "Waiting for #{cluster_2.arn} to become ACTIVE"
  cluster_2 = client_2.wait_until(:cluster_active, identifier: cluster_2.identifier) do |w|
    w.max_attempts = 30
    w.delay = 10
  end

  [ cluster_1, cluster_2 ]
rescue Aws::Errors::ServiceError => e
  abort "Failed to create multi-region clusters: #{e.message}"
end

def main
  region_1 = "us-east-1"
  region_2 = "us-east-2"
  witness_region = "us-west-2"

  cluster_1, cluster_2 = create_multi_region_clusters(region_1, region_2, witness_region)

  puts "Created multi region clusters:"
  pp cluster_1
  pp cluster_2
end

main if $PROGRAM_NAME == __FILE__
```

### Get Multi-Region Cluster Information

#### Python SDK

```python
import boto3
from datetime import datetime
import json

def get_cluster(region, identifier):
    try:
        client = boto3.client("dsql", region_name=region)
        return client.get_cluster(identifier=identifier)
    except:
        print(f"Unable to get cluster {identifier} in region {region}")
        raise

def main():
    region = "us-east-1"
    cluster_id = "<your cluster id>"
    response = get_cluster(region, cluster_id)
    print(json.dumps(response, indent=2, default=lambda obj: obj.isoformat() if isinstance(obj, datetime) else None))

if __name__ == "__main__":
    main()
```

### Update Multi-Region Cluster

#### Python SDK

```python
import boto3

def update_cluster(region, cluster_id, deletion_protection_enabled):
    try:
        client = boto3.client("dsql", region_name=region)
        return client.update_cluster(identifier=cluster_id, deletionProtectionEnabled=deletion_protection_enabled)
    except:
        print("Unable to update cluster")
        raise

def main():
    region = "us-east-1"
    cluster_id = "<your cluster id>"
    deletion_protection_enabled = False
    response = update_cluster(region, cluster_id, deletion_protection_enabled)
    print(f"Updated {response['arn']} with deletion_protection_enabled: {deletion_protection_enabled}")

if __name__ == "__main__":
    main()
```

### Delete Multi-Region Clusters

#### Python SDK

```python
import boto3

def delete_multi_region_clusters(region_1, cluster_id_1, region_2, cluster_id_2):
    try:
        client_1 = boto3.client("dsql", region_name=region_1)
        client_2 = boto3.client("dsql", region_name=region_2)

        client_1.delete_cluster(identifier=cluster_id_1)
        print(f"Deleting cluster {cluster_id_1} in {region_1}")

        # cluster_1 will stay in PENDING_DELETE state until cluster_2 is deleted
        client_2.delete_cluster(identifier=cluster_id_2)
        print(f"Deleting cluster {cluster_id_2} in {region_2}")

        # Now that both clusters have been marked for deletion they will transition to DELETING state and finalize deletion
        print(f"Waiting for {cluster_id_1} to finish deletion")
        client_1.get_waiter("cluster_not_exists").wait(
            identifier=cluster_id_1,
            WaiterConfig={'Delay': 10, 'MaxAttempts': 30}
        )

        print(f"Waiting for {cluster_id_2} to finish deletion")
        client_2.get_waiter("cluster_not_exists").wait(
            identifier=cluster_id_2,
            WaiterConfig={'Delay': 10, 'MaxAttempts': 30}
        )
    except:
        print("Unable to delete cluster")
        raise

def main():
    region_1 = "us-east-1"
    cluster_id_1 = "<cluster 1 id>"
    region_2 = "us-east-2"
    cluster_id_2 = "<cluster 2 id>"

    delete_multi_region_clusters(region_1, cluster_id_1, region_2, cluster_id_2)
    print(f"Deleted {cluster_id_1} in {region_1} and {cluster_id_2} in {region_2}")

if __name__ == "__main__":
    main()
```

## Additional SDK Examples

For complete SDK examples in all supported languages (C++, Rust, .NET, Golang), visit the [Aurora DSQL Samples GitHub repository](https://github.com/aws-samples/aurora-dsql-samples).

## Multi-Region Cluster Management

### Key Concepts

**Witness Region**: Third region that maintains transaction logs for quorum decisions
**Cluster Linking**: Process of connecting clusters across regions
**Synchronization**: Automatic data replication between linked clusters
**Failover**: Automatic switching between regions during outages

### Best Practices

**Region Selection**:
- Choose regions close to your user base
- Consider data residency requirements
- Plan for disaster recovery scenarios
- Ensure witness region is geographically separate

**Performance Considerations**:
- Account for cross-region latency
- Plan for eventual consistency during network partitions
- Monitor replication lag between regions
- Design applications for multi-region architecture

**Security Considerations**:
- Configure IAM policies for cross-region access
- Ensure encryption in transit between regions
- Plan for compliance requirements across regions
- Monitor access patterns across regions

## Operational Considerations

### Cluster States
- **CREATING**: Cluster is being provisioned
- **ACTIVE**: Cluster is ready for connections
- **UPDATING**: Cluster configuration is being modified
- **DELETING**: Cluster is being removed
- **PENDING_DELETE**: Multi-region cluster waiting for peer deletion

### Deletion Process
For multi-region clusters, both clusters must be deleted. The first cluster will remain in PENDING_DELETE state until the second cluster is also deleted.

### Monitoring
- Monitor cluster status in both regions
- Track replication metrics
- Set up alerts for failover events
- Monitor cross-region network connectivity

## Related Documentation

- **Single-Region Clusters**: [Single-Region Clusters](single-region-clusters.md)
- **Getting Started**: [Getting Started](guides/getting-started/quickstart.md)
- **Authentication**: [Auth & Access Overview](authentication-and-authorization.md)
- **Troubleshooting**: [Troubleshooting Overview](troubleshooting.md)
<!-- /source: documentation/docs/multi-region-clusters.md -->

<!-- source: documentation/docs/programming-with-dsql.md priority=0 -->
{% include 'copy-page-script.md' %}
{% include 'copy-page-button.md' %}

# Programming with Amazon Aurora DSQL

## Overview

Aurora DSQL provides you with the following tools to manage your Aurora DSQL resources programmatically and connect to your databases.

## Programmatic Access Tools

### AWS Command Line Interface (CLI)
You can create and manage your resources by using the CLI in a command-line shell. The CLI provides direct access to the APIs for AWS services, such as Aurora DSQL.

**Documentation**: [AWS CLI Command Reference for DSQL](https://docs.aws.amazon.com/cli/latest/reference/dsql)

### AWS Software Development Kits (SDKs)
AWS provides SDKs for many popular technologies and programming languages. They make it easier for you to call AWS services from within your applications in that language or technology.

**Documentation**: [Tools for developing and managing applications on AWS](https://aws.amazon.com/developer/tools/)

### Aurora DSQL API
This API is another programming interface for Aurora DSQL. When using this API, you must format every HTTPS request correctly and add a valid digital signature to every request.

**Documentation**: [Aurora DSQL API Reference](api-reference.md)

### AWS CloudFormation
The [AWS::DSQL::Cluster](https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-dsql-cluster.html) is a CloudFormation resource that enables you to create and manage Aurora DSQL clusters as part of your infrastructure as code.

CloudFormation helps you define your entire AWS environment in code, making it easier to provision, update, and replicate your infrastructure in a consistent and reliable way. When you use the AWS::DSQL::Cluster resource in your CloudFormation templates, you can declaratively provision Aurora DSQL clusters alongside your other cloud resources.

## Accessing Aurora DSQL with PostgreSQL-Compatible Clients

Aurora DSQL uses the [PostgreSQL wire protocol](https://www.postgresql.org/docs/current/protocol.html). You can connect to Aurora DSQL using a variety of tools and clients, such as CloudShell, psql, DBeaver, and DataGrip.

### Connection Parameter Mapping

| PostgreSQL | Aurora DSQL | Notes |
|------------|-------------|-------|
| **Role** (User or Group) | Database Role | Aurora DSQL creates a role named `admin`. Custom database roles must be associated with IAM roles for authentication. |
| **Host** (hostname) | Cluster Endpoint | Single-Region clusters provide a single managed endpoint with automatic traffic redirection. |
| **Port** | Default `5432` | Uses the PostgreSQL default port. |
| **Database** (dbname) | `postgres` | Aurora DSQL creates this database when you create the cluster. |
| **SSL Mode** | SSL always enabled | Aurora DSQL supports `require` SSL Mode. Connections without SSL are rejected. |
| **Password** | Authentication Token | Aurora DSQL requires temporary authentication tokens instead of long-lived passwords. |

### Authentication Requirements

When connecting, Aurora DSQL requires a signed IAM [authentication token](generate-authentication-token.md) in place of a traditional password. These temporary tokens are generated using AWS Signature Version 4 and are used only during connection establishment. Once connected, the session remains active until it ends or the client disconnects.

If you attempt to open a new session with an expired token, the connection request fails and a new token must be generated.

## SQL Client Access

Aurora DSQL supports multiple PostgreSQL-compatible clients for connecting to your cluster. Each client requires a valid authentication token.

### Using CloudShell with psql

Use the following procedure to access Aurora DSQL with the PostgreSQL interactive terminal from CloudShell.

**Steps**:
1. Sign in to the [Aurora DSQL console](https://console.aws.amazon.com/dsql)
2. Choose the cluster you want to connect to
3. Choose **Connect with Query Editor** and then **Connect with CloudShell**
4. Choose whether to connect as admin or with a custom database role
5. Choose **Launch in CloudShell** and **Run** in the CloudShell dialog

### Using Local CLI with psql

Use `psql`, a terminal-based front-end to PostgreSQL utility, to interactively enter queries and view results.

**Note**: To improve query response times, use PostgreSQL version 17 client. Ensure you have Python version 3.8+ and psql version 14+.

**Connection Example**:
```bash
# Aurora DSQL requires a valid IAM token as the password when connecting
# Generate authentication token using AWS CLI
export PGPASSWORD=$(aws dsql generate-db-connect-admin-auth-token \
  --region us-east-1 \
  --expires-in 3600 \
  --hostname your_cluster_endpoint)

# Aurora DSQL requires SSL and will reject connections without it
export PGSSLMODE=require

# Connect with psql using the environment variables
psql --quiet \
  --username admin \
  --dbname postgres \
  --host your_cluster_endpoint
```

### Using DBeaver

DBeaver is an open-source, GUI-based database tool for connecting to and managing your database.

**Setup Steps**:
1. Choose **New Database Connection**
2. Select **PostgreSQL**
3. In **Connection settings/Main** tab:
   - **Host**: Your cluster endpoint
   - **Database**: `postgres`
   - **Authentication**: `Database Native`
   - **Username**: `admin`
   - **Password**: [Generate authentication token](generate-authentication-token.md)
4. Configure SSL mode (`PGSSLMODE=require` or `PGSSLMODE=verify-full`)
5. Test connection and begin running SQL statements

**Important**: Administrative features like Session Manager and Lock Manager don't apply to Aurora DSQL due to its unique architecture.

### Using JetBrains DataGrip

DataGrip is a cross-platform IDE for working with SQL and databases, including PostgreSQL.

**Setup Steps**:
1. Choose **New Data Source** and select **PostgreSQL**
2. In **Data Sources/General** tab:
   - **Host**: Your cluster endpoint
   - **Port**: `5432`
   - **Database**: `postgres`
   - **Authentication**: `User & Password`
   - **Username**: `admin`
   - **Password**: [Generate authentication token](generate-authentication-token.md)
3. Configure SSL mode in connection settings
4. Test connection and start running SQL statements

**Important**: Some views like Sessions don't apply to Aurora DSQL due to its unique architecture.

## Database Connectivity Tools

AWS provides various tools for connecting to and working with Aurora DSQL databases, including database drivers, ORM libraries, and specialized adapters.

### Database Drivers

Low-level libraries that directly connect to the database:

| Programming Language | Driver | Sample Repository |
|---------------------|--------|-------------------|
| **C++** | libpq | [C++ libpq samples](https://github.com/aws-samples/aurora-dsql-samples/tree/main/cpp/libpq) |
| **C# (.NET)** | Npgsql | [.NET Npgsql samples](https://github.com/aws-samples/aurora-dsql-samples/tree/main/dotnet/npgsql) |
| **Go** | pgx | [Go pgx samples](https://github.com/aws-samples/aurora-dsql-samples/tree/main/go/pgx) |
| **Java** | pgJDBC | [Java pgJDBC samples](https://github.com/aws-samples/aurora-dsql-samples/tree/main/java/pgjdbc) |
| **Java** | Aurora DSQL Connector for JDBC | [JDBC Connector](https://github.com/awslabs/aurora-dsql-jdbc-connector) |
| **JavaScript** | node-postgres | [Node.js postgres samples](https://github.com/aws-samples/aurora-dsql-samples/tree/main/javascript/node-postgres) |
| **JavaScript** | Postgres.js | [Postgres.js samples](https://github.com/aws-samples/aurora-dsql-samples/tree/main/javascript/postgres-js) |
| **Python** | Psycopg | [Python Psycopg samples](https://github.com/aws-samples/aurora-dsql-samples/tree/main/python/psycopg) |
| **Python** | Psycopg2 | [Python Psycopg2 samples](https://github.com/aws-samples/aurora-dsql-samples/tree/main/python/psycopg2) |
| **Ruby** | pg | [Ruby pg samples](https://github.com/aws-samples/aurora-dsql-samples/tree/main/ruby/ruby-pg) |
| **Rust** | SQLx | [Rust SQLx samples](https://github.com/aws-samples/aurora-dsql-samples/tree/main/rust/sqlx) |

### Object-Relational Mapping (ORM) Libraries

Standalone libraries that provide object-relational mapping functionality:

| Programming Language | ORM Library | Sample Repository |
|---------------------|-------------|-------------------|
| **Java** | Hibernate | [Hibernate Pet Clinic App](https://github.com/awslabs/aurora-dsql-hibernate/tree/main/examples/pet-clinic-app) |
| **Python** | SQLAlchemy | [SQLAlchemy Pet Clinic App](https://github.com/awslabs/aurora-dsql-sqlalchemy/tree/main/examples/pet-clinic-app) |
| **TypeScript** | Sequelize | [TypeScript Sequelize samples](https://github.com/aws-samples/aurora-dsql-samples/tree/main/typescript/sequelize) |
| **TypeScript** | TypeORM | [TypeScript TypeORM samples](https://github.com/aws-samples/aurora-dsql-samples/tree/main/typescript/type-orm) |

### Aurora DSQL Adapters and Dialects

Specific extensions that make existing ORMs work with Aurora DSQL:

| Programming Language | ORM/Framework | Repository |
|---------------------|---------------|------------|
| **Java** | Hibernate | [Aurora DSQL Hibernate Adapter](https://github.com/awslabs/aurora-dsql-hibernate/) |
| **Python** | Django | [Aurora DSQL Django Adapter](https://github.com/awslabs/aurora-dsql-django/) |
| **Python** | SQLAlchemy | [Aurora DSQL SQLAlchemy Adapter](https://github.com/awslabs/aurora-dsql-sqlalchemy/) |

## Connection Troubleshooting

### Authentication Token Expiration

**Behavior**: Established sessions remain authenticated for a maximum of 1 hour or until an explicit disconnect or client-side timeout occurs.

**Important Considerations**:
- If new connections need to be established, a new authentication token must be generated
- Opening a new session (listing tables, new SQL console) forces a new authentication attempt
- If the authentication token is no longer valid, new sessions will fail and all previously opened sessions become invalid
- Plan token duration carefully using the `expires-in` option (15 minutes default, maximum 7 days)

### SSL Requirements

**SSL Mode Support**:
- `PGSSLMODE=require`: Basic SSL encryption
- `PGSSLMODE=verify-full`: SSL with certificate verification

**Important**: Aurora DSQL enforces SSL communication on the server side and rejects non-SSL connections. For `verify-full` option, you need to install SSL certificates locally.

## Related Documentation

- **Authentication Tokens**: [Generate Authentication Token](generate-authentication-token.md)
- **Database Roles**: [Database Roles and IAM Authentication](database-roles-iam-authentication.md)
- **Getting Started**: [Getting Started](guides/getting-started/quickstart.md)
- **Troubleshooting**: [Troubleshooting Overview](troubleshooting.md)
- **SSL Certificates**: [SSL/TLS certificates configuration](https://docs.aws.amazon.com/aurora-dsql/latest/userguide/configure-root-certificates.html)
<!-- /source: documentation/docs/programming-with-dsql.md -->

<!-- source: documentation/docs/quotas-and-limits.md priority=0 -->
{% include 'copy-page-script.md' %}
{% include 'copy-page-button.md' %}

# Cluster Quotas and Database Limits in Amazon Aurora DSQL

## Overview

This guide describes the cluster quotas and database limits for Amazon Aurora DSQL. Understanding these limits is essential for planning your Aurora DSQL deployment and application design.

## Cluster Quotas

Your AWS account has the following cluster quotas in Aurora DSQL. To request an increase to the service quotas for single-Region and multi-Region clusters within a specific AWS Region, use the [Service Quotas](https://console.aws.amazon.com/servicequotas) console page. For other quota increases, contact AWS Support.

### Single-Region Clusters

| Quota | Default Limit | Configurable | Error Code | Error Message |
|-------|---------------|--------------|------------|---------------|
| **Maximum single-Region clusters per AWS account** | 20 clusters | Yes | `ServiceQuotaExceededException : 402` | `You have reached the cluster limit.` |

### Multi-Region Clusters

| Quota | Default Limit | Configurable | Error Code | Error Message |
|-------|---------------|--------------|------------|---------------|
| **Maximum multi-Region clusters per AWS account** | 5 clusters | Yes | `ServiceQuotaExceededException : 402` | `You have reached the cluster limit.` |

### Storage Quotas

| Quota | Default Limit | Configurable | Error Code | Error Message |
|-------|---------------|--------------|------------|---------------|
| **Maximum storage per cluster** | 10 TiB (up to 256 TiB with approved increase) | Yes | `DISK_FULL(53100)` | `Current cluster size exceeds cluster size limit.` |

### Connection Quotas

| Quota | Default Limit | Configurable | Error Code | Error Message |
|-------|---------------|--------------|------------|---------------|
| **Maximum connections per cluster** | 10,000 connections | Yes | `TOO_MANY_CONNECTIONS(53300)` | `Unable to accept connection, too many open connections.` |
| **Maximum connection rate per cluster** | 100 connections per second | No | `CONFIGURED_LIMIT_EXCEEDED(53400)` | `Unable to accept connection, rate exceeded.` |
| **Maximum connection burst capacity per cluster** | 1,000 connections | No | No error code | No error message |
| **Connection refill rate** | 100 connections per second | No | No error code | No error message |
| **Maximum connection duration** | 60 minutes | No | No error code | No error message |

### Operational Quotas

| Quota | Default Limit | Configurable | Error Code | Error Message |
|-------|---------------|--------------|------------|---------------|
| **Maximum concurrent restore jobs** | 4 | No | No error code | No error message |

## Database Limits

The following table describes the database limits in Aurora DSQL.

### Table and Column Limits

| Limit | Default Value | Configurable | Error Code | Error Message |
|-------|---------------|--------------|------------|---------------|
| **Maximum combined size of columns in primary key** | 1 KiB | No | `54000` | `ERROR: key size too large` |
| **Maximum combined size of columns in secondary index** | 1 KiB | No | `54000` | `ERROR: key size too large` |
| **Maximum size of a row in a table** | 2 MiB | No | `54000` | `ERROR: maximum row size exceeded` |
| **Maximum size of a column (not part of index)** | 1 MiB | No | `54000` | `ERROR: maximum column size exceeded` |
| **Maximum number of columns in primary key or secondary index** | 8 | No | `54011` | `ERROR: more than 8 column keys in an index are not supported` |
| **Maximum number of columns in a table** | 255 | No | `54011` | `ERROR: tables can have at most 255 columns` |
| **Maximum number of indexes in a table** | 24 | No | `54000` | `ERROR: more than 24 indexes per table are not allowed` |

### Database Structure Limits

| Limit | Default Value | Configurable | Error Code | Error Message |
|-------|---------------|--------------|------------|---------------|
| **Maximum number of schemas in a database** | 10 | No | `54000` | `ERROR: more than 10 schemas not allowed` |
| **Maximum number of tables in a database** | 1,000 tables | No | `54000` | `ERROR: creating more than 1000 tables not allowed` |
| **Maximum number of databases in a cluster** | 1 | No | No error code | `ERROR: unsupported statement` |
| **Maximum number of views in a database** | 5,000 | No | `54000` | `ERROR: creating more than 5000 views not allowed` |
| **Maximum view definition size** | 2 MiB | No | `54000` | `ERROR: view definition too large` |

### Transaction Limits

| Limit | Default Value | Configurable | Error Code | Error Message |
|-------|---------------|--------------|------------|---------------|
| **Maximum size of all data modified in write transaction** | 10 MiB | No | `54000` | `ERROR: transaction size limit 10mb exceeded DETAIL: Current transaction size {sizemb} 10mb` |
| **Maximum number of rows mutated per transaction** | 3,000 rows | No | `54000` | `ERROR: transaction row limit exceeded` |
| **Maximum transaction time** | 5 minutes | No | `54000` | `ERROR: transaction age limit of 300s exceeded` |

### Memory Limits

| Limit | Default Value | Configurable | Error Code | Error Message |
|-------|---------------|--------------|------------|---------------|
| **Maximum base memory per query operation** | 128 MiB per transaction | No | `53200` | `ERROR: query requires too much temp space, out of memory.` |

## Quota Management

### Requesting Quota Increases

**Service Quotas Console**: Use the [Service Quotas](https://console.aws.amazon.com/servicequotas) console to request increases for:
- Single-Region cluster limits
- Multi-Region cluster limits
- Storage limits per cluster
- Connection limits per cluster

**AWS Support**: Contact AWS Support for other quota increases not available through Service Quotas console.

### Monitoring Quota Usage

**Best Practices**:
1. **Monitor cluster count** against account limits
2. **Track storage usage** per cluster
3. **Monitor connection patterns** to avoid rate limits
4. **Plan capacity** based on application requirements

## Planning Considerations

### Application Design
- **Transaction Size**: Keep transactions under 10 MiB data modification limit
- **Row Mutations**: Limit to 3,000 rows per transaction
- **Connection Management**: Plan for 10,000 connection limit per cluster
- **Schema Design**: Consider 10 schema limit per database

### Performance Planning
- **Query Memory**: Design queries to stay within 128 MiB memory limit
- **Transaction Duration**: Keep transactions under 5-minute limit
- **Index Strategy**: Plan for maximum 24 indexes per table
- **Table Structure**: Consider 255 column limit per table

### Scalability Planning
- **Multi-Region Strategy**: Plan for 5 multi-Region cluster limit
- **Storage Growth**: Plan for 10 TiB default storage limit
- **Connection Scaling**: Consider connection rate limits (100/second)
- **View Management**: Plan for 5,000 view limit per database

## Error Code Reference

### Connection Error Codes
- **53300**: `TOO_MANY_CONNECTIONS` - Exceeded connection limit
- **53400**: `CONFIGURED_LIMIT_EXCEEDED` - Exceeded connection rate
- **53100**: `DISK_FULL` - Exceeded storage limit

### Database Error Codes
- **54000**: General database limit exceeded
- **54011**: Column or key limit exceeded
- **53200**: Memory limit exceeded

### API Error Codes
- **402**: `ServiceQuotaExceededException` - Service quota exceeded

## Related Documentation

- **PostgreSQL Compatibility**: [Supported PostgreSQL features](https://docs.aws.amazon.com/aurora-dsql/latest/userguide/working-with-postgresql-compatibility-supported-sql-features.html)
- **Data Types**: [Supported data types](https://docs.aws.amazon.com/aurora-dsql/latest/userguide/working-with-postgresql-compatibility-supported-data-types.html)
- **Systems Tables**: [Using systems tables and commands](https://docs.aws.amazon.com/aurora-dsql/latest/userguide/working-with-systems-tables.html)
<!-- /source: documentation/docs/quotas-and-limits.md -->

<!-- source: documentation/docs/single-region-clusters.md priority=0 -->
{% include 'copy-page-script.md' %}
{% include 'copy-page-button.md' %}

# Configuring Single-Region Clusters

## Overview

Learn how to manage your clusters using the AWS SDKs and CLI. Configure and manage clusters for an AWS Region using either the CLI or your preferred programming language including Python, C++, JavaScript, Java, Rust, Ruby, .NET, and Golang.

The CLI provides quick access through shell commands, while AWS Software Development Kits (SDKs) enable programmatic control through native language support.

## Using AWS SDKs

The AWS SDKs provide programmatic access to Aurora DSQL in your preferred programming language. The following sections show how to perform common cluster operations using different programming languages.

### Create Cluster

The following examples show how to create a single-Region cluster using different programming languages.

#### Python SDK

```python
import boto3

def create_cluster(region):
    try:
        client = boto3.client("dsql", region_name=region)
        tags = {"Name": "Python single region cluster"}
        cluster = client.create_cluster(tags=tags, deletionProtectionEnabled=True)
        print(f"Initiated creation of cluster: {cluster['identifier']}")

        print(f"Waiting for {cluster['arn']} to become ACTIVE")
        client.get_waiter("cluster_active").wait(
            identifier=cluster["identifier"],
            WaiterConfig={
                'Delay': 10,
                'MaxAttempts': 30
            }
        )

        return cluster
    except:
        print("Unable to create cluster")
        raise

def main():
    region = "us-east-1"
    response = create_cluster(region)
    print(f"Created cluster: {response['arn']}")

if __name__ == "__main__":
    main()
```

#### JavaScript SDK

```javascript
import { DSQLClient, CreateClusterCommand, waitUntilClusterActive } from "@aws-sdk/client-dsql";

async function createCluster(region) {
    const client = new DSQLClient({ region });

    try {
        const createClusterCommand = new CreateClusterCommand({
            deletionProtectionEnabled: true,
            tags: {
                Name: "javascript single region cluster"
            },
        });
        const response = await client.send(createClusterCommand);

        console.log(`Waiting for cluster ${response.identifier} to become ACTIVE`);
        await waitUntilClusterActive(
            {
                client: client,
                maxWaitTime: 300 // Wait for 5 minutes
            },
            {
                identifier: response.identifier
            }
        );
        console.log(`Cluster Id ${response.identifier} is now active`);
        return;
    } catch (error) {
        console.error(`Unable to create cluster in ${region}: `, error.message);
        throw error;
    }
}

async function main() {
    const region = "us-east-1";
    await createCluster(region);
}

main();
```

#### Java SDK

```java
import software.amazon.awssdk.auth.credentials.DefaultCredentialsProvider;
import software.amazon.awssdk.core.waiters.WaiterResponse;
import software.amazon.awssdk.regions.Region;
import software.amazon.awssdk.services.dsql.DsqlClient;
import software.amazon.awssdk.services.dsql.model.CreateClusterRequest;
import software.amazon.awssdk.services.dsql.model.CreateClusterResponse;
import software.amazon.awssdk.services.dsql.model.GetClusterResponse;

import java.time.Duration;
import java.util.Map;

public class CreateCluster {
    public static void main(String[] args) {
        Region region = Region.US_EAST_1;

        try (DsqlClient client = DsqlClient.builder()
                .region(region)
                .credentialsProvider(DefaultCredentialsProvider.create())
                .build()) {
            
            CreateClusterRequest request = CreateClusterRequest.builder()
                    .deletionProtectionEnabled(true)
                    .tags(Map.of("Name", "java single region cluster"))
                    .build();
            CreateClusterResponse cluster = client.createCluster(request);
            System.out.println("Created " + cluster.arn());

            System.out.println("Waiting for cluster to become ACTIVE");
            WaiterResponse<GetClusterResponse> waiterResponse = client.waiter().waitUntilClusterActive(
                    getCluster -> getCluster.identifier(cluster.identifier()),
                    config -> config.waitTimeout(Duration.ofMinutes(5))
            );
            waiterResponse.matched().response().ifPresent(System.out::println);
        }
    }
}
```

### Get Cluster Information

The following examples show how to get information about a single-Region cluster.

#### Python SDK

```python
import boto3
from datetime import datetime
import json

def get_cluster(region, identifier):
    try:
        client = boto3.client("dsql", region_name=region)
        return client.get_cluster(identifier=identifier)
    except:
        print(f"Unable to get cluster {identifier} in region {region}")
        raise

def main():
    region = "us-east-1"
    cluster_id = "<your cluster id>"
    response = get_cluster(region, cluster_id)
    print(json.dumps(response, indent=2, default=lambda obj: obj.isoformat() if isinstance(obj, datetime) else None))

if __name__ == "__main__":
    main()
```

#### JavaScript SDK

```javascript
import { DSQLClient, GetClusterCommand } from "@aws-sdk/client-dsql";

async function getCluster(region, clusterId) {
    const client = new DSQLClient({ region });

    const getClusterCommand = new GetClusterCommand({
        identifier: clusterId,
    });

    try {
        return await client.send(getClusterCommand);
    } catch (error) {
        if (error.name === "ResourceNotFoundException") {
            console.log("Cluster ID not found or deleted");
        }
        throw error;
    }
}

async function main() {
    const region = "us-east-1";
    const clusterId = "<CLUSTER_ID>";

    const response = await getCluster(region, clusterId);
    console.log("Cluster: ", response);
}

main();
```

### Update Cluster

The following examples show how to update a single-Region cluster.

#### Python SDK

```python
import boto3

def update_cluster(region, cluster_id, deletion_protection_enabled):
    try:
        client = boto3.client("dsql", region_name=region)
        return client.update_cluster(identifier=cluster_id, deletionProtectionEnabled=deletion_protection_enabled)
    except:
        print("Unable to update cluster")
        raise

def main():
    region = "us-east-1"
    cluster_id = "<your cluster id>"
    deletion_protection_enabled = False
    response = update_cluster(region, cluster_id, deletion_protection_enabled)
    print(f"Updated {response['arn']} with deletion_protection_enabled: {deletion_protection_enabled}")

if __name__ == "__main__":
    main()
```

### Delete Cluster

The following examples show how to delete a single-Region cluster.

#### Python SDK

```python
import boto3

def delete_cluster(region, identifier):
    try:
        client = boto3.client("dsql", region_name=region)
        cluster = client.delete_cluster(identifier=identifier)
        print(f"Initiated delete of {cluster['arn']}")

        print("Waiting for cluster to finish deletion")
        client.get_waiter("cluster_not_exists").wait(
            identifier=cluster["identifier"],
            WaiterConfig={
                'Delay': 10,
                'MaxAttempts': 30
            }
        )
    except:
        print("Unable to delete cluster " + identifier)
        raise

def main():
    region = "us-east-1"
    cluster_id = "<cluster id>"
    delete_cluster(region, cluster_id)
    print(f"Deleted {cluster_id}")

if __name__ == "__main__":
    main()
```

## Using AWS CLI

The AWS CLI provides a command-line interface for managing your Aurora DSQL clusters. The following examples demonstrate common cluster management operations.

### Create Cluster

Create a cluster using the `create-cluster` command.

**Note**: Cluster creation is an asynchronous operation. Call the `GetCluster` API until the status changes to `ACTIVE`. You can connect to your cluster after it becomes active.

**Command**:
```bash
aws dsql create-cluster --region us-east-1
```

**Note**: To disable deletion protection during creation, include the `--no-deletion-protection-enabled` flag.

**Response**:
```json
{
    "identifier": "abc0def1baz2quux3quuux4",
    "arn": "arn:aws:dsql:us-east-1:111122223333:cluster/abc0def1baz2quux3quuux4",
    "status": "CREATING",
    "creationTime": "2024-05-25T16:56:49.784000-07:00",
    "deletionProtectionEnabled": true,
    "tag": {},
    "encryptionDetails": {
        "encryptionType": "AWS_OWNED_KMS_KEY",
        "encryptionStatus": "ENABLED"
    }
}
```

### Get Cluster Information

Get information about a cluster using the `get-cluster` command.

**Command**:
```bash
aws dsql get-cluster \
  --region us-east-1 \
  --identifier your_cluster_id
```

**Response**:
```json
{
    "identifier": "abc0def1baz2quux3quuux4",
    "arn": "arn:aws:dsql:us-east-1:111122223333:cluster/abc0def1baz2quux3quuux4",
    "status": "ACTIVE",
    "creationTime": "2024-11-27T00:32:14.434000-08:00",
    "deletionProtectionEnabled": false,
    "encryptionDetails": {
        "encryptionType": "CUSTOMER_MANAGED_KMS_KEY",
        "kmsKeyArn": "arn:aws:kms:us-east-1:111122223333:key/123a456b-c789-01de-2f34-g5hi6j7k8lm9",
        "encryptionStatus": "ENABLED"
    }
}
```

### Update Cluster

Update an existing cluster using the `update-cluster` command.

**Note**: Updates are asynchronous operations. Call the `GetCluster` API until the status changes to `ACTIVE` to see your changes.

**Command**:
```bash
aws dsql update-cluster \
  --region us-east-1 \
  --no-deletion-protection-enabled \
  --identifier your_cluster_id
```

**Response**:
```json
{
    "identifier": "abc0def1baz2quux3quuux4",
    "arn": "arn:aws:dsql:us-east-1:111122223333:cluster/abc0def1baz2quux3quuux4",
    "status": "UPDATING",
    "creationTime": "2024-05-24T09:15:32.708000-07:00"
}
```

### Delete Cluster

Delete an existing cluster using the `delete-cluster` command.

**Note**: You can only delete clusters that have deletion protection disabled. By default, deletion protection is enabled when you create new clusters.

**Command**:
```bash
aws dsql delete-cluster \
  --region us-east-1 \
  --identifier your_cluster_id
```

**Response**:
```json
{
    "identifier": "abc0def1baz2quux3quuux4",
    "arn": "arn:aws:dsql:us-east-1:111122223333:cluster/abc0def1baz2quux3quuux4",
    "status": "DELETING",
    "creationTime": "2024-05-24T09:16:43.778000-07:00"
}
```

### List Clusters

List your clusters using the `list-clusters` command.

**Command**:
```bash
aws dsql list-clusters --region us-east-1
```

**Response**:
```json
{
    "clusters": [
        {
            "identifier": "abc0def1baz2quux3quux4quuux",
            "arn": "arn:aws:dsql:us-east-1:111122223333:cluster/abc0def1baz2quux3quux4quuux"
        },
        {
            "identifier": "abc0def1baz2quux3quux5quuuux",
            "arn": "arn:aws:dsql:us-east-1:111122223333:cluster/abc0def1baz2quux3quux5quuuux"
        }
    ]
}
```

## Additional Resources

For more code samples and examples, visit the [Aurora DSQL Samples GitHub repository](https://github.com/aws-samples/aurora-dsql-samples).

## Related Documentation

- **Getting Started**: [Getting Started](guides/getting-started/quickstart.md)
- **Multi-Region Clusters**: [Multi-Region cluster management](multi-region-clusters.md)
- **Authentication**: [Auth & Access Overview](authentication-and-authorization.md)
- **Troubleshooting**: [Troubleshooting Overview](troubleshooting.md)
<!-- /source: documentation/docs/single-region-clusters.md -->

<!-- source: documentation/docs/troubleshooting.md priority=0 -->
{% include 'copy-page-script.md' %}
{% include 'copy-page-button.md' %}

# Troubleshooting Amazon Aurora DSQL

## Overview

This guide provides troubleshooting advice for common errors and issues when using Amazon Aurora DSQL. If you encounter an issue not listed here, contact AWS support.

## Connection Errors

### SSL Error Code 6

**Error Message**: `error: unrecognized SSL error code: 6` or `unable to accept connection, sni was not received`

**Root Cause**: PostgreSQL client version earlier than 14 lacks Server Name Indication (SNI) support, which is required for Aurora DSQL connections.

**Resolution**:
1. Check your client version: `psql --version`
2. Upgrade PostgreSQL client to version 14 or later
3. Retry the connection

### Network Unreachable Error

**Error Message**: `error: NetworkUnreachable`

**Root Cause**: Client doesn't support IPv6 connections on dual-stack server configuration.

**Technical Details**: When a server supports dual-stack mode, clients first resolve hostnames to both IPv4 and IPv6 addresses. They attempt IPv4 connection first, then IPv6 if initial connection fails. IPv4-only systems show generic NetworkUnreachable error instead of clear "IPv6 not supported" message.

**Resolution**: Ensure IPv6 support is available or use IPv4-only endpoint if provided.

## Authentication Errors

### IAM Authentication Failed

**Error Message**: `IAM authentication failed for user "..."`

**Root Cause**: Authentication token or IAM role has expired.

**Common Scenarios**:
- Authentication token exceeded maximum duration (1 week)
- Temporary IAM role expired before connection attempt
- IAM role credentials no longer valid

**Resolution**:
1. Generate new authentication token
2. Verify IAM role is still valid and accessible
3. Check IAM role expiration time
4. Retry connection with fresh credentials

**Related Documentation**: [Authentication and authorization guide](authentication-and-authorization.md)

### Invalid Access Key ID

**Error Message**: `An error occurred (InvalidAccessKeyId) when calling the GetObject operation: The AWS Access Key ID you provided does not exist in our records`

**Root Cause**: IAM credential validation failure.

**Resolution**:
1. Verify AWS Access Key ID is correct
2. Check if credentials have been rotated or deleted
3. Ensure credentials are properly configured in your environment

**Related Documentation**: [Why requests are signed](https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_sigv.html#why-requests-are-signed)

### IAM Role Not Found

**Error Message**: `IAM role <role> does not exist`

**Root Cause**: Aurora DSQL cannot locate the specified IAM role.

**Resolution**:
1. Verify IAM role name and ARN are correct
2. Check if role exists in the correct AWS account
3. Confirm role hasn't been deleted or renamed

**Related Documentation**: [IAM roles](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html)

### Invalid IAM ARN Format

**Error Message**: `IAM role must look like an IAM ARN`

**Root Cause**: IAM role ARN format is incorrect.

**Resolution**:
1. Verify ARN follows correct format: `arn:aws:iam::account-id:role/role-name`
2. Check for typos in ARN string
3. Ensure proper ARN structure and syntax

**Related Documentation**: [IAM ARN format](https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_identifiers.html#identifiers-arns)

## Authorization Errors

### Role Not Supported

**Error Message**: `Role <role> not supported`

**Root Cause**: Aurora DSQL doesn't support certain PostgreSQL GRANT operations.

**Resolution**: Review supported PostgreSQL commands and use alternative approaches.

**Related Documentation**: [Supported PostgreSQL commands](https://docs.aws.amazon.com/aurora-dsql/latest/userguide/working-with-postgresql-compatibility-supported-sql-subsets.html)

### Cannot Establish Trust with Role

**Error Message**: `Cannot establish trust with role <role>`

**Root Cause**: Aurora DSQL doesn't support certain PostgreSQL GRANT operations.

**Resolution**: Use Aurora DSQL-specific role management commands instead of standard PostgreSQL GRANT operations.

**Related Documentation**: [Supported PostgreSQL commands](https://docs.aws.amazon.com/aurora-dsql/latest/userguide/working-with-postgresql-compatibility-supported-sql-subsets.html)

### Database Role Does Not Exist

**Error Message**: `Role <role> does not exist`

**Root Cause**: Aurora DSQL cannot find the specified database user role.

**Resolution**:
1. Verify the database role was created properly
2. Check role name spelling and case sensitivity
3. Ensure role was created with proper permissions

**Related Documentation**: [Custom database roles](https://docs.aws.amazon.com/aurora-dsql/latest/userguide/using-database-and-iam-roles.html#using-database-and-iam-roles-custom-database-roles)

### Permission Denied for IAM Trust

**Error Message**: `ERROR: permission denied to grant IAM trust with role <role>`

**Root Cause**: Must be connected with admin role to grant access to database roles.

**Resolution**:
1. Connect to cluster using admin role
2. Verify you have `dsql:DbConnectAdmin` permission
3. Retry the grant operation

**Related Documentation**: [Database role authorization](https://docs.aws.amazon.com/aurora-dsql/latest/userguide/using-database-and-iam-roles.html#using-database-and-iam-roles-custom-database-roles-sql)

### Role Missing LOGIN Attribute

**Error Message**: `ERROR: role <role> must have the LOGIN attribute`

**Root Cause**: Database roles must have LOGIN permission to be used for connections.

**Resolution**:
1. Create role with LOGIN permission: `CREATE ROLE example WITH LOGIN;`
2. Or modify existing role: `ALTER ROLE example WITH LOGIN;`

**Related Documentation**: 
- [CREATE ROLE](https://www.postgresql.org/docs/current/sql-createrole.html)
- [ALTER ROLE](https://www.postgresql.org/docs/current/sql-alterrole.html)

### Cannot Drop Role with Dependencies

**Error Message**: `ERROR: role <role> cannot be dropped because some objects depend on it`

**Root Cause**: Database role has active IAM relationship that must be revoked first.

**Resolution**:
1. Revoke IAM relationship: `AWS IAM REVOKE example FROM 'arn:aws:iam::account:role/role-name';`
2. Then drop the database role
3. Verify no other dependencies exist

**Related Documentation**: [Revoking authorization](authentication-and-authorization.md#revoking-authorization-using-iam-and-postgresql)

## SQL Errors

### Feature Not Supported

**Error Message**: `Error: Not supported`

**Root Cause**: Attempted to use PostgreSQL feature not supported in Aurora DSQL.

**Resolution**:
1. Check Aurora DSQL feature compatibility documentation
2. Use supported alternative commands or approaches
3. Review PostgreSQL compatibility guide

**Related Documentation**: [Supported PostgreSQL features](https://docs.aws.amazon.com/aurora-dsql/latest/userguide/working-with-postgresql-compatibility-supported-sql-features.html)

### Index Creation Error

**Error Message**: `Error: use CREATE INDEX ASYNC instead`

**Root Cause**: Creating indexes on tables with existing data requires asynchronous operation.

**Resolution**:
1. Use `CREATE INDEX ASYNC` command instead of `CREATE INDEX`
2. Monitor index creation progress
3. Wait for completion before using index

**Related Documentation**: [Asynchronous index creation](https://docs.aws.amazon.com/aurora-dsql/latest/userguide/working-with-create-index-async.html)

## Concurrency Control Errors

### Mutation Conflicts

**Error Message**: `OC000 "ERROR: mutation conflicts with another transaction, retry as needed"`

**Root Cause**: Transaction attempted to modify same data as concurrent transaction.

**Technical Details**: Indicates contention on modified tuples between concurrent transactions.

**Resolution**:
1. Implement retry logic in application
2. Add exponential backoff for retries
3. Consider reducing transaction scope to minimize conflicts

**Related Documentation**: [Concurrency control](https://docs.aws.amazon.com/aurora-dsql/latest/userguide/working-with-concurrency-control.html)

### Schema Update Conflicts

**Error Message**: `OC001 "ERROR: schema has been updated by another transaction, retry as needed"`

**Root Cause**: Session catalog cache became outdated due to concurrent schema changes.

**Technical Process**:
1. Session loaded catalog version V1 at time T1
2. Another transaction updated catalog to V2 at time T2
3. Original session attempted storage read with outdated V1 catalog
4. Storage layer rejected request due to version mismatch

**Resolution**:
1. Retry the transaction (Aurora DSQL will refresh catalog cache)
2. New transaction will use updated catalog version
3. Ensure no additional schema changes occur during retry

## SSL/TLS Connection Errors

### Certificate Verification Failed

**Error Message**: `SSL error: certificate verify failed`

**Root Cause**: Client cannot verify server certificate.

**Resolution**:
1. Install Amazon Root CA 1 certificate properly
2. Set `PGSSLROOTCERT` environment variable to correct certificate file
3. Verify certificate file has correct permissions
4. Retry connection

### Unrecognized SSL Error Code

**Error Message**: `Unrecognized SSL error code: 6`

**Root Cause**: PostgreSQL client version below 14 lacks proper SSL support.

**Resolution**: Upgrade PostgreSQL client to version 17 or later.

### SSL Unregistered Scheme (Windows)

**Error Message**: `SSL error: unregistered scheme (Windows)`

**Root Cause**: Known issue with Windows psql client using system certificates.

**Resolution**: Use downloaded certificate file method for Windows connections instead of system certificates.
<!-- /source: documentation/docs/troubleshooting.md -->

<!-- source: documentation/docs/what-is-amazon-aurora-dsql.md priority=0 -->
{% include 'copy-page-script.md' %}
{% include 'copy-page-button.md' %}

# What is Amazon Aurora DSQL?

## Overview

Amazon Aurora DSQL is a serverless, distributed relational database service optimized for transactional workloads. Amazon Aurora DSQL offers virtually unlimited scale and doesn't require you to manage infrastructure. The active-active highly available architecture provides 99.99% single-Region and 99.999% multi-Region availability.

## When to Use Amazon Aurora DSQL

Aurora DSQL is optimized for transactional workloads that benefit from ACID transactions and a relational data model. Because it's serverless, Aurora DSQL is ideal for application patterns of microservice, serverless, and event-driven architectures. Aurora DSQL is PostgreSQL-compatible, so you can use familiar drivers, object-relational mappings (ORMs), frameworks, and SQL features.

Aurora DSQL automatically manages system infrastructure and scales compute, I/O, and storage based on your workload. Because you have no servers to provision or manage, you don't have to worry about maintenance downtime related to provisioning, patching, or infrastructure upgrades.

Aurora DSQL helps you to build and maintain enterprise applications that are always available at any scale. The active-active serverless design automates failure recovery, so you don't need to worry about traditional database failover. Your applications benefit from Multi-AZ and multi-Region availability, and you don't have to be concerned about eventual consistency or missing data related to failovers.

## Key Features in Amazon Aurora DSQL

### Distributed Architecture

Amazon Aurora DSQL is composed of the following multi-tenant components:

1. **Relay and connectivity**
2. **Compute and databases**
3. **Transaction log, concurrency control, and isolation**
4. **Storage**

A control plane coordinates these components. Each component provides redundancy across three Availability Zones (AZs), with:
- Automatic cluster scaling
- Self-healing in case of component failures

### Single-Region and Multi-Region Clusters

Amazon Aurora DSQL clusters provide the following benefits:

- **Synchronous data replication**
- **Consistent read operations**
- **Automatic failure recovery**
- **Data consistency across multiple AZs or Regions**

#### Failure Recovery

If an infrastructure component fails, Amazon Aurora DSQL automatically routes requests to healthy infrastructure without manual intervention. Amazon Aurora DSQL provides **atomicity, consistency, isolation, and durability (ACID) transactions** with:
- Strong consistency
- Snapshot isolation
- Atomicity
- Cross-AZ and cross-Region durability

#### Multi-Region Capabilities

Multi-Region peered clusters provide the same resilience and connectivity as single-Region clusters. But they improve availability by offering:
- Two Regional endpoints (one in each peered cluster Region)
- Both endpoints present a single logical database
- Available for concurrent read and write operations
- Strong data consistency

You can build applications that run in multiple Regions at the same time for performance and resilienceâ€”and know that readers always see the same data.

### Compatibility with PostgreSQL Databases

The distributed database layer (compute) in Amazon Aurora DSQL is based on a current major version of PostgreSQL. You can connect to Amazon Aurora DSQL with familiar PostgreSQL drivers and tools, such as `psql`.

#### Version Compatibility

- Amazon Aurora DSQL is currently compatible with **PostgreSQL version 16**
- Supports a subset of PostgreSQL features, expressions, and data types

## Technical Specifications

### Availability Guarantees

- **Single-Region**: 99.99% availability
- **Multi-Region**: 99.999% availability

### Architecture Benefits

- **Serverless**: No infrastructure management required
- **Distributed**: Built for high availability and scalability
- **Active-Active**: Highly available architecture
- **ACID Compliant**: Full transactional consistency
- **PostgreSQL Compatible**: Use familiar tools and syntax

### Scaling Characteristics

- **Virtually unlimited scale**
- **Automatic scaling** of compute, I/O, and storage
- **Multi-AZ redundancy**
- **Multi-Region support**

## Region Availability for Amazon Aurora DSQL

With Amazon Aurora DSQL, you can deploy database instances across multiple AWS Regions to support global applications and meet data residency requirements. Region availability determines where you can create and manage Aurora DSQL database clusters. Database administrators and application architects who need to design highly available, globally distributed database systems often need to understand Region support for their workloads. Common use cases include setting up cross-Region disaster recovery, serving users from geographically closer database instances to reduce latency, and maintaining data copies in specific locations for compliance.

The following table shows the AWS Regions where Aurora DSQL is currently available and the endpoint for each AWS Region:

### Supported AWS Regions

| Region Name | Region Code | Endpoint | Protocol |
|-------------|-------------|----------|----------|
| US East (Ohio) | us-east-2 | dsql.us-east-2.api.aws<br/>dsql-fips.us-east-2.api.aws | HTTPS<br/>HTTPS |
| US East (N. Virginia) | us-east-1 | dsql.us-east-1.api.aws<br/>dsql-fips.us-east-1.api.aws | HTTPS<br/>HTTPS |
| US West (Oregon) | us-west-2 | dsql.us-west-2.api.aws<br/>dsql-fips.us-west-2.api.aws | HTTPS<br/>HTTPS |
| Asia Pacific (Osaka) | ap-northeast-3 | dsql.ap-northeast-3.api.aws | HTTPS |
| Asia Pacific (Seoul) | ap-northeast-2 | dsql.ap-northeast-2.api.aws | HTTPS |
| Asia Pacific (Tokyo) | ap-northeast-1 | dsql.ap-northeast-1.api.aws | HTTPS |
| Europe (Frankfurt) | eu-central-1 | dsql.eu-central-1.api.aws | HTTPS |
| Europe (Ireland) | eu-west-1 | dsql.eu-west-1.api.aws | HTTPS |
| Europe (London) | eu-west-2 | dsql.eu-west-2.api.aws | HTTPS |
| Europe (Paris) | eu-west-3 | dsql.eu-west-3.api.aws | HTTPS |

### Multi-Region Cluster Availability for Amazon Aurora DSQL

You can create Aurora DSQL multi-Region clusters within specific AWS Region sets. Each Region set groups geographically related Regions that can work together in a multi-Region cluster.

#### US Regions
- US East (N. Virginia)
- US East (Ohio)
- US West (Oregon)

#### Asia Pacific Regions
- Asia Pacific (Osaka)
- Asia Pacific (Seoul)
- Asia Pacific (Tokyo)

#### European Regions
- Europe (Frankfurt)
- Europe (Ireland)
- Europe (London)
- Europe (Paris)

#### Important Limitations
Multi-Region clusters must be created within a single Region set. For example, you can't create a cluster that includes both US East (N. Virginia) and Europe (Ireland) Regions.

**Important**: Aurora DSQL currently doesn't support cross-continent multi-Region clusters.

## Pricing

For cost information, see [Amazon Aurora DSQL pricing](https://aws.amazon.com/rds/aurora/dsql/pricing/).

## Next Steps

For information about the core components in Amazon Aurora DSQL and to get started with the service, see the following:

1. **Getting Started** - Complete guide to creating your first Aurora DSQL cluster
2. **PostgreSQL Compatibility** - Detailed information about supported SQL features
3. **Accessing Aurora DSQL** - Methods for connecting to your clusters with PostgreSQL-compatible clients
4. **Working with Aurora DSQL** - Advanced cluster management and operations

### Cross-References
- **High Availability Architecture**: Learn more about how the distributed architecture supports high availability
- **SQL Feature Compatibility**: Understand the subset of PostgreSQL features, expressions, and data types supported
<!-- /source: documentation/docs/what-is-amazon-aurora-dsql.md -->

<!-- source: README.md priority=0 -->
<div align="center">
  <h1>
    Aurora DSQL Starter Kit
  </h1>

  <h2>
    Documentation and resources for Amazon Aurora DSQL
  </h2>

  <p>
    <a href="https://docs.aws.amazon.com/aurora-dsql/">AWS Documentation</a>
    â—† <a href="https://console.aws.amazon.com/dsql">AWS Console</a>
    â—† <a href="https://aws.amazon.com/rds/aurora/dsql/">Product Page</a>
  </p>
</div>

## Overview

Amazon Aurora DSQL is a serverless, distributed SQL database optimized for transactional workloads. This starter kit provides comprehensive documentation and resources to help you get started with Aurora DSQL.

## ðŸ“š Documentation

This repository contains documentation built with [MkDocs](https://www.mkdocs.org/) and the Material theme, following AWS documentation best practices.

## ðŸš€ What is Aurora DSQL?

Amazon Aurora DSQL is a serverless, distributed SQL database that provides:

- **Serverless Architecture** - No infrastructure to manage
- **Distributed Design** - Built for high availability and scalability
- **PostgreSQL Compatibility** - Use familiar PostgreSQL tools and syntax
- **Multi-Region Support** - Deploy across multiple AWS regions
- **ACID Compliance** - Full transactional consistency

## ðŸ“– Getting Started

Check out the [Getting Started Guide](docs/guides/getting-started/quickstart.md) to:

1. Create your first Aurora DSQL cluster
2. Connect to your cluster
3. Run your first queries
4. Set up multi-region clusters

## ðŸ“ License & Contributing

- **License:** 
  - Documentation: CC BY-SA 4.0 - see [LICENSE](LICENSE)
  - Sample Code: MIT-0 - see [LICENSE-SAMPLECODE](LICENSE-SAMPLECODE)
  - Full details: [LICENSE-SUMMARY](LICENSE-SUMMARY)
- **Contributing:** See [CONTRIBUTING.md](CONTRIBUTING.md)
- **Security:** See [CODE_OF_CONDUCT.md](CODE_OF_CONDUCT.md) for security issue notifications
<!-- /source: README.md -->
