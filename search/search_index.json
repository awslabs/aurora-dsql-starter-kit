{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"index.html","title":"Welcome","text":"\ud83d\udccb Copy Page"},{"location":"index.html#aurora-dsql-documentation","title":"Aurora DSQL Documentation","text":"<p>Welcome to the Amazon Aurora DSQL documentation. Aurora DSQL is a serverless, distributed relational database optimized for transactional workloads.</p>"},{"location":"index.html#what-is-aurora-dsql","title":"What is Aurora DSQL?","text":"<p>Amazon Aurora DSQL is a serverless, distributed SQL database that provides:</p> <ul> <li>Serverless Architecture - No infrastructure to manage</li> <li>Distributed Design - Built for high availability and scalability</li> <li>PostgreSQL Compatibility - Use familiar PostgreSQL tools and syntax</li> <li>Multi-Region Support - Deploy across multiple AWS regions</li> <li>ACID Compliance - Full transactional consistency</li> </ul>"},{"location":"index.html#getting-started","title":"Getting Started","text":"<p>Ready to start using Aurora DSQL? Check out our Quickstart Guide to:</p> <ol> <li>Create your first Aurora DSQL cluster</li> <li>Connect to your cluster</li> <li>Run your first queries</li> <li>Set up multi-region clusters</li> </ol>"},{"location":"index.html#key-features","title":"Key Features","text":""},{"location":"index.html#single-region-clusters","title":"Single-Region Clusters","text":"<p>Perfect for getting started or applications that don't require multi-region deployment.</p>"},{"location":"index.html#multi-region-clusters","title":"Multi-Region Clusters","text":"<p>Deploy your database across multiple AWS regions for: - High availability - Disaster recovery - Low-latency access for global users</p>"},{"location":"index.html#query-editor","title":"Query Editor","text":"<p>Built-in query editor in the AWS Console for quick database interactions.</p>"},{"location":"index.html#iam-authentication","title":"IAM Authentication","text":"<p>Secure database access using AWS IAM roles and policies.</p>"},{"location":"index.html#documentation-structure","title":"Documentation Structure","text":"<ul> <li>Getting Started - Quick guides to get you up and running</li> <li>User Guide - Detailed documentation on features and capabilities</li> <li>API Reference - Complete API documentation</li> <li>Examples - Sample code and use cases</li> </ul>"},{"location":"index.html#need-help","title":"Need Help?","text":"<ul> <li>AWS Documentation</li> <li>Troubleshooting Guide</li> <li>AWS Support</li> </ul>"},{"location":"api-reference.html","title":"API Reference Overview","text":"\ud83d\udccb Copy Page"},{"location":"api-reference.html#amazon-aurora-dsql-api-reference","title":"Amazon Aurora DSQL API Reference","text":""},{"location":"api-reference.html#overview","title":"Overview","text":"<p>In addition to the AWS Console and the AWS Command Line Interface (CLI), Aurora DSQL also provides an API interface. You can use the API operations to manage your resources in Aurora DSQL.</p>"},{"location":"api-reference.html#api-documentation-links","title":"API Documentation Links","text":""},{"location":"api-reference.html#actions","title":"Actions","text":"<p>For an alphabetical list of API operations, see Actions.</p>"},{"location":"api-reference.html#data-types","title":"Data Types","text":"<p>For an alphabetical list of data types, see Data types.</p>"},{"location":"api-reference.html#common-parameters","title":"Common Parameters","text":"<p>For a list of common query parameters, see Common parameters.</p>"},{"location":"api-reference.html#error-codes","title":"Error Codes","text":"<p>For descriptions of the error codes, see Common errors.</p>"},{"location":"api-reference.html#cli-reference","title":"CLI Reference","text":"<p>For more information about the AWS CLI, see AWS Command Line Interface reference for Aurora DSQL.</p>"},{"location":"api-reference.html#related-documentation","title":"Related Documentation","text":"<ul> <li>Authentication Guide: Auth &amp; Access Overview</li> <li>Getting Started: Getting Started</li> <li>Troubleshooting: Troubleshooting Overview</li> </ul>"},{"location":"authentication-and-authorization.html","title":"Auth & Access Overview","text":"\ud83d\udccb Copy Page"},{"location":"authentication-and-authorization.html#authentication-and-authorization-for-amazon-aurora-dsql","title":"Authentication and Authorization for Amazon Aurora DSQL","text":""},{"location":"authentication-and-authorization.html#overview","title":"Overview","text":"<p>Aurora DSQL uses IAM roles and policies for cluster authorization. You associate IAM roles with PostgreSQL database roles for database authorization. This approach combines benefits from IAM with PostgreSQL privileges. Aurora DSQL uses these features to provide a comprehensive authorization and access policy for your cluster, database, and data.</p>"},{"location":"authentication-and-authorization.html#managing-your-cluster-using-iam","title":"Managing Your Cluster Using IAM","text":"<p>To manage your cluster, use IAM for authentication and authorization:</p>"},{"location":"authentication-and-authorization.html#iam-authentication","title":"IAM Authentication","text":"<p>To authenticate your IAM identity when you manage Aurora DSQL clusters, you must use IAM. You can provide authentication using: - AWS Management Console - AWS CLI - AWS SDK</p>"},{"location":"authentication-and-authorization.html#iam-authorization","title":"IAM Authorization","text":"<p>To manage Aurora DSQL clusters, grant authorization using IAM actions for Aurora DSQL. For example, to describe a cluster, make sure that your IAM identity has permissions for the IAM action <code>dsql:GetCluster</code>, as in the following sample policy action:</p> <pre><code>{\n  \"Effect\": \"Allow\",\n  \"Action\": \"dsql:GetCluster\",\n  \"Resource\": \"arn:aws:dsql:us-east-1:123456789012:cluster/my-cluster\"\n}\n</code></pre>"},{"location":"authentication-and-authorization.html#connecting-to-your-cluster-using-iam","title":"Connecting to Your Cluster Using IAM","text":"<p>To connect to your cluster, use IAM for authentication and authorization:</p>"},{"location":"authentication-and-authorization.html#iam-authentication_1","title":"IAM Authentication","text":"<p>Generate a temporary authentication token using an IAM identity with authorization to connect to your cluster. To learn more, see Generating an authentication token in Amazon Aurora DSQL.</p>"},{"location":"authentication-and-authorization.html#iam-authorization_1","title":"IAM Authorization","text":"<p>Grant the following IAM policy actions to the IAM identity you're using to establish the connection to your cluster's endpoint:</p>"},{"location":"authentication-and-authorization.html#admin-role-connection","title":"Admin Role Connection","text":"<p>Use <code>dsql:DbConnectAdmin</code> if you're using the <code>admin</code> role. Aurora DSQL creates and manages this role for you. The following sample IAM policy action permits <code>admin</code> to connect to <code>my-cluster</code>:</p> <pre><code>{\n  \"Effect\": \"Allow\",\n  \"Action\": \"dsql:DbConnectAdmin\",\n  \"Resource\": \"arn:aws:dsql:us-east-1:123456789012:cluster/my-cluster\"\n}\n</code></pre>"},{"location":"authentication-and-authorization.html#custom-database-role-connection","title":"Custom Database Role Connection","text":"<p>Use <code>dsql:DbConnect</code> if you're using a custom database role. You create and manage this role by using SQL commands in your database. The following sample IAM policy action permits a custom database role to connect to <code>my-cluster</code> for up to one hour:</p> <pre><code>{\n  \"Effect\": \"Allow\",\n  \"Action\": \"dsql:DbConnect\",\n  \"Resource\": \"arn:aws:dsql:us-east-1:123456789012:cluster/my-cluster\"\n}\n</code></pre> <p>Important: After you establish a connection, your role is authorized for up to one hour for the connection.</p>"},{"location":"authentication-and-authorization.html#postgresql-database-roles-and-iam-roles","title":"PostgreSQL Database Roles and IAM Roles","text":""},{"location":"authentication-and-authorization.html#database-role-management","title":"Database Role Management","text":"<p>PostgreSQL manages database access permissions using the concept of roles. A role can be thought of as either a database user, or a group of database users, depending on how the role is set up. You create PostgreSQL roles using SQL commands. To manage database-level authorization, grant PostgreSQL permissions to your PostgreSQL database roles.</p> <p>Aurora DSQL supports two types of database roles: - Admin role: Aurora DSQL automatically creates a predefined <code>admin</code> role for you in your Aurora DSQL cluster. You can't modify the <code>admin</code> role. - Custom roles: When you connect to your database as <code>admin</code>, you can issue SQL to create new database-level roles to associate with your IAM roles.</p> <p>To let IAM roles connect to your database, associate your custom database roles with your IAM roles.</p>"},{"location":"authentication-and-authorization.html#authentication-process","title":"Authentication Process","text":"<p>Use the <code>admin</code> role to connect to your cluster. After you connect your database, use the command <code>AWS IAM GRANT</code> to associate a custom database role with the IAM identity authorized to connect to the cluster:</p> <pre><code>AWS IAM GRANT custom-db-role TO 'arn:aws:iam::account-id:role/iam-role-name';\n</code></pre> <p>To learn more, see Authorizing database roles to connect to your cluster.</p>"},{"location":"authentication-and-authorization.html#authorization-process","title":"Authorization Process","text":"<p>Use the <code>admin</code> role to connect to your cluster. Run SQL commands to set up custom database roles and grant permissions. To learn more, see: - PostgreSQL database roles - PostgreSQL privileges</p>"},{"location":"authentication-and-authorization.html#using-iam-policy-actions-with-aurora-dsql","title":"Using IAM Policy Actions with Aurora DSQL","text":"<p>The IAM policy action you use depends on the role you use to connect to your cluster: either <code>admin</code> or a custom database role. The policy also depends on the IAM actions required for this role.</p>"},{"location":"authentication-and-authorization.html#using-iam-policy-actions-to-connect-to-clusters","title":"Using IAM Policy Actions to Connect to Clusters","text":""},{"location":"authentication-and-authorization.html#admin-role-connection_1","title":"Admin Role Connection","text":"<p>When you connect to your cluster with the default database role of <code>admin</code>, use an IAM identity with authorization to perform the following IAM policy action:</p> <pre><code>\"dsql:DbConnectAdmin\"\n</code></pre>"},{"location":"authentication-and-authorization.html#custom-database-role-connection_1","title":"Custom Database Role Connection","text":"<p>When you connect to your cluster with a custom database role, first associate the IAM role with the database role. The IAM identity you use to connect to your cluster must have authorization to perform the following IAM policy action:</p> <pre><code>\"dsql:DbConnect\"\n</code></pre> <p>To learn more about custom database roles, see Using database roles and IAM authentication.</p>"},{"location":"authentication-and-authorization.html#using-iam-policy-actions-to-manage-clusters","title":"Using IAM Policy Actions to Manage Clusters","text":"<p>When managing your Aurora DSQL clusters, specify policy actions only for the actions that your role needs to perform. For example, if your role only needs to get cluster information, you might limit role permissions to only the <code>GetCluster</code> and <code>ListClusters</code> permissions, as in the following sample policy:</p> <pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"dsql:GetCluster\",\n        \"dsql:ListClusters\"\n      ],\n      \"Resource\": \"arn:aws:dsql:us-east-1:123456789012:cluster/my-cluster\"\n    }\n  ]\n}\n</code></pre>"},{"location":"authentication-and-authorization.html#all-available-iam-policy-actions-for-managing-clusters","title":"All Available IAM Policy Actions for Managing Clusters","text":"<p>The following example policy shows all available IAM policy actions for managing clusters:</p> <pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"dsql:CreateCluster\",\n        \"dsql:GetCluster\",\n        \"dsql:UpdateCluster\",\n        \"dsql:DeleteCluster\",\n        \"dsql:ListClusters\",\n        \"dsql:TagResource\",\n        \"dsql:ListTagsForResource\",\n        \"dsql:UntagResource\"\n      ],\n      \"Resource\": \"*\"\n    }\n  ]\n}\n</code></pre>"},{"location":"authentication-and-authorization.html#revoking-authorization-using-iam-and-postgresql","title":"Revoking Authorization Using IAM and PostgreSQL","text":"<p>You can revoke permissions for your IAM roles to access your database-level roles:</p>"},{"location":"authentication-and-authorization.html#revoking-admin-authorization-to-connect-to-clusters","title":"Revoking Admin Authorization to Connect to Clusters","text":"<p>To revoke authorization to connect to your cluster with the <code>admin</code> role: 1. Revoke the IAM identity's access to <code>dsql:DbConnectAdmin</code> 2. Either edit the IAM policy or detach the policy from the identity</p> <p>After revoking connection authorization from the IAM identity, Aurora DSQL rejects all new connection attempts from that IAM identity. Any active connections that use the IAM identity might stay authorized for the duration of the connection. For more information on connection durations, see Quotas and limits.</p>"},{"location":"authentication-and-authorization.html#revoking-custom-role-authorization-to-connect-to-clusters","title":"Revoking Custom Role Authorization to Connect to Clusters","text":"<p>To revoke access to database roles other than <code>admin</code>: 1. Revoke the IAM identity's access to <code>dsql:DbConnect</code> 2. Either edit the IAM policy or detach the policy from the identity</p> <p>You can also remove the association between the database role and IAM by using the command <code>AWS IAM REVOKE</code> in your database. To learn more about revoking access from database roles, see Revoking database authorization from an IAM role.</p>"},{"location":"authentication-and-authorization.html#important-notes-on-permission-management","title":"Important Notes on Permission Management","text":"<ul> <li>You can't manage permissions of the predefined <code>admin</code> database role</li> <li>To learn how to manage permissions for custom database roles, see PostgreSQL privileges</li> <li>Modifications to privileges take effect on the next transaction after Aurora DSQL successfully commits the modification transaction</li> </ul>"},{"location":"backup-and-restore.html","title":"Backup and Restore","text":"\ud83d\udccb Copy Page"},{"location":"backup-and-restore.html#backup-and-restore-for-amazon-aurora-dsql","title":"Backup and restore for Amazon Aurora DSQL","text":"<p>Amazon Aurora DSQL helps you meet your regulatory compliance and business continuity requirements through integration with AWS Backup, a fully managed data protection service that makes it easy to centralize and automate backups across AWS services, in the cloud, and on premises. The service streamlines backup creation, management, and restoration for both single-Region and multi-Region Aurora DSQL clusters.</p> <p>Key features include the following:</p> <ul> <li>Centralized backup management through the AWS Management Console, SDK, or AWS CLI</li> <li>Full cluster backups</li> <li>Automated backup schedules and retention policies</li> <li>Cross-Region and cross-account capabilities</li> <li>WORM (write-once, read-many) configuration for all the backups you store</li> </ul> <p>For more information on the features of AWS Backup Vault Lock and an extensive list of available AWS Backup features for Aurora DSQL, see Vault lock benefits and AWS Backup feature availability in the AWS Backup Developer Guide.</p>"},{"location":"backup-and-restore.html#getting-started-with-aws-backup","title":"Getting started with AWS Backup","text":"<p>AWS Backup creates complete copies of your Aurora DSQL clusters. You can get started using AWS Backup for Aurora DSQL by following the steps in Getting started with AWS Backup:</p> <ol> <li>Create on-demand backups for immediate protection.</li> <li>Establish backup plans for automated, scheduled backups.</li> <li>Configure retention periods and cross-Region copying.</li> <li>Set up monitoring and notifications for backup activities.</li> </ol>"},{"location":"backup-and-restore.html#restoring-your-backups","title":"Restoring your backups","text":"<p>When you restore Aurora DSQL clusters, AWS Backup always creates new clusters to preserve your source data.</p>"},{"location":"backup-and-restore.html#restoring-single-region-clusters","title":"Restoring single-Region clusters","text":"<p>To restore an Aurora DSQL single-Region cluster, use the AWS Backup console or CLI to select the recovery point (backup) you wish to restore. Configure the settings for the new cluster that will be created from your backup. For detailed instructions, see Restore a single-Region Aurora DSQL cluster.</p>"},{"location":"backup-and-restore.html#restoring-multi-region-clusters","title":"Restoring multi-Region clusters","text":"<p>Restoring an Aurora DSQL multi-Region cluster is supported through both the AWS Backup console and the AWS CLI. For detailed instructions, see Restore a multi-Region Aurora DSQL cluster.</p> <p>To restore to a multi-Region Aurora DSQL cluster, you can use a backup taken in a single AWS Region. However, before you initiate the restore process, you must ensure there is an identical copy of your backup in all AWS Regions for your multi-Region clusters. If you don't yet have those copies, you must first copy the backup to another AWS Region that supports multi-Region clusters.</p> <p>We recommend creating backup copies in key AWS Regions to enable robust disaster recovery options and meet compliance requirements. To view available AWS Regions for Aurora DSQL, see Region Availability for Amazon Aurora DSQL.</p> <p>For detailed instructions on these steps, see Amazon Aurora DSQL restore documentation.</p>"},{"location":"backup-and-restore.html#monitoring-and-compliance","title":"Monitoring and compliance","text":"<p>AWS Backup provides comprehensive visibility into backup and restore operations with the following resources.</p> <ul> <li>A centralized dashboard for tracking backup and restore jobs</li> <li>Integration with CloudWatch and CloudTrail.</li> <li>AWS Backup Audit Manager for compliance reporting and auditing.</li> </ul> <p>See Logging Aurora DSQL operations using AWS CloudTrail to learn more about logging records of actions taken by a user, role, or an AWS service while using Aurora DSQL.</p>"},{"location":"backup-and-restore.html#additional-resources","title":"Additional resources","text":"<p>To learn more about AWS Backup features and and using it in tandem with Aurora DSQL, see the following resources:</p> <ul> <li>Managed policies for AWS Backup</li> <li>Amazon Aurora DSQL restore</li> <li>Supported services by AWS Region</li> <li>Encryption for backups in AWS Backup</li> </ul> <p>By using AWS Backup for Aurora DSQL, you implement a robust, compliant, and automated backup strategy that protects your critical database resources while minimizing administrative overhead. Whether you manage a single cluster or a complex multi-Region deployment, AWS Backup provides the tools you need to ensure your data remains secure and recoverable.</p>"},{"location":"connectors-overview.html","title":"Connectors Overview","text":"\ud83d\udccb Copy Page"},{"location":"connectors-overview.html#connectors-for-amazon-aurora-dsql","title":"Connectors for Amazon Aurora DSQL","text":""},{"location":"connectors-overview.html#overview","title":"Overview","text":"<p>Aurora DSQL provides specialized connectors that extend existing database drivers to enable seamless IAM authentication and integration with AWS services. These connectors are designed to work with popular programming languages and frameworks while maintaining compatibility with existing PostgreSQL workflows.</p>"},{"location":"connectors-overview.html#available-connectors","title":"Available Connectors","text":""},{"location":"connectors-overview.html#java-jdbc-connector","title":"Java JDBC Connector","text":"<p>Purpose: Extends PostgreSQL JDBC driver functionality for Aurora DSQL</p> <p>Key Features: - Seamless IAM authentication integration - Automatic token generation and refresh - Compatible with existing JDBC workflows - Built on top of PostgreSQL JDBC driver</p> <p>Use Cases: - Java applications using JDBC - Enterprise Java applications - Spring Boot applications - Hibernate ORM integration</p> <p>Repository: Aurora DSQL JDBC Connector</p>"},{"location":"connectors-overview.html#python-connector","title":"Python Connector","text":"<p>Purpose: Extends Python PostgreSQL drivers for Aurora DSQL</p> <p>Key Features: - Works with Psycopg, Psycopg2, and asyncpg - Automatic IAM token handling - Seamless integration with existing Python workflows - Support for both synchronous and asynchronous operations</p> <p>Use Cases: - Python applications using PostgreSQL drivers - Django web applications - SQLAlchemy ORM integration - Data science and analytics applications</p> <p>Supported Libraries: - Psycopg (latest version) - Psycopg2 (legacy support) - asyncpg (asynchronous operations)</p> <p>Repository: Aurora DSQL Python Connector</p>"},{"location":"connectors-overview.html#nodejs-connectors","title":"Node.js Connectors","text":"<p>Purpose: Extends Node.js PostgreSQL drivers for Aurora DSQL</p> <p>Key Features: - Support for node-postgres and Postgres.js - Automatic IAM authentication handling - Compatible with existing Node.js PostgreSQL workflows - TypeScript support available</p> <p>Use Cases: - Node.js web applications - Express.js applications - TypeScript applications - Serverless functions (Lambda)</p> <p>Supported Libraries: - node-postgres (pg) - Postgres.js</p> <p>Repository: Aurora DSQL Node.js Connectors</p>"},{"location":"connectors-overview.html#connector-benefits","title":"Connector Benefits","text":""},{"location":"connectors-overview.html#simplified-authentication","title":"Simplified Authentication","text":"<ul> <li>Automatic token generation: Connectors handle IAM token creation and refresh</li> <li>Seamless integration: No changes required to existing application code</li> <li>Security best practices: Built-in support for AWS security standards</li> </ul>"},{"location":"connectors-overview.html#framework-compatibility","title":"Framework Compatibility","text":"<ul> <li>Existing workflows: Maintain compatibility with current PostgreSQL applications</li> <li>ORM support: Works with popular ORMs like Hibernate, SQLAlchemy, Django</li> <li>Migration friendly: Easy transition from standard PostgreSQL to Aurora DSQL</li> </ul>"},{"location":"connectors-overview.html#aws-integration","title":"AWS Integration","text":"<ul> <li>IAM authentication: Native support for AWS IAM roles and policies</li> <li>AWS SDK integration: Leverages existing AWS SDK configurations</li> <li>Service integration: Designed for AWS service ecosystem</li> </ul>"},{"location":"connectors-overview.html#getting-started-with-connectors","title":"Getting Started with Connectors","text":""},{"location":"connectors-overview.html#installation-process","title":"Installation Process","text":"<ol> <li>Install connector package for your programming language</li> <li>Configure AWS credentials (IAM roles, access keys, or profiles)</li> <li>Update connection strings to use Aurora DSQL endpoints</li> <li>Test connectivity with your Aurora DSQL cluster</li> </ol>"},{"location":"connectors-overview.html#configuration-requirements","title":"Configuration Requirements","text":"<ul> <li>AWS credentials: Valid IAM credentials with Aurora DSQL permissions</li> <li>Cluster endpoint: Aurora DSQL cluster endpoint URL</li> <li>Database role: Admin role or custom database role</li> <li>SSL configuration: SSL/TLS encryption enabled</li> </ul>"},{"location":"connectors-overview.html#best-practices","title":"Best Practices","text":"<ul> <li>Use IAM roles when possible for enhanced security</li> <li>Configure connection pooling appropriately for your workload</li> <li>Handle token expiration gracefully in your applications</li> <li>Monitor connection health and implement retry logic</li> </ul>"},{"location":"connectors-overview.html#future-connector-releases","title":"Future Connector Releases","text":"<p>Additional connectors are planned for future releases. For the latest information on connector availability, see the Aurora DSQL samples repository.</p>"},{"location":"connectors-overview.html#planned-languages-and-frameworks","title":"Planned Languages and Frameworks","text":"<ul> <li>Additional ORM integrations</li> <li>More programming language support</li> <li>Enhanced framework-specific features</li> <li>Improved performance optimizations</li> </ul>"},{"location":"connectors-overview.html#related-documentation","title":"Related Documentation","text":"<ul> <li>Database Drivers: Programming with DSQL Overview</li> <li>Authentication: Generate Authentication Token</li> <li>Database Roles: Database Roles and IAM Authentication</li> <li>Sample Code: Aurora DSQL Samples Repository</li> <li>Getting Started: Getting Started</li> </ul>"},{"location":"considerations.html","title":"Considerations Overview","text":"\ud83d\udccb Copy Page"},{"location":"considerations.html#considerations-for-working-with-amazon-aurora-dsql","title":"Considerations for Working with Amazon Aurora DSQL","text":""},{"location":"considerations.html#overview","title":"Overview","text":"<p>Consider the following behaviors when you work with Amazon Aurora DSQL. For more information about PostgreSQL compatibility and support, see the compatibility documentation. For quotas and limits, see the quotas and limits guide.</p>"},{"location":"considerations.html#key-considerations","title":"Key Considerations","text":""},{"location":"considerations.html#count-operations-on-large-tables","title":"COUNT(*) Operations on Large Tables","text":"<p>Behavior: Aurora DSQL doesn't complete <code>COUNT(*)</code> operations before transaction timeout for large tables.</p> <p>Recommendation: To retrieve table row count from the system catalog, use the systems tables and commands available in Aurora DSQL.</p> <p>Related Documentation: Using systems tables and commands in Aurora DSQL</p>"},{"location":"considerations.html#prepared-statements-behavior","title":"Prepared Statements Behavior","text":"<p>Behavior: Drivers calling <code>PG_PREPARED_STATEMENTS</code> might provide an inconsistent view of cached prepared statements for the cluster.</p> <p>Technical Details:  - You might see more than the expected number of prepared statements per connection for the same cluster and IAM role - Aurora DSQL doesn't preserve statement names that you prepare</p> <p>Impact: This affects statement caching behavior but doesn't impact functionality</p>"},{"location":"considerations.html#multi-region-cluster-recovery","title":"Multi-Region Cluster Recovery","text":"<p>Behavior: In rare multi-Region linked-cluster impairment scenarios, it might take longer than expected for transaction commit availability to resume.</p> <p>Technical Details: - Automated cluster recovery operations can result in transient concurrency control or connection errors - In most cases, you will only see the effects for a percentage of your workload</p> <p>Recommendation: When you see these transient errors, retry your transaction or reconnect with your client.</p>"},{"location":"considerations.html#sql-client-schema-display","title":"SQL Client Schema Display","text":"<p>Behavior: Some SQL clients, such as DataGrip, make expansive calls to system metadata to populate schema information.</p> <p>Technical Details: - Aurora DSQL doesn't support all of this metadata information and returns errors - This issue doesn't affect SQL query functionality - It might affect schema display in certain clients</p> <p>Impact: Query functionality remains unaffected, only visual schema display may be impacted</p>"},{"location":"considerations.html#admin-role-permissions","title":"Admin Role Permissions","text":"<p>Behavior: The admin role has a set of permissions related to database management tasks.</p> <p>Technical Details: - By default, these permissions don't extend to objects that other users create - The admin role can't grant or revoke permissions on user-created objects to other users - The admin user can grant itself any other role to get the necessary permissions on these objects</p> <p>Recommendation: Use role-based access control for managing permissions on user-created objects</p>"},{"location":"considerations.html#general-behavioral-considerations","title":"General Behavioral Considerations","text":""},{"location":"considerations.html#transaction-behavior","title":"Transaction Behavior","text":"<ul> <li>Timeout Limits: Large operations may timeout before completion</li> <li>Retry Logic: Implement retry mechanisms for transient errors</li> <li>Connection Management: Plan for connection lifecycle and recovery scenarios</li> </ul>"},{"location":"considerations.html#client-compatibility","title":"Client Compatibility","text":"<ul> <li>Version Requirements: Use supported PostgreSQL client versions</li> <li>Feature Support: Not all PostgreSQL features are available</li> <li>Error Handling: Implement proper error handling for unsupported operations</li> </ul>"},{"location":"considerations.html#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Large Table Operations: Consider alternative approaches for operations on large datasets</li> <li>Index Usage: Optimize queries to use available indexes effectively</li> <li>Connection Pooling: Implement appropriate connection pooling strategies</li> </ul>"},{"location":"considerations.html#security-considerations","title":"Security Considerations","text":"<ul> <li>Role Management: Understand admin role limitations with user-created objects</li> <li>Permission Inheritance: Plan role hierarchy for proper access control</li> <li>IAM Integration: Leverage IAM roles for secure database access</li> </ul>"},{"location":"considerations.html#best-practices","title":"Best Practices","text":""},{"location":"considerations.html#application-design","title":"Application Design","text":"<ol> <li>Implement retry logic for transient errors during cluster recovery</li> <li>Use appropriate timeouts for large table operations</li> <li>Plan for client compatibility requirements</li> <li>Design role hierarchy to handle admin role limitations</li> </ol>"},{"location":"considerations.html#operational-practices","title":"Operational Practices","text":"<ol> <li>Monitor cluster health and recovery operations</li> <li>Test client compatibility before production deployment</li> <li>Implement proper error handling for unsupported features</li> <li>Plan capacity based on quotas and limits</li> </ol>"},{"location":"considerations.html#development-practices","title":"Development Practices","text":"<ol> <li>Use supported PostgreSQL features only</li> <li>Test with realistic data volumes to understand timeout behavior</li> <li>Implement proper connection management in applications</li> <li>Design for eventual consistency during recovery scenarios</li> </ol>"},{"location":"copy-page-button.html","title":"Copy page button","text":"\ud83d\udccb Copy Page"},{"location":"copy-page-script.html","title":"Copy page script","text":""},{"location":"database-roles-iam-authentication.html","title":"Database Roles and IAM Authentication","text":"\ud83d\udccb Copy Page"},{"location":"database-roles-iam-authentication.html#database-roles-and-iam-authentication","title":"Database Roles and IAM Authentication","text":""},{"location":"database-roles-iam-authentication.html#overview","title":"Overview","text":"<p>Amazon Aurora DSQL supports authentication using both IAM roles and IAM users. You can use either method to authenticate and access Aurora DSQL databases.</p>"},{"location":"database-roles-iam-authentication.html#iam-identity-types","title":"IAM Identity Types","text":""},{"location":"database-roles-iam-authentication.html#iam-roles","title":"IAM Roles","text":"<p>An IAM role is an identity within your AWS account that has specific permissions but is not associated with a specific person. Using IAM roles provide temporary security credentials. You can temporarily assume an IAM role in several ways:</p> <ul> <li>By switching roles in the AWS Console</li> <li>By calling a CLI or AWS API operation</li> <li>By using a custom URL</li> </ul> <p>After assuming a role, you can access Aurora DSQL using the role's temporary credentials. For more information about methods for using roles, see IAM Identities in the IAM user guide.</p>"},{"location":"database-roles-iam-authentication.html#iam-users","title":"IAM Users","text":"<p>An IAM user is an identity within your AWS account that has specific permissions and is associated with a single person or application. IAM users have long-term credentials such as passwords and access keys that can be used to access Aurora DSQL.</p> <p>Note: To run SQL commands with IAM authentication, you can use either IAM role ARNs or IAM user ARNs in the examples below.</p>"},{"location":"database-roles-iam-authentication.html#database-role-types","title":"Database Role Types","text":"<p>Aurora DSQL supports two types of database roles:</p>"},{"location":"database-roles-iam-authentication.html#admin-role","title":"Admin Role","text":"<ul> <li>Aurora DSQL automatically creates a predefined <code>admin</code> role for you in your Aurora DSQL cluster</li> <li>You can't modify the <code>admin</code> role</li> <li>When you connect to your database as <code>admin</code>, you can issue SQL to create new database-level roles</li> </ul>"},{"location":"database-roles-iam-authentication.html#custom-roles","title":"Custom Roles","text":"<ul> <li>You create and manage custom roles using SQL commands in your database</li> <li>Custom roles must be associated with your IAM roles to allow IAM identities to connect to your database</li> </ul>"},{"location":"database-roles-iam-authentication.html#working-with-custom-database-roles","title":"Working with Custom Database Roles","text":""},{"location":"database-roles-iam-authentication.html#authorizing-database-roles-to-connect-to-your-cluster","title":"Authorizing Database Roles to Connect to Your Cluster","text":"<ol> <li> <p>Create an IAM role and grant connection authorization with the IAM policy action: <code>dsql:DbConnect</code></p> </li> <li> <p>Grant cluster access permissions - The IAM policy must also grant permission to access the cluster resources. Use a wildcard (<code>*</code>) or follow the instructions for using IAM condition keys with Aurora DSQL.</p> </li> </ol>"},{"location":"database-roles-iam-authentication.html#authorizing-database-roles-to-use-sql-in-your-database","title":"Authorizing Database Roles to Use SQL in Your Database","text":"<p>You must use an IAM role with authorization to connect to your cluster.</p> <p>Step-by-step process:</p> <ol> <li> <p>Connect to your Aurora DSQL cluster using a SQL utility with the <code>admin</code> database role and an IAM identity that is authorized for IAM action <code>dsql:DbConnectAdmin</code></p> </li> <li> <p>Create a new database role with the <code>WITH LOGIN</code> option:    <pre><code>CREATE ROLE example WITH LOGIN;\n</code></pre></p> </li> <li> <p>Associate the database role with the IAM role ARN:    <pre><code>AWS IAM GRANT example TO 'arn:aws:iam::012345678912:role/example';\n</code></pre></p> </li> <li> <p>Grant database-level permissions to the database role:    <pre><code>GRANT USAGE ON SCHEMA myschema TO example;\nGRANT SELECT, INSERT, UPDATE ON ALL TABLES IN SCHEMA myschema TO example;\n</code></pre></p> </li> </ol> <p>For more information, see PostgreSQL GRANT and PostgreSQL Privileges in the PostgreSQL documentation.</p>"},{"location":"database-roles-iam-authentication.html#viewing-iam-to-database-role-mappings","title":"Viewing IAM to Database Role Mappings","text":"<p>To view the mappings between IAM roles and database roles, query the <code>sys.iam_pg_role_mappings</code> system table:</p> <pre><code>SELECT * FROM sys.iam_pg_role_mappings;\n</code></pre> <p>Example output: <pre><code> iam_oid |                  arn                   | pg_role_oid | pg_role_name | grantor_pg_role_oid | grantor_pg_role_name\n---------+----------------------------------------+-------------+--------------+---------------------+----------------------\n   26398 | arn:aws:iam::012345678912:role/example |       26396 | example      |               15579 | admin\n(1 row)\n</code></pre></p> <p>This table shows all the mappings between IAM roles (identified by their ARN) and PostgreSQL database roles.</p>"},{"location":"database-roles-iam-authentication.html#revoking-database-authorization-from-an-iam-role","title":"Revoking Database Authorization from an IAM Role","text":"<p>To revoke database authorization, use the <code>AWS IAM REVOKE</code> operation:</p> <pre><code>AWS IAM REVOKE example FROM 'arn:aws:iam::012345678912:role/example';\n</code></pre>"},{"location":"database-roles-iam-authentication.html#authentication-process-flow","title":"Authentication Process Flow","text":""},{"location":"database-roles-iam-authentication.html#for-admin-role","title":"For Admin Role","text":"<ol> <li>IAM Authentication: Generate temporary authentication token using IAM identity with <code>dsql:DbConnectAdmin</code> permission</li> <li>Database Connection: Use token as password to connect with <code>admin</code> role</li> <li>Database Operations: Perform administrative tasks, create custom roles</li> </ol>"},{"location":"database-roles-iam-authentication.html#for-custom-role","title":"For Custom Role","text":"<ol> <li>Role Creation: Admin creates custom database role with <code>CREATE ROLE example WITH LOGIN</code></li> <li>IAM Association: Admin associates role with IAM identity using <code>AWS IAM GRANT</code></li> <li>Permission Granting: Admin grants specific database permissions to custom role</li> <li>IAM Authentication: User generates token using IAM identity with <code>dsql:DbConnect</code> permission</li> <li>Database Connection: User connects using custom role and token</li> </ol>"},{"location":"database-roles-iam-authentication.html#authorization-process-flow","title":"Authorization Process Flow","text":""},{"location":"database-roles-iam-authentication.html#database-level-authorization","title":"Database-Level Authorization","text":"<ol> <li>Connect as admin: Use admin role to connect to cluster</li> <li>Create custom roles: Issue SQL commands to create new database-level roles</li> <li>Grant permissions: Use PostgreSQL GRANT commands to assign specific permissions</li> <li>Associate with IAM: Use <code>AWS IAM GRANT</code> to link database roles with IAM identities</li> </ol>"},{"location":"database-roles-iam-authentication.html#cluster-level-authorization","title":"Cluster-Level Authorization","text":"<ul> <li>Managed through IAM policies</li> <li>Use <code>dsql:DbConnectAdmin</code> for admin role access</li> <li>Use <code>dsql:DbConnect</code> for custom role access</li> </ul>"},{"location":"database-roles-iam-authentication.html#best-practices","title":"Best Practices","text":""},{"location":"database-roles-iam-authentication.html#security-recommendations","title":"Security Recommendations","text":"<ul> <li>Use custom database roles for production applications instead of admin role</li> <li>Grant minimal necessary permissions to custom roles</li> <li>Regularly review and audit role mappings using <code>sys.iam_pg_role_mappings</code></li> <li>Use temporary IAM roles when possible for enhanced security</li> </ul>"},{"location":"database-roles-iam-authentication.html#permission-management","title":"Permission Management","text":"<ul> <li>You can't manage permissions of the predefined <code>admin</code> database role</li> <li>Modifications to privileges take effect on the next transaction after Aurora DSQL successfully commits the modification transaction</li> <li>Use PostgreSQL standard commands for managing custom role permissions</li> </ul>"},{"location":"generate-authentication-token.html","title":"Generate Authentication Token","text":"\ud83d\udccb Copy Page"},{"location":"generate-authentication-token.html#generating-an-authentication-token-in-amazon-aurora-dsql","title":"Generating an Authentication Token in Amazon Aurora DSQL","text":""},{"location":"generate-authentication-token.html#overview","title":"Overview","text":"<p>To connect to Amazon Aurora DSQL with a SQL client, generate an authentication token to use as the password. This token is used only for authenticating the connection. After the connection is established, the connection remains valid even if the authentication token expires.</p>"},{"location":"generate-authentication-token.html#token-expiration-and-duration","title":"Token Expiration and Duration","text":"<ul> <li>AWS Console: Token automatically expires in one hour by default</li> <li>CLI or SDKs: Default is 15 minutes</li> <li>Maximum duration: 604,800 seconds (one week)</li> </ul> <p>To connect to Aurora DSQL from your client again, you can use the same authentication token if it hasn't expired, or you can generate a new one.</p>"},{"location":"generate-authentication-token.html#prerequisites","title":"Prerequisites","text":"<p>To get started with generating a token: 1. Create an IAM policy 2. Create a cluster in Aurora DSQL</p> <p>At a minimum, you must have the IAM permissions for connecting to clusters, depending on which database role you use to connect.</p>"},{"location":"generate-authentication-token.html#using-the-aws-console-to-generate-an-authentication-token","title":"Using the AWS Console to Generate an Authentication Token","text":"<p>Aurora DSQL authenticates users with a token rather than a password. You can generate the token from the console.</p> <p>To generate an authentication token:</p> <ol> <li> <p>Sign in to the AWS Console and open the Aurora DSQL console at https://console.aws.amazon.com/dsql</p> </li> <li> <p>Choose the cluster ID of the cluster for which you want to generate an authentication token</p> </li> <li> <p>Choose Connect and then select Get Token</p> </li> <li> <p>Choose whether you want to connect as an <code>admin</code> or with a custom database role</p> </li> <li> <p>Copy the generated authentication token and use it for connecting with SQL clients</p> </li> </ol>"},{"location":"generate-authentication-token.html#using-aws-cloudshell-to-generate-an-authentication-token","title":"Using AWS CloudShell to Generate an Authentication Token","text":"<p>Before you can generate an authentication token using CloudShell, make sure that you have created an Aurora DSQL cluster.</p> <p>To generate an authentication token using CloudShell:</p> <ol> <li> <p>Sign in to the AWS Console and open the Aurora DSQL console at https://console.aws.amazon.com/dsql</p> </li> <li> <p>At the bottom left of the AWS console, choose CloudShell</p> </li> <li> <p>Run the following command to generate an authentication token for the <code>admin</code> role:</p> </li> </ol> <pre><code># Use `generate-db-connect-auth-token` if you are _not_ logging in as `admin` user\naws dsql generate-db-connect-admin-auth-token \\\n  --expires-in 3600 \\\n  --region us-east-1 \\\n  --hostname your_cluster_endpoint\n</code></pre> <p>Note: If you're not connecting as <code>admin</code>, use <code>generate-db-connect-auth-token</code> instead.</p> <ol> <li>Use the following command to use <code>psql</code> to start a connection to your cluster:</li> </ol> <pre><code>PGSSLMODE=require \\\npsql --dbname postgres \\\n  --username admin \\\n  --host cluster_endpoint\n</code></pre> <ol> <li> <p>When prompted for a password, paste the generated token</p> </li> <li> <p>Press Enter to see the PostgreSQL prompt: <code>postgres=&gt;</code></p> </li> </ol>"},{"location":"generate-authentication-token.html#using-the-aws-cli-to-generate-an-authentication-token","title":"Using the AWS CLI to Generate an Authentication Token","text":"<p>When your cluster is <code>ACTIVE</code>, you can generate an authentication token using the <code>aws dsql</code> command:</p> <ul> <li>Admin role: Use <code>generate-db-connect-admin-auth-token</code></li> <li>Custom database role: Use <code>generate-db-connect-auth-token</code></li> </ul> <p>The following example uses these attributes to generate an authentication token for the <code>admin</code> role: - your_cluster_endpoint: The endpoint of the cluster (format: <code>your_cluster_identifier.dsql.region.on.aws</code>) - region: The AWS Region, such as <code>us-east-2</code> or <code>us-east-1</code></p> <p>Linux and macOS: <pre><code># Use `generate-db-connect-auth-token` if you are _not_ logging in as `admin` user\naws dsql generate-db-connect-admin-auth-token \\\n  --region region \\\n  --expires-in 3600 \\\n  --hostname your_cluster_endpoint\n</code></pre></p> <p>Windows: <pre><code># Use `generate-db-connect-auth-token` if you are _not_ logging in as `admin` user\naws dsql generate-db-connect-admin-auth-token ^\n  --region=region ^\n  --expires-in=3600 ^\n  --hostname=your_cluster_endpoint\n</code></pre></p>"},{"location":"generate-authentication-token.html#using-the-sdks-to-generate-a-token","title":"Using the SDKs to Generate a Token","text":"<p>You can generate an authentication token for your cluster when it is in <code>ACTIVE</code> status. The SDK examples use the following attributes to generate an authentication token for the <code>admin</code> role:</p> <ul> <li>your_cluster_endpoint: The endpoint of your Aurora DSQL cluster (format: <code>your_cluster_identifier.dsql.region.on.aws</code>)</li> <li>region: The AWS Region in which your cluster is located</li> </ul>"},{"location":"generate-authentication-token.html#python-sdk","title":"Python SDK","text":"<p>You can generate the token in the following ways: - Admin role: Use <code>generate_db_connect_admin_auth_token</code> - Custom database role: Use <code>generate_connect_auth_token</code></p> <pre><code>import boto3\n\ndef generate_token(your_cluster_endpoint, region):\n    client = boto3.client(\"dsql\", region_name=region)\n    # use `generate_db_connect_auth_token` instead if you are not connecting as admin.\n    token = client.generate_db_connect_admin_auth_token(your_cluster_endpoint, region)\n    print(token)\n    return token\n</code></pre>"},{"location":"generate-authentication-token.html#c-sdk","title":"C++ SDK","text":"<p>You can generate the token in the following ways: - Admin role: Use <code>GenerateDBConnectAdminAuthToken</code> - Custom database role: Use <code>GenerateDBConnectAuthToken</code></p> <pre><code>#include &lt;aws/core/Aws.h&gt;\n#include &lt;aws/dsql/DSQLClient.h&gt;\n#include &lt;iostream&gt;\n\nusing namespace Aws;\nusing namespace Aws::DSQL;\n\nstd::string generateToken(String yourClusterEndpoint, String region) {\n    Aws::SDKOptions options;\n    Aws::InitAPI(options);\n    DSQLClientConfiguration clientConfig;\n    clientConfig.region = region;\n    DSQLClient client{clientConfig};\n    std::string token = \"\";\n\n    // If you are not using the admin role to connect, use GenerateDBConnectAuthToken instead\n    const auto presignedString = client.GenerateDBConnectAdminAuthToken(yourClusterEndpoint, region);\n    if (presignedString.IsSuccess()) {\n        token = presignedString.GetResult();\n    } else {\n        std::cerr &lt;&lt; \"Token generation failed.\" &lt;&lt; std::endl;\n    }\n\n    std::cout &lt;&lt; token &lt;&lt; std::endl;\n\n    Aws::ShutdownAPI(options);\n    return token;\n}\n</code></pre>"},{"location":"generate-authentication-token.html#javascript-sdk","title":"JavaScript SDK","text":"<p>You can generate the token in the following ways: - Admin role: Use <code>getDbConnectAdminAuthToken</code> - Custom database role: Use <code>getDbConnectAuthToken</code></p> <pre><code>import { DsqlSigner } from \"@aws-sdk/dsql-signer\";\n\nasync function generateToken(yourClusterEndpoint, region) {\n  const signer = new DsqlSigner({\n    hostname: yourClusterEndpoint,\n    region,\n  });\n  try {\n    // Use `getDbConnectAuthToken` if you are _not_ logging in as the `admin` user\n    const token = await signer.getDbConnectAdminAuthToken();\n    console.log(token);\n    return token;\n  } catch (error) {\n      console.error(\"Failed to generate token: \", error);\n      throw error;\n  }\n}\n</code></pre>"},{"location":"generate-authentication-token.html#java-sdk","title":"Java SDK","text":"<p>You can generate the token in the following ways: - Admin role: Use <code>generateDbConnectAdminAuthToken</code> - Custom database role: Use <code>generateDbConnectAuthToken</code></p> <pre><code>import software.amazon.awssdk.auth.credentials.DefaultCredentialsProvider;\nimport software.amazon.awssdk.services.dsql.DsqlUtilities;\nimport software.amazon.awssdk.regions.Region;\n\npublic class GenerateAuthToken { \n    public static String generateToken(String yourClusterEndpoint, Region region) {\n        DsqlUtilities utilities = DsqlUtilities.builder()\n                .region(region)\n                .credentialsProvider(DefaultCredentialsProvider.create())\n                .build();\n\n        // Use `generateDbConnectAuthToken` if you are _not_ logging in as `admin` user \n        String token = utilities.generateDbConnectAdminAuthToken(builder -&gt; {\n            builder.hostname(yourClusterEndpoint)\n                    .region(region);\n        });\n\n        System.out.println(token);\n        return token;\n    }\n}\n</code></pre>"},{"location":"generate-authentication-token.html#rust-sdk","title":"Rust SDK","text":"<p>You can generate the token in the following ways: - Admin role: Use <code>db_connect_admin_auth_token</code> - Custom database role: Use <code>db_connect_auth_token</code></p> <pre><code>use aws_config::{BehaviorVersion, Region};\nuse aws_sdk_dsql::auth_token::{AuthTokenGenerator, Config};\n\nasync fn generate_token(your_cluster_endpoint: String, region: String) -&gt; String {\n    let sdk_config = aws_config::load_defaults(BehaviorVersion::latest()).await;\n    let signer = AuthTokenGenerator::new(\n        Config::builder()\n            .hostname(&amp;your_cluster_endpoint)\n            .region(Region::new(region))\n            .build()\n            .unwrap(),\n    );\n\n    // Use `db_connect_auth_token` if you are _not_ logging in as `admin` user\n    let token = signer.db_connect_admin_auth_token(&amp;sdk_config).await.unwrap();\n    println!(\"{}\", token);\n    token.to_string()\n}\n</code></pre>"},{"location":"generate-authentication-token.html#ruby-sdk","title":"Ruby SDK","text":"<p>You can generate the token in the following ways: - Admin role: Use <code>generate_db_connect_admin_auth_token</code> - Custom database role: Use <code>generate_db_connect_auth_token</code></p> <pre><code>require 'aws-sdk-dsql'\n\ndef generate_token(your_cluster_endpoint, region)\n  credentials = Aws::SharedCredentials.new()\n\n  begin\n      token_generator = Aws::DSQL::AuthTokenGenerator.new({\n          :credentials =&gt; credentials\n      })\n\n      # if you're not using admin role, use generate_db_connect_auth_token instead\n      token = token_generator.generate_db_connect_admin_auth_token({\n          :endpoint =&gt; your_cluster_endpoint,\n          :region =&gt; region\n      })\n  rescue =&gt; error\n    puts error.full_message\n  end\nend\n</code></pre>"},{"location":"generate-authentication-token.html#net-sdk","title":".NET SDK","text":"<p>Note: The official SDK for .NET doesn't include a built-in API call to generate an authentication token for Aurora DSQL. Instead, you must use <code>DSQLAuthTokenGenerator</code>, which is a utility class.</p> <p>You can generate the token in the following ways: - Admin role: Use <code>DbConnectAdmin</code> - Custom database role: Use <code>DbConnect</code></p> <pre><code>using Amazon;\nusing Amazon.DSQL.Util;\nusing Amazon.Runtime;\n\nvar yourClusterEndpoint = \"insert-dsql-cluster-endpoint\";\n\nAWSCredentials credentials = FallbackCredentialsFactory.GetCredentials();\n\n// Use `DSQLAuthTokenGenerator.GenerateDbConnectAuthToken` if you are _not_ logging in as `admin` user\nvar token = DSQLAuthTokenGenerator.GenerateDbConnectAdminAuthToken(credentials, RegionEndpoint.USEast1, yourClusterEndpoint);\n\nConsole.WriteLine(token);\n</code></pre>"},{"location":"generate-authentication-token.html#golang-sdk","title":"Golang SDK","text":"<p>Note: The Golang SDK doesn't provide a built-in method for generating a pre-signed token. You must manually construct the signed request.</p> <p>In the following code example, specify the <code>action</code> based on the PostgreSQL user: - Admin role: Use the <code>DbConnectAdmin</code> action - Custom database role: Use the <code>DbConnect</code> action</p> <pre><code>import (\n    \"context\"\n    \"time\"\n\n    \"github.com/aws/aws-sdk-go-v2/config\"\n    \"github.com/aws/aws-sdk-go-v2/feature/dsql/auth\"\n)\n\nfunc GenerateDbConnectAdminAuthToken(ctx context.Context, clusterEndpoint, region string, expiry time.Duration) (string, error) {\n    cfg, err := config.LoadDefaultConfig(ctx)\n    if err != nil {\n        return \"\", err\n    }\n\n    tokenOptions := func(options *auth.TokenOptions) {\n        options.ExpiresIn = expiry\n    }\n\n    // Use `auth.GenerateDbConnectAuthToken` if you are _not_ logging in as `admin` user\n    token, err := auth.GenerateDBConnectAdminAuthToken(ctx, clusterEndpoint, region, cfg.Credentials, tokenOptions)\n    if err != nil {\n        return \"\", err\n    }\n\n    return token, nil\n}\n</code></pre>"},{"location":"managing-clusters.html","title":"Managing Clusters Overview","text":"\ud83d\udccb Copy Page"},{"location":"managing-clusters.html#managing-amazon-aurora-dsql-clusters","title":"Managing Amazon Aurora DSQL Clusters","text":""},{"location":"managing-clusters.html#overview","title":"Overview","text":"<p>Learn how to set up and optimize performance for your Aurora DSQL deployments. Aurora DSQL provides several configuration options to help you establish the right database infrastructure for your needs.</p>"},{"location":"managing-clusters.html#cluster-management-topics","title":"Cluster Management Topics","text":"<p>The features and functionality discussed in this guide ensure that your Aurora DSQL environment is more resilient, responsive, and capable of supporting your applications as they grow and evolve.</p>"},{"location":"managing-clusters.html#single-region-clusters","title":"Single-Region Clusters","text":"<p>Purpose: Deploy Aurora DSQL clusters within a single AWS Region</p> <p>Key Features: - Simplified deployment and management - Lower latency for regional applications - Cost-effective for single-region workloads - Automatic scaling within the region</p> <p>Use Cases: - Applications with users in a specific geographic region - Development and testing environments - Cost-sensitive workloads - Applications with strict data residency requirements</p>"},{"location":"managing-clusters.html#multi-region-clusters","title":"Multi-Region Clusters","text":"<p>Purpose: Deploy Aurora DSQL clusters across multiple AWS Regions</p> <p>Key Features: - High availability across regions - Disaster recovery capabilities - Global application support - Cross-region data replication</p> <p>Use Cases: - Global applications with worldwide users - Business continuity and disaster recovery - Compliance with data sovereignty requirements - High availability requirements</p>"},{"location":"managing-clusters.html#cloudformation-setup","title":"CloudFormation Setup","text":"<p>Purpose: Infrastructure as Code deployment for Aurora DSQL clusters</p> <p>Key Features: - Automated cluster provisioning - Repeatable deployments - Version-controlled infrastructure - Integration with AWS CloudFormation</p> <p>Use Cases: - Automated deployment pipelines - Consistent environment provisioning - Infrastructure version control - Large-scale deployments</p>"},{"location":"managing-clusters.html#cluster-lifecycle-management","title":"Cluster Lifecycle Management","text":"<p>Purpose: Manage Aurora DSQL clusters throughout their operational lifecycle</p> <p>Key Features: - Cluster creation and deletion - Configuration updates and modifications - Monitoring and maintenance - Performance optimization</p> <p>Use Cases: - Ongoing cluster maintenance - Performance tuning and optimization - Capacity planning and scaling - Operational monitoring</p>"},{"location":"managing-clusters.html#configuration-options","title":"Configuration Options","text":""},{"location":"managing-clusters.html#deployment-strategies","title":"Deployment Strategies","text":"<p>Single-Region Deployment: - Choose appropriate AWS Region based on user location - Configure cluster size based on expected workload - Set up monitoring and alerting - Plan for backup and recovery</p> <p>Multi-Region Deployment: - Select primary and secondary regions - Configure cross-region replication - Set up failover procedures - Plan for data consistency requirements</p>"},{"location":"managing-clusters.html#performance-optimization","title":"Performance Optimization","text":"<p>Cluster Sizing: - Assess application requirements - Plan for peak usage patterns - Consider growth projections - Monitor performance metrics</p> <p>Connection Management: - Implement connection pooling - Plan for connection limits - Configure timeout settings - Monitor connection usage</p>"},{"location":"managing-clusters.html#best-practices","title":"Best Practices","text":""},{"location":"managing-clusters.html#planning-and-design","title":"Planning and Design","text":"<ol> <li>Assess requirements before cluster creation</li> <li>Choose appropriate regions based on user distribution</li> <li>Plan for scalability and future growth</li> <li>Consider compliance and data residency requirements</li> </ol>"},{"location":"managing-clusters.html#operational-excellence","title":"Operational Excellence","text":"<ol> <li>Monitor cluster performance regularly</li> <li>Implement automated backups and recovery procedures</li> <li>Set up alerting for critical metrics</li> <li>Plan for maintenance windows and updates</li> </ol>"},{"location":"managing-clusters.html#security-and-compliance","title":"Security and Compliance","text":"<ol> <li>Configure appropriate access controls using IAM</li> <li>Implement encryption for data at rest and in transit</li> <li>Regular security audits and access reviews</li> <li>Compliance monitoring for regulatory requirements</li> </ol>"},{"location":"managing-clusters.html#related-documentation","title":"Related Documentation","text":"<ul> <li>Getting Started: Getting Started</li> <li>Authentication: Auth &amp; Access Overview</li> <li>Quotas and Limits: Quotas and Limits Overview</li> <li>Troubleshooting: Troubleshooting Overview</li> </ul>"},{"location":"multi-region-clusters.html","title":"Multi-Region Clusters","text":"\ud83d\udccb Copy Page"},{"location":"multi-region-clusters.html#configuring-multi-region-clusters","title":"Configuring Multi-Region Clusters","text":""},{"location":"multi-region-clusters.html#overview","title":"Overview","text":"<p>Learn how to work with clusters that span multiple AWS Regions. Configure and manage clusters across multiple AWS Regions using either the CLI or your preferred programming language including Python, C++, JavaScript, Java, Rust, Ruby, .NET, and Golang.</p>"},{"location":"multi-region-clusters.html#multi-region-cluster-concepts","title":"Multi-Region Cluster Concepts","text":""},{"location":"multi-region-clusters.html#witness-region","title":"Witness Region","text":"<p>A witness region is a third AWS Region that helps maintain consistency and availability for your multi-region cluster. The witness region doesn't host client endpoints but maintains transaction logs to support quorum decisions.</p>"},{"location":"multi-region-clusters.html#cluster-linking","title":"Cluster Linking","text":"<p>Multi-region clusters are created by linking two clusters in different regions. This creates a synchronized, distributed database that spans multiple geographic locations.</p>"},{"location":"multi-region-clusters.html#high-availability","title":"High Availability","text":"<p>Multi-region clusters provide enhanced availability and disaster recovery capabilities by maintaining synchronized data across multiple AWS Regions.</p>"},{"location":"multi-region-clusters.html#using-aws-sdks","title":"Using AWS SDKs","text":"<p>The AWS SDKs provide programmatic access to Aurora DSQL in your preferred programming language. The following sections show how to perform common multi-region cluster operations.</p>"},{"location":"multi-region-clusters.html#create-multi-region-cluster","title":"Create Multi-Region Cluster","text":"<p>The following examples show how to create a multi-Region cluster using different programming languages.</p>"},{"location":"multi-region-clusters.html#python-sdk","title":"Python SDK","text":"<pre><code>import boto3\n\ndef create_multi_region_clusters(region_1, region_2, witness_region):\n    try:\n        client_1 = boto3.client(\"dsql\", region_name=region_1)\n        client_2 = boto3.client(\"dsql\", region_name=region_2)\n\n        # We can only set the witness region for the first cluster\n        cluster_1 = client_1.create_cluster(\n            deletionProtectionEnabled=True,\n            multiRegionProperties={\"witnessRegion\": witness_region},\n            tags={\"Name\": \"Python multi region cluster\"}\n        )\n        print(f\"Created {cluster_1['arn']}\")\n\n        # For the second cluster we can set witness region and designate cluster_1 as a peer\n        cluster_2 = client_2.create_cluster(\n            deletionProtectionEnabled=True,\n            multiRegionProperties={\"witnessRegion\": witness_region, \"clusters\": [cluster_1[\"arn\"]]},\n            tags={\"Name\": \"Python multi region cluster\"}\n        )\n        print(f\"Created {cluster_2['arn']}\")\n\n        # Now that we know the cluster_2 arn we can set it as a peer of cluster_1\n        client_1.update_cluster(\n            identifier=cluster_1[\"identifier\"],\n            multiRegionProperties={\"witnessRegion\": witness_region, \"clusters\": [cluster_2[\"arn\"]]}\n        )\n        print(f\"Added {cluster_2['arn']} as a peer of {cluster_1['arn']}\")\n\n        # Now that multiRegionProperties is fully defined for both clusters they'll begin the transition to ACTIVE\n        print(f\"Waiting for {cluster_1['arn']} to become ACTIVE\")\n        client_1.get_waiter(\"cluster_active\").wait(\n            identifier=cluster_1[\"identifier\"],\n            WaiterConfig={'Delay': 10, 'MaxAttempts': 30}\n        )\n\n        print(f\"Waiting for {cluster_2['arn']} to become ACTIVE\")\n        client_2.get_waiter(\"cluster_active\").wait(\n            identifier=cluster_2[\"identifier\"],\n            WaiterConfig={'Delay': 10, 'MaxAttempts': 30}\n        )\n\n        return (cluster_1, cluster_2)\n    except:\n        print(\"Unable to create cluster\")\n        raise\n\ndef main():\n    region_1 = \"us-east-1\"\n    region_2 = \"us-east-2\"\n    witness_region = \"us-west-2\"\n    (cluster_1, cluster_2) = create_multi_region_clusters(region_1, region_2, witness_region)\n    print(\"Created multi region clusters:\")\n    print(\"Cluster id: \" + cluster_1['arn'])\n    print(\"Cluster id: \" + cluster_2['arn'])\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"multi-region-clusters.html#javascript-sdk","title":"JavaScript SDK","text":"<pre><code>import { DSQLClient, CreateClusterCommand, UpdateClusterCommand, waitUntilClusterActive } from \"@aws-sdk/client-dsql\";\n\nasync function createMultiRegionCluster(region1, region2, witnessRegion) {\n    const client1 = new DSQLClient({ region: region1 });\n    const client2 = new DSQLClient({ region: region2 });\n\n    try {\n        // We can only set the witness region for the first cluster\n        console.log(`Creating cluster in ${region1}`);\n        const createClusterCommand1 = new CreateClusterCommand({\n            deletionProtectionEnabled: true,\n            tags: { Name: \"javascript multi region cluster 1\" },\n            multiRegionProperties: { witnessRegion: witnessRegion }\n        });\n        const response1 = await client1.send(createClusterCommand1);\n        console.log(`Created ${response1.arn}`);\n\n        // For the second cluster we can set witness region and designate the first cluster as a peer\n        console.log(`Creating cluster in ${region2}`);\n        const createClusterCommand2 = new CreateClusterCommand({\n            deletionProtectionEnabled: true,\n            tags: { Name: \"javascript multi region cluster 2\" },\n            multiRegionProperties: {\n                witnessRegion: witnessRegion,\n                clusters: [response1.arn]\n            }\n        });\n        const response2 = await client2.send(createClusterCommand2);\n        console.log(`Created ${response2.arn}`);\n\n        // Now that we know the second cluster arn we can set it as a peer of the first cluster\n        const updateClusterCommand = new UpdateClusterCommand({\n            identifier: response1.identifier,\n            multiRegionProperties: {\n                witnessRegion: witnessRegion,\n                clusters: [response2.arn]\n            }\n        });\n        await client1.send(updateClusterCommand);\n        console.log(`Added ${response2.arn} as a peer of ${response1.arn}`);\n\n        // Now that multiRegionProperties is fully defined for both clusters they'll begin the transition to ACTIVE\n        console.log(`Waiting for cluster ${response1.identifier} to become ACTIVE`);\n        await waitUntilClusterActive(\n            { client: client1, maxWaitTime: 300 },\n            { identifier: response1.identifier }\n        );\n        console.log(`Cluster 1 is now active`);\n\n        console.log(`Waiting for cluster ${response2.identifier} to become ACTIVE`);\n        await waitUntilClusterActive(\n            { client: client2, maxWaitTime: 300 },\n            { identifier: response2.identifier }\n        );\n        console.log(`Cluster 2 is now active`);\n        console.log(\"The multi region clusters are now active\");\n        return;\n    } catch (error) {\n        console.error(\"Failed to create cluster: \", error.message);\n        throw error;\n    }\n}\n\nasync function main() {\n    const region1 = \"us-east-1\";\n    const region2 = \"us-east-2\";\n    const witnessRegion = \"us-west-2\";\n\n    await createMultiRegionCluster(region1, region2, witnessRegion);\n}\n\nmain();\n</code></pre>"},{"location":"multi-region-clusters.html#java-sdk","title":"Java SDK","text":"<pre><code>import software.amazon.awssdk.auth.credentials.DefaultCredentialsProvider;\nimport software.amazon.awssdk.regions.Region;\nimport software.amazon.awssdk.services.dsql.DsqlClient;\nimport software.amazon.awssdk.services.dsql.DsqlClientBuilder;\nimport software.amazon.awssdk.services.dsql.model.CreateClusterRequest;\nimport software.amazon.awssdk.services.dsql.model.CreateClusterResponse;\nimport software.amazon.awssdk.services.dsql.model.GetClusterResponse;\nimport software.amazon.awssdk.services.dsql.model.UpdateClusterRequest;\n\nimport java.time.Duration;\nimport java.util.Map;\n\npublic class CreateMultiRegionCluster {\n    public static void main(String[] args) {\n        Region region1 = Region.US_EAST_1;\n        Region region2 = Region.US_EAST_2;\n        Region witnessRegion = Region.US_WEST_2;\n\n        DsqlClientBuilder clientBuilder = DsqlClient.builder()\n                .credentialsProvider(DefaultCredentialsProvider.create());\n\n        try (DsqlClient client1 = clientBuilder.region(region1).build();\n             DsqlClient client2 = clientBuilder.region(region2).build()) {\n\n            // We can only set the witness region for the first cluster\n            System.out.println(\"Creating cluster in \" + region1);\n            CreateClusterRequest request1 = CreateClusterRequest.builder()\n                    .deletionProtectionEnabled(true)\n                    .multiRegionProperties(mrp -&gt; mrp.witnessRegion(witnessRegion.toString()))\n                    .tags(Map.of(\"Name\", \"java multi region cluster\"))\n                    .build();\n            CreateClusterResponse cluster1 = client1.createCluster(request1);\n            System.out.println(\"Created \" + cluster1.arn());\n\n            // For the second cluster we can set the witness region and designate cluster1 as a peer\n            System.out.println(\"Creating cluster in \" + region2);\n            CreateClusterRequest request2 = CreateClusterRequest.builder()\n                    .deletionProtectionEnabled(true)\n                    .multiRegionProperties(mrp -&gt;\n                            mrp.witnessRegion(witnessRegion.toString()).clusters(cluster1.arn())\n                    )\n                    .tags(Map.of(\"Name\", \"java multi region cluster\"))\n                    .build();\n            CreateClusterResponse cluster2 = client2.createCluster(request2);\n            System.out.println(\"Created \" + cluster2.arn());\n\n            // Now that we know the cluster2 ARN we can set it as a peer of cluster1\n            UpdateClusterRequest updateReq = UpdateClusterRequest.builder()\n                    .identifier(cluster1.identifier())\n                    .multiRegionProperties(mrp -&gt;\n                            mrp.witnessRegion(witnessRegion.toString()).clusters(cluster2.arn())\n                    )\n                    .build();\n            client1.updateCluster(updateReq);\n            System.out.printf(\"Added %s as a peer of %s%n\", cluster2.arn(), cluster1.arn());\n\n            // Wait for both clusters to become ACTIVE\n            System.out.printf(\"Waiting for cluster %s to become ACTIVE%n\", cluster1.arn());\n            GetClusterResponse activeCluster1 = client1.waiter().waitUntilClusterActive(\n                    getCluster -&gt; getCluster.identifier(cluster1.identifier()),\n                    config -&gt; config.waitTimeout(Duration.ofMinutes(5))\n            ).matched().response().orElseThrow();\n\n            System.out.printf(\"Waiting for cluster %s to become ACTIVE%n\", cluster2.arn());\n            GetClusterResponse activeCluster2 = client2.waiter().waitUntilClusterActive(\n                    getCluster -&gt; getCluster.identifier(cluster2.identifier()),\n                    config -&gt; config.waitTimeout(Duration.ofMinutes(5))\n            ).matched().response().orElseThrow();\n\n            System.out.println(\"Created multi region clusters:\");\n            System.out.println(activeCluster1);\n            System.out.println(activeCluster2);\n        }\n    }\n}\n</code></pre>"},{"location":"multi-region-clusters.html#ruby-sdk","title":"Ruby SDK","text":"<pre><code>require \"aws-sdk-dsql\"\nrequire \"pp\"\n\ndef create_multi_region_clusters(region_1, region_2, witness_region)\n  client_1 = Aws::DSQL::Client.new(region: region_1)\n  client_2 = Aws::DSQL::Client.new(region: region_2)\n\n  # We can only set the witness region for the first cluster\n  puts \"Creating cluster in #{region_1}\"\n  cluster_1 = client_1.create_cluster(\n    deletion_protection_enabled: true,\n    multi_region_properties: { witness_region: witness_region },\n    tags: { Name: \"ruby multi region cluster\" }\n  )\n  puts \"Created #{cluster_1.arn}\"\n\n  # For the second cluster we can set witness region and designate cluster_1 as a peer\n  puts \"Creating cluster in #{region_2}\"\n  cluster_2 = client_2.create_cluster(\n    deletion_protection_enabled: true,\n    multi_region_properties: {\n      witness_region: witness_region,\n      clusters: [ cluster_1.arn ]\n    },\n    tags: { Name: \"ruby multi region cluster\" }\n  )\n  puts \"Created #{cluster_2.arn}\"\n\n  # Now that we know the cluster_2 arn we can set it as a peer of cluster_1\n  client_1.update_cluster(\n    identifier: cluster_1.identifier,\n    multi_region_properties: {\n      witness_region: witness_region,\n      clusters: [ cluster_2.arn ]\n    }\n  )\n  puts \"Added #{cluster_2.arn} as a peer of #{cluster_1.arn}\"\n\n  # Now that multi_region_properties is fully defined for both clusters they'll begin the transition to ACTIVE\n  puts \"Waiting for #{cluster_1.arn} to become ACTIVE\"\n  cluster_1 = client_1.wait_until(:cluster_active, identifier: cluster_1.identifier) do |w|\n    w.max_attempts = 30\n    w.delay = 10\n  end\n\n  puts \"Waiting for #{cluster_2.arn} to become ACTIVE\"\n  cluster_2 = client_2.wait_until(:cluster_active, identifier: cluster_2.identifier) do |w|\n    w.max_attempts = 30\n    w.delay = 10\n  end\n\n  [ cluster_1, cluster_2 ]\nrescue Aws::Errors::ServiceError =&gt; e\n  abort \"Failed to create multi-region clusters: #{e.message}\"\nend\n\ndef main\n  region_1 = \"us-east-1\"\n  region_2 = \"us-east-2\"\n  witness_region = \"us-west-2\"\n\n  cluster_1, cluster_2 = create_multi_region_clusters(region_1, region_2, witness_region)\n\n  puts \"Created multi region clusters:\"\n  pp cluster_1\n  pp cluster_2\nend\n\nmain if $PROGRAM_NAME == __FILE__\n</code></pre>"},{"location":"multi-region-clusters.html#get-multi-region-cluster-information","title":"Get Multi-Region Cluster Information","text":""},{"location":"multi-region-clusters.html#python-sdk_1","title":"Python SDK","text":"<pre><code>import boto3\nfrom datetime import datetime\nimport json\n\ndef get_cluster(region, identifier):\n    try:\n        client = boto3.client(\"dsql\", region_name=region)\n        return client.get_cluster(identifier=identifier)\n    except:\n        print(f\"Unable to get cluster {identifier} in region {region}\")\n        raise\n\ndef main():\n    region = \"us-east-1\"\n    cluster_id = \"&lt;your cluster id&gt;\"\n    response = get_cluster(region, cluster_id)\n    print(json.dumps(response, indent=2, default=lambda obj: obj.isoformat() if isinstance(obj, datetime) else None))\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"multi-region-clusters.html#update-multi-region-cluster","title":"Update Multi-Region Cluster","text":""},{"location":"multi-region-clusters.html#python-sdk_2","title":"Python SDK","text":"<pre><code>import boto3\n\ndef update_cluster(region, cluster_id, deletion_protection_enabled):\n    try:\n        client = boto3.client(\"dsql\", region_name=region)\n        return client.update_cluster(identifier=cluster_id, deletionProtectionEnabled=deletion_protection_enabled)\n    except:\n        print(\"Unable to update cluster\")\n        raise\n\ndef main():\n    region = \"us-east-1\"\n    cluster_id = \"&lt;your cluster id&gt;\"\n    deletion_protection_enabled = False\n    response = update_cluster(region, cluster_id, deletion_protection_enabled)\n    print(f\"Updated {response['arn']} with deletion_protection_enabled: {deletion_protection_enabled}\")\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"multi-region-clusters.html#delete-multi-region-clusters","title":"Delete Multi-Region Clusters","text":""},{"location":"multi-region-clusters.html#python-sdk_3","title":"Python SDK","text":"<pre><code>import boto3\n\ndef delete_multi_region_clusters(region_1, cluster_id_1, region_2, cluster_id_2):\n    try:\n        client_1 = boto3.client(\"dsql\", region_name=region_1)\n        client_2 = boto3.client(\"dsql\", region_name=region_2)\n\n        client_1.delete_cluster(identifier=cluster_id_1)\n        print(f\"Deleting cluster {cluster_id_1} in {region_1}\")\n\n        # cluster_1 will stay in PENDING_DELETE state until cluster_2 is deleted\n        client_2.delete_cluster(identifier=cluster_id_2)\n        print(f\"Deleting cluster {cluster_id_2} in {region_2}\")\n\n        # Now that both clusters have been marked for deletion they will transition to DELETING state and finalize deletion\n        print(f\"Waiting for {cluster_id_1} to finish deletion\")\n        client_1.get_waiter(\"cluster_not_exists\").wait(\n            identifier=cluster_id_1,\n            WaiterConfig={'Delay': 10, 'MaxAttempts': 30}\n        )\n\n        print(f\"Waiting for {cluster_id_2} to finish deletion\")\n        client_2.get_waiter(\"cluster_not_exists\").wait(\n            identifier=cluster_id_2,\n            WaiterConfig={'Delay': 10, 'MaxAttempts': 30}\n        )\n    except:\n        print(\"Unable to delete cluster\")\n        raise\n\ndef main():\n    region_1 = \"us-east-1\"\n    cluster_id_1 = \"&lt;cluster 1 id&gt;\"\n    region_2 = \"us-east-2\"\n    cluster_id_2 = \"&lt;cluster 2 id&gt;\"\n\n    delete_multi_region_clusters(region_1, cluster_id_1, region_2, cluster_id_2)\n    print(f\"Deleted {cluster_id_1} in {region_1} and {cluster_id_2} in {region_2}\")\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"multi-region-clusters.html#additional-sdk-examples","title":"Additional SDK Examples","text":"<p>For complete SDK examples in all supported languages (C++, Rust, .NET, Golang), visit the Aurora DSQL Samples GitHub repository.</p>"},{"location":"multi-region-clusters.html#multi-region-cluster-management","title":"Multi-Region Cluster Management","text":""},{"location":"multi-region-clusters.html#key-concepts","title":"Key Concepts","text":"<p>Witness Region: Third region that maintains transaction logs for quorum decisions Cluster Linking: Process of connecting clusters across regions Synchronization: Automatic data replication between linked clusters Failover: Automatic switching between regions during outages</p>"},{"location":"multi-region-clusters.html#best-practices","title":"Best Practices","text":"<p>Region Selection: - Choose regions close to your user base - Consider data residency requirements - Plan for disaster recovery scenarios - Ensure witness region is geographically separate</p> <p>Performance Considerations: - Account for cross-region latency - Plan for eventual consistency during network partitions - Monitor replication lag between regions - Design applications for multi-region architecture</p> <p>Security Considerations: - Configure IAM policies for cross-region access - Ensure encryption in transit between regions - Plan for compliance requirements across regions - Monitor access patterns across regions</p>"},{"location":"multi-region-clusters.html#operational-considerations","title":"Operational Considerations","text":""},{"location":"multi-region-clusters.html#cluster-states","title":"Cluster States","text":"<ul> <li>CREATING: Cluster is being provisioned</li> <li>ACTIVE: Cluster is ready for connections</li> <li>UPDATING: Cluster configuration is being modified</li> <li>DELETING: Cluster is being removed</li> <li>PENDING_DELETE: Multi-region cluster waiting for peer deletion</li> </ul>"},{"location":"multi-region-clusters.html#deletion-process","title":"Deletion Process","text":"<p>For multi-region clusters, both clusters must be deleted. The first cluster will remain in PENDING_DELETE state until the second cluster is also deleted.</p>"},{"location":"multi-region-clusters.html#monitoring","title":"Monitoring","text":"<ul> <li>Monitor cluster status in both regions</li> <li>Track replication metrics</li> <li>Set up alerts for failover events</li> <li>Monitor cross-region network connectivity</li> </ul>"},{"location":"multi-region-clusters.html#related-documentation","title":"Related Documentation","text":"<ul> <li>Single-Region Clusters: Single-Region Clusters</li> <li>Getting Started: Getting Started</li> <li>Authentication: Auth &amp; Access Overview</li> <li>Troubleshooting: Troubleshooting Overview</li> </ul>"},{"location":"programming-with-dsql.html","title":"Programming with DSQL Overview","text":"\ud83d\udccb Copy Page"},{"location":"programming-with-dsql.html#programming-with-amazon-aurora-dsql","title":"Programming with Amazon Aurora DSQL","text":""},{"location":"programming-with-dsql.html#overview","title":"Overview","text":"<p>Aurora DSQL provides you with the following tools to manage your Aurora DSQL resources programmatically and connect to your databases.</p>"},{"location":"programming-with-dsql.html#programmatic-access-tools","title":"Programmatic Access Tools","text":""},{"location":"programming-with-dsql.html#aws-command-line-interface-cli","title":"AWS Command Line Interface (CLI)","text":"<p>You can create and manage your resources by using the CLI in a command-line shell. The CLI provides direct access to the APIs for AWS services, such as Aurora DSQL.</p> <p>Documentation: AWS CLI Command Reference for DSQL</p>"},{"location":"programming-with-dsql.html#aws-software-development-kits-sdks","title":"AWS Software Development Kits (SDKs)","text":"<p>AWS provides SDKs for many popular technologies and programming languages. They make it easier for you to call AWS services from within your applications in that language or technology.</p> <p>Documentation: Tools for developing and managing applications on AWS</p>"},{"location":"programming-with-dsql.html#aurora-dsql-api","title":"Aurora DSQL API","text":"<p>This API is another programming interface for Aurora DSQL. When using this API, you must format every HTTPS request correctly and add a valid digital signature to every request.</p> <p>Documentation: Aurora DSQL API Reference</p>"},{"location":"programming-with-dsql.html#aws-cloudformation","title":"AWS CloudFormation","text":"<p>The AWS::DSQL::Cluster is a CloudFormation resource that enables you to create and manage Aurora DSQL clusters as part of your infrastructure as code.</p> <p>CloudFormation helps you define your entire AWS environment in code, making it easier to provision, update, and replicate your infrastructure in a consistent and reliable way. When you use the AWS::DSQL::Cluster resource in your CloudFormation templates, you can declaratively provision Aurora DSQL clusters alongside your other cloud resources.</p>"},{"location":"programming-with-dsql.html#accessing-aurora-dsql-with-postgresql-compatible-clients","title":"Accessing Aurora DSQL with PostgreSQL-Compatible Clients","text":"<p>Aurora DSQL uses the PostgreSQL wire protocol. You can connect to Aurora DSQL using a variety of tools and clients, such as CloudShell, psql, DBeaver, and DataGrip.</p>"},{"location":"programming-with-dsql.html#connection-parameter-mapping","title":"Connection Parameter Mapping","text":"PostgreSQL Aurora DSQL Notes Role (User or Group) Database Role Aurora DSQL creates a role named <code>admin</code>. Custom database roles must be associated with IAM roles for authentication. Host (hostname) Cluster Endpoint Single-Region clusters provide a single managed endpoint with automatic traffic redirection. Port Default <code>5432</code> Uses the PostgreSQL default port. Database (dbname) <code>postgres</code> Aurora DSQL creates this database when you create the cluster. SSL Mode SSL always enabled Aurora DSQL supports <code>require</code> SSL Mode. Connections without SSL are rejected. Password Authentication Token Aurora DSQL requires temporary authentication tokens instead of long-lived passwords."},{"location":"programming-with-dsql.html#authentication-requirements","title":"Authentication Requirements","text":"<p>When connecting, Aurora DSQL requires a signed IAM authentication token in place of a traditional password. These temporary tokens are generated using AWS Signature Version 4 and are used only during connection establishment. Once connected, the session remains active until it ends or the client disconnects.</p> <p>If you attempt to open a new session with an expired token, the connection request fails and a new token must be generated.</p>"},{"location":"programming-with-dsql.html#sql-client-access","title":"SQL Client Access","text":"<p>Aurora DSQL supports multiple PostgreSQL-compatible clients for connecting to your cluster. Each client requires a valid authentication token.</p>"},{"location":"programming-with-dsql.html#using-cloudshell-with-psql","title":"Using CloudShell with psql","text":"<p>Use the following procedure to access Aurora DSQL with the PostgreSQL interactive terminal from CloudShell.</p> <p>Steps: 1. Sign in to the Aurora DSQL console 2. Choose the cluster you want to connect to 3. Choose Connect with Query Editor and then Connect with CloudShell 4. Choose whether to connect as admin or with a custom database role 5. Choose Launch in CloudShell and Run in the CloudShell dialog</p>"},{"location":"programming-with-dsql.html#using-local-cli-with-psql","title":"Using Local CLI with psql","text":"<p>Use <code>psql</code>, a terminal-based front-end to PostgreSQL utility, to interactively enter queries and view results.</p> <p>Note: To improve query response times, use PostgreSQL version 17 client. Ensure you have Python version 3.8+ and psql version 14+.</p> <p>Connection Example: <pre><code># Aurora DSQL requires a valid IAM token as the password when connecting\n# Generate authentication token using AWS CLI\nexport PGPASSWORD=$(aws dsql generate-db-connect-admin-auth-token \\\n  --region us-east-1 \\\n  --expires-in 3600 \\\n  --hostname your_cluster_endpoint)\n\n# Aurora DSQL requires SSL and will reject connections without it\nexport PGSSLMODE=require\n\n# Connect with psql using the environment variables\npsql --quiet \\\n  --username admin \\\n  --dbname postgres \\\n  --host your_cluster_endpoint\n</code></pre></p>"},{"location":"programming-with-dsql.html#using-dbeaver","title":"Using DBeaver","text":"<p>DBeaver is an open-source, GUI-based database tool for connecting to and managing your database.</p> <p>Setup Steps: 1. Choose New Database Connection 2. Select PostgreSQL 3. In Connection settings/Main tab:    - Host: Your cluster endpoint    - Database: <code>postgres</code>    - Authentication: <code>Database Native</code>    - Username: <code>admin</code>    - Password: Generate authentication token 4. Configure SSL mode (<code>PGSSLMODE=require</code> or <code>PGSSLMODE=verify-full</code>) 5. Test connection and begin running SQL statements</p> <p>Important: Administrative features like Session Manager and Lock Manager don't apply to Aurora DSQL due to its unique architecture.</p>"},{"location":"programming-with-dsql.html#using-jetbrains-datagrip","title":"Using JetBrains DataGrip","text":"<p>DataGrip is a cross-platform IDE for working with SQL and databases, including PostgreSQL.</p> <p>Setup Steps: 1. Choose New Data Source and select PostgreSQL 2. In Data Sources/General tab:    - Host: Your cluster endpoint    - Port: <code>5432</code>    - Database: <code>postgres</code>    - Authentication: <code>User &amp; Password</code>    - Username: <code>admin</code>    - Password: Generate authentication token 3. Configure SSL mode in connection settings 4. Test connection and start running SQL statements</p> <p>Important: Some views like Sessions don't apply to Aurora DSQL due to its unique architecture.</p>"},{"location":"programming-with-dsql.html#database-connectivity-tools","title":"Database Connectivity Tools","text":"<p>AWS provides various tools for connecting to and working with Aurora DSQL databases, including database drivers, ORM libraries, and specialized adapters.</p>"},{"location":"programming-with-dsql.html#database-drivers","title":"Database Drivers","text":"<p>Low-level libraries that directly connect to the database:</p> Programming Language Driver Sample Repository C++ libpq C++ libpq samples C# (.NET) Npgsql .NET Npgsql samples Go pgx Go pgx samples Java pgJDBC Java pgJDBC samples Java Aurora DSQL Connector for JDBC JDBC Connector JavaScript node-postgres Node.js postgres samples JavaScript Postgres.js Postgres.js samples Python Psycopg Python Psycopg samples Python Psycopg2 Python Psycopg2 samples Ruby pg Ruby pg samples Rust SQLx Rust SQLx samples"},{"location":"programming-with-dsql.html#object-relational-mapping-orm-libraries","title":"Object-Relational Mapping (ORM) Libraries","text":"<p>Standalone libraries that provide object-relational mapping functionality:</p> Programming Language ORM Library Sample Repository Java Hibernate Hibernate Pet Clinic App Python SQLAlchemy SQLAlchemy Pet Clinic App TypeScript Sequelize TypeScript Sequelize samples TypeScript TypeORM TypeScript TypeORM samples"},{"location":"programming-with-dsql.html#aurora-dsql-adapters-and-dialects","title":"Aurora DSQL Adapters and Dialects","text":"<p>Specific extensions that make existing ORMs work with Aurora DSQL:</p> Programming Language ORM/Framework Repository Java Hibernate Aurora DSQL Hibernate Adapter Python Django Aurora DSQL Django Adapter Python SQLAlchemy Aurora DSQL SQLAlchemy Adapter"},{"location":"programming-with-dsql.html#connection-troubleshooting","title":"Connection Troubleshooting","text":""},{"location":"programming-with-dsql.html#authentication-token-expiration","title":"Authentication Token Expiration","text":"<p>Behavior: Established sessions remain authenticated for a maximum of 1 hour or until an explicit disconnect or client-side timeout occurs.</p> <p>Important Considerations: - If new connections need to be established, a new authentication token must be generated - Opening a new session (listing tables, new SQL console) forces a new authentication attempt - If the authentication token is no longer valid, new sessions will fail and all previously opened sessions become invalid - Plan token duration carefully using the <code>expires-in</code> option (15 minutes default, maximum 7 days)</p>"},{"location":"programming-with-dsql.html#ssl-requirements","title":"SSL Requirements","text":"<p>SSL Mode Support: - <code>PGSSLMODE=require</code>: Basic SSL encryption - <code>PGSSLMODE=verify-full</code>: SSL with certificate verification</p> <p>Important: Aurora DSQL enforces SSL communication on the server side and rejects non-SSL connections. For <code>verify-full</code> option, you need to install SSL certificates locally.</p>"},{"location":"programming-with-dsql.html#related-documentation","title":"Related Documentation","text":"<ul> <li>Authentication Tokens: Generate Authentication Token</li> <li>Database Roles: Database Roles and IAM Authentication</li> <li>Getting Started: Getting Started</li> <li>Troubleshooting: Troubleshooting Overview</li> <li>SSL Certificates: SSL/TLS certificates configuration</li> </ul>"},{"location":"quotas-and-limits.html","title":"Quotas and Limits Overview","text":"\ud83d\udccb Copy Page"},{"location":"quotas-and-limits.html#cluster-quotas-and-database-limits-in-amazon-aurora-dsql","title":"Cluster Quotas and Database Limits in Amazon Aurora DSQL","text":""},{"location":"quotas-and-limits.html#overview","title":"Overview","text":"<p>This guide describes the cluster quotas and database limits for Amazon Aurora DSQL. Understanding these limits is essential for planning your Aurora DSQL deployment and application design.</p>"},{"location":"quotas-and-limits.html#cluster-quotas","title":"Cluster Quotas","text":"<p>Your AWS account has the following cluster quotas in Aurora DSQL. To request an increase to the service quotas for single-Region and multi-Region clusters within a specific AWS Region, use the Service Quotas console page. For other quota increases, contact AWS Support.</p>"},{"location":"quotas-and-limits.html#single-region-clusters","title":"Single-Region Clusters","text":"Quota Default Limit Configurable Error Code Error Message Maximum single-Region clusters per AWS account 20 clusters Yes <code>ServiceQuotaExceededException : 402</code> <code>You have reached the cluster limit.</code>"},{"location":"quotas-and-limits.html#multi-region-clusters","title":"Multi-Region Clusters","text":"Quota Default Limit Configurable Error Code Error Message Maximum multi-Region clusters per AWS account 5 clusters Yes <code>ServiceQuotaExceededException : 402</code> <code>You have reached the cluster limit.</code>"},{"location":"quotas-and-limits.html#storage-quotas","title":"Storage Quotas","text":"Quota Default Limit Configurable Error Code Error Message Maximum storage per cluster 10 TiB (up to 256 TiB with approved increase) Yes <code>DISK_FULL(53100)</code> <code>Current cluster size exceeds cluster size limit.</code>"},{"location":"quotas-and-limits.html#connection-quotas","title":"Connection Quotas","text":"Quota Default Limit Configurable Error Code Error Message Maximum connections per cluster 10,000 connections Yes <code>TOO_MANY_CONNECTIONS(53300)</code> <code>Unable to accept connection, too many open connections.</code> Maximum connection rate per cluster 100 connections per second No <code>CONFIGURED_LIMIT_EXCEEDED(53400)</code> <code>Unable to accept connection, rate exceeded.</code> Maximum connection burst capacity per cluster 1,000 connections No No error code No error message Connection refill rate 100 connections per second No No error code No error message Maximum connection duration 60 minutes No No error code No error message"},{"location":"quotas-and-limits.html#operational-quotas","title":"Operational Quotas","text":"Quota Default Limit Configurable Error Code Error Message Maximum concurrent restore jobs 4 No No error code No error message"},{"location":"quotas-and-limits.html#database-limits","title":"Database Limits","text":"<p>The following table describes the database limits in Aurora DSQL.</p>"},{"location":"quotas-and-limits.html#table-and-column-limits","title":"Table and Column Limits","text":"Limit Default Value Configurable Error Code Error Message Maximum combined size of columns in primary key 1 KiB No <code>54000</code> <code>ERROR: key size too large</code> Maximum combined size of columns in secondary index 1 KiB No <code>54000</code> <code>ERROR: key size too large</code> Maximum size of a row in a table 2 MiB No <code>54000</code> <code>ERROR: maximum row size exceeded</code> Maximum size of a column (not part of index) 1 MiB No <code>54000</code> <code>ERROR: maximum column size exceeded</code> Maximum number of columns in primary key or secondary index 8 No <code>54011</code> <code>ERROR: more than 8 column keys in an index are not supported</code> Maximum number of columns in a table 255 No <code>54011</code> <code>ERROR: tables can have at most 255 columns</code> Maximum number of indexes in a table 24 No <code>54000</code> <code>ERROR: more than 24 indexes per table are not allowed</code>"},{"location":"quotas-and-limits.html#database-structure-limits","title":"Database Structure Limits","text":"Limit Default Value Configurable Error Code Error Message Maximum number of schemas in a database 10 No <code>54000</code> <code>ERROR: more than 10 schemas not allowed</code> Maximum number of tables in a database 1,000 tables No <code>54000</code> <code>ERROR: creating more than 1000 tables not allowed</code> Maximum number of databases in a cluster 1 No No error code <code>ERROR: unsupported statement</code> Maximum number of views in a database 5,000 No <code>54000</code> <code>ERROR: creating more than 5000 views not allowed</code> Maximum view definition size 2 MiB No <code>54000</code> <code>ERROR: view definition too large</code>"},{"location":"quotas-and-limits.html#transaction-limits","title":"Transaction Limits","text":"Limit Default Value Configurable Error Code Error Message Maximum size of all data modified in write transaction 10 MiB No <code>54000</code> <code>ERROR: transaction size limit 10mb exceeded DETAIL: Current transaction size {sizemb} 10mb</code> Maximum number of rows mutated per transaction 3,000 rows No <code>54000</code> <code>ERROR: transaction row limit exceeded</code> Maximum transaction time 5 minutes No <code>54000</code> <code>ERROR: transaction age limit of 300s exceeded</code>"},{"location":"quotas-and-limits.html#memory-limits","title":"Memory Limits","text":"Limit Default Value Configurable Error Code Error Message Maximum base memory per query operation 128 MiB per transaction No <code>53200</code> <code>ERROR: query requires too much temp space, out of memory.</code>"},{"location":"quotas-and-limits.html#quota-management","title":"Quota Management","text":""},{"location":"quotas-and-limits.html#requesting-quota-increases","title":"Requesting Quota Increases","text":"<p>Service Quotas Console: Use the Service Quotas console to request increases for: - Single-Region cluster limits - Multi-Region cluster limits - Storage limits per cluster - Connection limits per cluster</p> <p>AWS Support: Contact AWS Support for other quota increases not available through Service Quotas console.</p>"},{"location":"quotas-and-limits.html#monitoring-quota-usage","title":"Monitoring Quota Usage","text":"<p>Best Practices: 1. Monitor cluster count against account limits 2. Track storage usage per cluster 3. Monitor connection patterns to avoid rate limits 4. Plan capacity based on application requirements</p>"},{"location":"quotas-and-limits.html#planning-considerations","title":"Planning Considerations","text":""},{"location":"quotas-and-limits.html#application-design","title":"Application Design","text":"<ul> <li>Transaction Size: Keep transactions under 10 MiB data modification limit</li> <li>Row Mutations: Limit to 3,000 rows per transaction</li> <li>Connection Management: Plan for 10,000 connection limit per cluster</li> <li>Schema Design: Consider 10 schema limit per database</li> </ul>"},{"location":"quotas-and-limits.html#performance-planning","title":"Performance Planning","text":"<ul> <li>Query Memory: Design queries to stay within 128 MiB memory limit</li> <li>Transaction Duration: Keep transactions under 5-minute limit</li> <li>Index Strategy: Plan for maximum 24 indexes per table</li> <li>Table Structure: Consider 255 column limit per table</li> </ul>"},{"location":"quotas-and-limits.html#scalability-planning","title":"Scalability Planning","text":"<ul> <li>Multi-Region Strategy: Plan for 5 multi-Region cluster limit</li> <li>Storage Growth: Plan for 10 TiB default storage limit</li> <li>Connection Scaling: Consider connection rate limits (100/second)</li> <li>View Management: Plan for 5,000 view limit per database</li> </ul>"},{"location":"quotas-and-limits.html#error-code-reference","title":"Error Code Reference","text":""},{"location":"quotas-and-limits.html#connection-error-codes","title":"Connection Error Codes","text":"<ul> <li>53300: <code>TOO_MANY_CONNECTIONS</code> - Exceeded connection limit</li> <li>53400: <code>CONFIGURED_LIMIT_EXCEEDED</code> - Exceeded connection rate</li> <li>53100: <code>DISK_FULL</code> - Exceeded storage limit</li> </ul>"},{"location":"quotas-and-limits.html#database-error-codes","title":"Database Error Codes","text":"<ul> <li>54000: General database limit exceeded</li> <li>54011: Column or key limit exceeded</li> <li>53200: Memory limit exceeded</li> </ul>"},{"location":"quotas-and-limits.html#api-error-codes","title":"API Error Codes","text":"<ul> <li>402: <code>ServiceQuotaExceededException</code> - Service quota exceeded</li> </ul>"},{"location":"quotas-and-limits.html#related-documentation","title":"Related Documentation","text":"<ul> <li>PostgreSQL Compatibility: Supported PostgreSQL features</li> <li>Data Types: Supported data types</li> <li>Systems Tables: Using systems tables and commands</li> </ul>"},{"location":"single-region-clusters.html","title":"Single-Region Clusters","text":"\ud83d\udccb Copy Page"},{"location":"single-region-clusters.html#configuring-single-region-clusters","title":"Configuring Single-Region Clusters","text":""},{"location":"single-region-clusters.html#overview","title":"Overview","text":"<p>Learn how to manage your clusters using the AWS SDKs and CLI. Configure and manage clusters for an AWS Region using either the CLI or your preferred programming language including Python, C++, JavaScript, Java, Rust, Ruby, .NET, and Golang.</p> <p>The CLI provides quick access through shell commands, while AWS Software Development Kits (SDKs) enable programmatic control through native language support.</p>"},{"location":"single-region-clusters.html#using-aws-sdks","title":"Using AWS SDKs","text":"<p>The AWS SDKs provide programmatic access to Aurora DSQL in your preferred programming language. The following sections show how to perform common cluster operations using different programming languages.</p>"},{"location":"single-region-clusters.html#create-cluster","title":"Create Cluster","text":"<p>The following examples show how to create a single-Region cluster using different programming languages.</p>"},{"location":"single-region-clusters.html#python-sdk","title":"Python SDK","text":"<pre><code>import boto3\n\ndef create_cluster(region):\n    try:\n        client = boto3.client(\"dsql\", region_name=region)\n        tags = {\"Name\": \"Python single region cluster\"}\n        cluster = client.create_cluster(tags=tags, deletionProtectionEnabled=True)\n        print(f\"Initiated creation of cluster: {cluster['identifier']}\")\n\n        print(f\"Waiting for {cluster['arn']} to become ACTIVE\")\n        client.get_waiter(\"cluster_active\").wait(\n            identifier=cluster[\"identifier\"],\n            WaiterConfig={\n                'Delay': 10,\n                'MaxAttempts': 30\n            }\n        )\n\n        return cluster\n    except:\n        print(\"Unable to create cluster\")\n        raise\n\ndef main():\n    region = \"us-east-1\"\n    response = create_cluster(region)\n    print(f\"Created cluster: {response['arn']}\")\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"single-region-clusters.html#javascript-sdk","title":"JavaScript SDK","text":"<pre><code>import { DSQLClient, CreateClusterCommand, waitUntilClusterActive } from \"@aws-sdk/client-dsql\";\n\nasync function createCluster(region) {\n    const client = new DSQLClient({ region });\n\n    try {\n        const createClusterCommand = new CreateClusterCommand({\n            deletionProtectionEnabled: true,\n            tags: {\n                Name: \"javascript single region cluster\"\n            },\n        });\n        const response = await client.send(createClusterCommand);\n\n        console.log(`Waiting for cluster ${response.identifier} to become ACTIVE`);\n        await waitUntilClusterActive(\n            {\n                client: client,\n                maxWaitTime: 300 // Wait for 5 minutes\n            },\n            {\n                identifier: response.identifier\n            }\n        );\n        console.log(`Cluster Id ${response.identifier} is now active`);\n        return;\n    } catch (error) {\n        console.error(`Unable to create cluster in ${region}: `, error.message);\n        throw error;\n    }\n}\n\nasync function main() {\n    const region = \"us-east-1\";\n    await createCluster(region);\n}\n\nmain();\n</code></pre>"},{"location":"single-region-clusters.html#java-sdk","title":"Java SDK","text":"<pre><code>import software.amazon.awssdk.auth.credentials.DefaultCredentialsProvider;\nimport software.amazon.awssdk.core.waiters.WaiterResponse;\nimport software.amazon.awssdk.regions.Region;\nimport software.amazon.awssdk.services.dsql.DsqlClient;\nimport software.amazon.awssdk.services.dsql.model.CreateClusterRequest;\nimport software.amazon.awssdk.services.dsql.model.CreateClusterResponse;\nimport software.amazon.awssdk.services.dsql.model.GetClusterResponse;\n\nimport java.time.Duration;\nimport java.util.Map;\n\npublic class CreateCluster {\n    public static void main(String[] args) {\n        Region region = Region.US_EAST_1;\n\n        try (DsqlClient client = DsqlClient.builder()\n                .region(region)\n                .credentialsProvider(DefaultCredentialsProvider.create())\n                .build()) {\n\n            CreateClusterRequest request = CreateClusterRequest.builder()\n                    .deletionProtectionEnabled(true)\n                    .tags(Map.of(\"Name\", \"java single region cluster\"))\n                    .build();\n            CreateClusterResponse cluster = client.createCluster(request);\n            System.out.println(\"Created \" + cluster.arn());\n\n            System.out.println(\"Waiting for cluster to become ACTIVE\");\n            WaiterResponse&lt;GetClusterResponse&gt; waiterResponse = client.waiter().waitUntilClusterActive(\n                    getCluster -&gt; getCluster.identifier(cluster.identifier()),\n                    config -&gt; config.waitTimeout(Duration.ofMinutes(5))\n            );\n            waiterResponse.matched().response().ifPresent(System.out::println);\n        }\n    }\n}\n</code></pre>"},{"location":"single-region-clusters.html#get-cluster-information","title":"Get Cluster Information","text":"<p>The following examples show how to get information about a single-Region cluster.</p>"},{"location":"single-region-clusters.html#python-sdk_1","title":"Python SDK","text":"<pre><code>import boto3\nfrom datetime import datetime\nimport json\n\ndef get_cluster(region, identifier):\n    try:\n        client = boto3.client(\"dsql\", region_name=region)\n        return client.get_cluster(identifier=identifier)\n    except:\n        print(f\"Unable to get cluster {identifier} in region {region}\")\n        raise\n\ndef main():\n    region = \"us-east-1\"\n    cluster_id = \"&lt;your cluster id&gt;\"\n    response = get_cluster(region, cluster_id)\n    print(json.dumps(response, indent=2, default=lambda obj: obj.isoformat() if isinstance(obj, datetime) else None))\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"single-region-clusters.html#javascript-sdk_1","title":"JavaScript SDK","text":"<pre><code>import { DSQLClient, GetClusterCommand } from \"@aws-sdk/client-dsql\";\n\nasync function getCluster(region, clusterId) {\n    const client = new DSQLClient({ region });\n\n    const getClusterCommand = new GetClusterCommand({\n        identifier: clusterId,\n    });\n\n    try {\n        return await client.send(getClusterCommand);\n    } catch (error) {\n        if (error.name === \"ResourceNotFoundException\") {\n            console.log(\"Cluster ID not found or deleted\");\n        }\n        throw error;\n    }\n}\n\nasync function main() {\n    const region = \"us-east-1\";\n    const clusterId = \"&lt;CLUSTER_ID&gt;\";\n\n    const response = await getCluster(region, clusterId);\n    console.log(\"Cluster: \", response);\n}\n\nmain();\n</code></pre>"},{"location":"single-region-clusters.html#update-cluster","title":"Update Cluster","text":"<p>The following examples show how to update a single-Region cluster.</p>"},{"location":"single-region-clusters.html#python-sdk_2","title":"Python SDK","text":"<pre><code>import boto3\n\ndef update_cluster(region, cluster_id, deletion_protection_enabled):\n    try:\n        client = boto3.client(\"dsql\", region_name=region)\n        return client.update_cluster(identifier=cluster_id, deletionProtectionEnabled=deletion_protection_enabled)\n    except:\n        print(\"Unable to update cluster\")\n        raise\n\ndef main():\n    region = \"us-east-1\"\n    cluster_id = \"&lt;your cluster id&gt;\"\n    deletion_protection_enabled = False\n    response = update_cluster(region, cluster_id, deletion_protection_enabled)\n    print(f\"Updated {response['arn']} with deletion_protection_enabled: {deletion_protection_enabled}\")\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"single-region-clusters.html#delete-cluster","title":"Delete Cluster","text":"<p>The following examples show how to delete a single-Region cluster.</p>"},{"location":"single-region-clusters.html#python-sdk_3","title":"Python SDK","text":"<pre><code>import boto3\n\ndef delete_cluster(region, identifier):\n    try:\n        client = boto3.client(\"dsql\", region_name=region)\n        cluster = client.delete_cluster(identifier=identifier)\n        print(f\"Initiated delete of {cluster['arn']}\")\n\n        print(\"Waiting for cluster to finish deletion\")\n        client.get_waiter(\"cluster_not_exists\").wait(\n            identifier=cluster[\"identifier\"],\n            WaiterConfig={\n                'Delay': 10,\n                'MaxAttempts': 30\n            }\n        )\n    except:\n        print(\"Unable to delete cluster \" + identifier)\n        raise\n\ndef main():\n    region = \"us-east-1\"\n    cluster_id = \"&lt;cluster id&gt;\"\n    delete_cluster(region, cluster_id)\n    print(f\"Deleted {cluster_id}\")\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"single-region-clusters.html#using-aws-cli","title":"Using AWS CLI","text":"<p>The AWS CLI provides a command-line interface for managing your Aurora DSQL clusters. The following examples demonstrate common cluster management operations.</p>"},{"location":"single-region-clusters.html#create-cluster_1","title":"Create Cluster","text":"<p>Create a cluster using the <code>create-cluster</code> command.</p> <p>Note: Cluster creation is an asynchronous operation. Call the <code>GetCluster</code> API until the status changes to <code>ACTIVE</code>. You can connect to your cluster after it becomes active.</p> <p>Command: <pre><code>aws dsql create-cluster --region us-east-1\n</code></pre></p> <p>Note: To disable deletion protection during creation, include the <code>--no-deletion-protection-enabled</code> flag.</p> <p>Response: <pre><code>{\n    \"identifier\": \"abc0def1baz2quux3quuux4\",\n    \"arn\": \"arn:aws:dsql:us-east-1:111122223333:cluster/abc0def1baz2quux3quuux4\",\n    \"status\": \"CREATING\",\n    \"creationTime\": \"2024-05-25T16:56:49.784000-07:00\",\n    \"deletionProtectionEnabled\": true,\n    \"tag\": {},\n    \"encryptionDetails\": {\n        \"encryptionType\": \"AWS_OWNED_KMS_KEY\",\n        \"encryptionStatus\": \"ENABLED\"\n    }\n}\n</code></pre></p>"},{"location":"single-region-clusters.html#get-cluster-information_1","title":"Get Cluster Information","text":"<p>Get information about a cluster using the <code>get-cluster</code> command.</p> <p>Command: <pre><code>aws dsql get-cluster \\\n  --region us-east-1 \\\n  --identifier your_cluster_id\n</code></pre></p> <p>Response: <pre><code>{\n    \"identifier\": \"abc0def1baz2quux3quuux4\",\n    \"arn\": \"arn:aws:dsql:us-east-1:111122223333:cluster/abc0def1baz2quux3quuux4\",\n    \"status\": \"ACTIVE\",\n    \"creationTime\": \"2024-11-27T00:32:14.434000-08:00\",\n    \"deletionProtectionEnabled\": false,\n    \"encryptionDetails\": {\n        \"encryptionType\": \"CUSTOMER_MANAGED_KMS_KEY\",\n        \"kmsKeyArn\": \"arn:aws:kms:us-east-1:111122223333:key/123a456b-c789-01de-2f34-g5hi6j7k8lm9\",\n        \"encryptionStatus\": \"ENABLED\"\n    }\n}\n</code></pre></p>"},{"location":"single-region-clusters.html#update-cluster_1","title":"Update Cluster","text":"<p>Update an existing cluster using the <code>update-cluster</code> command.</p> <p>Note: Updates are asynchronous operations. Call the <code>GetCluster</code> API until the status changes to <code>ACTIVE</code> to see your changes.</p> <p>Command: <pre><code>aws dsql update-cluster \\\n  --region us-east-1 \\\n  --no-deletion-protection-enabled \\\n  --identifier your_cluster_id\n</code></pre></p> <p>Response: <pre><code>{\n    \"identifier\": \"abc0def1baz2quux3quuux4\",\n    \"arn\": \"arn:aws:dsql:us-east-1:111122223333:cluster/abc0def1baz2quux3quuux4\",\n    \"status\": \"UPDATING\",\n    \"creationTime\": \"2024-05-24T09:15:32.708000-07:00\"\n}\n</code></pre></p>"},{"location":"single-region-clusters.html#delete-cluster_1","title":"Delete Cluster","text":"<p>Delete an existing cluster using the <code>delete-cluster</code> command.</p> <p>Note: You can only delete clusters that have deletion protection disabled. By default, deletion protection is enabled when you create new clusters.</p> <p>Command: <pre><code>aws dsql delete-cluster \\\n  --region us-east-1 \\\n  --identifier your_cluster_id\n</code></pre></p> <p>Response: <pre><code>{\n    \"identifier\": \"abc0def1baz2quux3quuux4\",\n    \"arn\": \"arn:aws:dsql:us-east-1:111122223333:cluster/abc0def1baz2quux3quuux4\",\n    \"status\": \"DELETING\",\n    \"creationTime\": \"2024-05-24T09:16:43.778000-07:00\"\n}\n</code></pre></p>"},{"location":"single-region-clusters.html#list-clusters","title":"List Clusters","text":"<p>List your clusters using the <code>list-clusters</code> command.</p> <p>Command: <pre><code>aws dsql list-clusters --region us-east-1\n</code></pre></p> <p>Response: <pre><code>{\n    \"clusters\": [\n        {\n            \"identifier\": \"abc0def1baz2quux3quux4quuux\",\n            \"arn\": \"arn:aws:dsql:us-east-1:111122223333:cluster/abc0def1baz2quux3quux4quuux\"\n        },\n        {\n            \"identifier\": \"abc0def1baz2quux3quux5quuuux\",\n            \"arn\": \"arn:aws:dsql:us-east-1:111122223333:cluster/abc0def1baz2quux3quux5quuuux\"\n        }\n    ]\n}\n</code></pre></p>"},{"location":"single-region-clusters.html#additional-resources","title":"Additional Resources","text":"<p>For more code samples and examples, visit the Aurora DSQL Samples GitHub repository.</p>"},{"location":"single-region-clusters.html#related-documentation","title":"Related Documentation","text":"<ul> <li>Getting Started: Getting Started</li> <li>Multi-Region Clusters: Multi-Region cluster management</li> <li>Authentication: Auth &amp; Access Overview</li> <li>Troubleshooting: Troubleshooting Overview</li> </ul>"},{"location":"troubleshooting.html","title":"Troubleshooting Overview","text":"\ud83d\udccb Copy Page"},{"location":"troubleshooting.html#troubleshooting-amazon-aurora-dsql","title":"Troubleshooting Amazon Aurora DSQL","text":""},{"location":"troubleshooting.html#overview","title":"Overview","text":"<p>This guide provides troubleshooting advice for common errors and issues when using Amazon Aurora DSQL. If you encounter an issue not listed here, contact AWS support.</p>"},{"location":"troubleshooting.html#connection-errors","title":"Connection Errors","text":""},{"location":"troubleshooting.html#ssl-error-code-6","title":"SSL Error Code 6","text":"<p>Error Message: <code>error: unrecognized SSL error code: 6</code> or <code>unable to accept connection, sni was not received</code></p> <p>Root Cause: PostgreSQL client version earlier than 14 lacks Server Name Indication (SNI) support, which is required for Aurora DSQL connections.</p> <p>Resolution: 1. Check your client version: <code>psql --version</code> 2. Upgrade PostgreSQL client to version 14 or later 3. Retry the connection</p>"},{"location":"troubleshooting.html#network-unreachable-error","title":"Network Unreachable Error","text":"<p>Error Message: <code>error: NetworkUnreachable</code></p> <p>Root Cause: Client doesn't support IPv6 connections on dual-stack server configuration.</p> <p>Technical Details: When a server supports dual-stack mode, clients first resolve hostnames to both IPv4 and IPv6 addresses. They attempt IPv4 connection first, then IPv6 if initial connection fails. IPv4-only systems show generic NetworkUnreachable error instead of clear \"IPv6 not supported\" message.</p> <p>Resolution: Ensure IPv6 support is available or use IPv4-only endpoint if provided.</p>"},{"location":"troubleshooting.html#authentication-errors","title":"Authentication Errors","text":""},{"location":"troubleshooting.html#iam-authentication-failed","title":"IAM Authentication Failed","text":"<p>Error Message: <code>IAM authentication failed for user \"...\"</code></p> <p>Root Cause: Authentication token or IAM role has expired.</p> <p>Common Scenarios: - Authentication token exceeded maximum duration (1 week) - Temporary IAM role expired before connection attempt - IAM role credentials no longer valid</p> <p>Resolution: 1. Generate new authentication token 2. Verify IAM role is still valid and accessible 3. Check IAM role expiration time 4. Retry connection with fresh credentials</p> <p>Related Documentation: Authentication and authorization guide</p>"},{"location":"troubleshooting.html#invalid-access-key-id","title":"Invalid Access Key ID","text":"<p>Error Message: <code>An error occurred (InvalidAccessKeyId) when calling the GetObject operation: The AWS Access Key ID you provided does not exist in our records</code></p> <p>Root Cause: IAM credential validation failure.</p> <p>Resolution: 1. Verify AWS Access Key ID is correct 2. Check if credentials have been rotated or deleted 3. Ensure credentials are properly configured in your environment</p> <p>Related Documentation: Why requests are signed</p>"},{"location":"troubleshooting.html#iam-role-not-found","title":"IAM Role Not Found","text":"<p>Error Message: <code>IAM role &lt;role&gt; does not exist</code></p> <p>Root Cause: Aurora DSQL cannot locate the specified IAM role.</p> <p>Resolution: 1. Verify IAM role name and ARN are correct 2. Check if role exists in the correct AWS account 3. Confirm role hasn't been deleted or renamed</p> <p>Related Documentation: IAM roles</p>"},{"location":"troubleshooting.html#invalid-iam-arn-format","title":"Invalid IAM ARN Format","text":"<p>Error Message: <code>IAM role must look like an IAM ARN</code></p> <p>Root Cause: IAM role ARN format is incorrect.</p> <p>Resolution: 1. Verify ARN follows correct format: <code>arn:aws:iam::account-id:role/role-name</code> 2. Check for typos in ARN string 3. Ensure proper ARN structure and syntax</p> <p>Related Documentation: IAM ARN format</p>"},{"location":"troubleshooting.html#authorization-errors","title":"Authorization Errors","text":""},{"location":"troubleshooting.html#role-not-supported","title":"Role Not Supported","text":"<p>Error Message: <code>Role &lt;role&gt; not supported</code></p> <p>Root Cause: Aurora DSQL doesn't support certain PostgreSQL GRANT operations.</p> <p>Resolution: Review supported PostgreSQL commands and use alternative approaches.</p> <p>Related Documentation: Supported PostgreSQL commands</p>"},{"location":"troubleshooting.html#cannot-establish-trust-with-role","title":"Cannot Establish Trust with Role","text":"<p>Error Message: <code>Cannot establish trust with role &lt;role&gt;</code></p> <p>Root Cause: Aurora DSQL doesn't support certain PostgreSQL GRANT operations.</p> <p>Resolution: Use Aurora DSQL-specific role management commands instead of standard PostgreSQL GRANT operations.</p> <p>Related Documentation: Supported PostgreSQL commands</p>"},{"location":"troubleshooting.html#database-role-does-not-exist","title":"Database Role Does Not Exist","text":"<p>Error Message: <code>Role &lt;role&gt; does not exist</code></p> <p>Root Cause: Aurora DSQL cannot find the specified database user role.</p> <p>Resolution: 1. Verify the database role was created properly 2. Check role name spelling and case sensitivity 3. Ensure role was created with proper permissions</p> <p>Related Documentation: Custom database roles</p>"},{"location":"troubleshooting.html#permission-denied-for-iam-trust","title":"Permission Denied for IAM Trust","text":"<p>Error Message: <code>ERROR: permission denied to grant IAM trust with role &lt;role&gt;</code></p> <p>Root Cause: Must be connected with admin role to grant access to database roles.</p> <p>Resolution: 1. Connect to cluster using admin role 2. Verify you have <code>dsql:DbConnectAdmin</code> permission 3. Retry the grant operation</p> <p>Related Documentation: Database role authorization</p>"},{"location":"troubleshooting.html#role-missing-login-attribute","title":"Role Missing LOGIN Attribute","text":"<p>Error Message: <code>ERROR: role &lt;role&gt; must have the LOGIN attribute</code></p> <p>Root Cause: Database roles must have LOGIN permission to be used for connections.</p> <p>Resolution: 1. Create role with LOGIN permission: <code>CREATE ROLE example WITH LOGIN;</code> 2. Or modify existing role: <code>ALTER ROLE example WITH LOGIN;</code></p> <p>Related Documentation:  - CREATE ROLE - ALTER ROLE</p>"},{"location":"troubleshooting.html#cannot-drop-role-with-dependencies","title":"Cannot Drop Role with Dependencies","text":"<p>Error Message: <code>ERROR: role &lt;role&gt; cannot be dropped because some objects depend on it</code></p> <p>Root Cause: Database role has active IAM relationship that must be revoked first.</p> <p>Resolution: 1. Revoke IAM relationship: <code>AWS IAM REVOKE example FROM 'arn:aws:iam::account:role/role-name';</code> 2. Then drop the database role 3. Verify no other dependencies exist</p> <p>Related Documentation: Revoking authorization</p>"},{"location":"troubleshooting.html#sql-errors","title":"SQL Errors","text":""},{"location":"troubleshooting.html#feature-not-supported","title":"Feature Not Supported","text":"<p>Error Message: <code>Error: Not supported</code></p> <p>Root Cause: Attempted to use PostgreSQL feature not supported in Aurora DSQL.</p> <p>Resolution: 1. Check Aurora DSQL feature compatibility documentation 2. Use supported alternative commands or approaches 3. Review PostgreSQL compatibility guide</p> <p>Related Documentation: Supported PostgreSQL features</p>"},{"location":"troubleshooting.html#index-creation-error","title":"Index Creation Error","text":"<p>Error Message: <code>Error: use CREATE INDEX ASYNC instead</code></p> <p>Root Cause: Creating indexes on tables with existing data requires asynchronous operation.</p> <p>Resolution: 1. Use <code>CREATE INDEX ASYNC</code> command instead of <code>CREATE INDEX</code> 2. Monitor index creation progress 3. Wait for completion before using index</p> <p>Related Documentation: Asynchronous index creation</p>"},{"location":"troubleshooting.html#concurrency-control-errors","title":"Concurrency Control Errors","text":""},{"location":"troubleshooting.html#mutation-conflicts","title":"Mutation Conflicts","text":"<p>Error Message: <code>OC000 \"ERROR: mutation conflicts with another transaction, retry as needed\"</code></p> <p>Root Cause: Transaction attempted to modify same data as concurrent transaction.</p> <p>Technical Details: Indicates contention on modified tuples between concurrent transactions.</p> <p>Resolution: 1. Implement retry logic in application 2. Add exponential backoff for retries 3. Consider reducing transaction scope to minimize conflicts</p> <p>Related Documentation: Concurrency control</p>"},{"location":"troubleshooting.html#schema-update-conflicts","title":"Schema Update Conflicts","text":"<p>Error Message: <code>OC001 \"ERROR: schema has been updated by another transaction, retry as needed\"</code></p> <p>Root Cause: Session catalog cache became outdated due to concurrent schema changes.</p> <p>Technical Process: 1. Session loaded catalog version V1 at time T1 2. Another transaction updated catalog to V2 at time T2 3. Original session attempted storage read with outdated V1 catalog 4. Storage layer rejected request due to version mismatch</p> <p>Resolution: 1. Retry the transaction (Aurora DSQL will refresh catalog cache) 2. New transaction will use updated catalog version 3. Ensure no additional schema changes occur during retry</p>"},{"location":"troubleshooting.html#ssltls-connection-errors","title":"SSL/TLS Connection Errors","text":""},{"location":"troubleshooting.html#certificate-verification-failed","title":"Certificate Verification Failed","text":"<p>Error Message: <code>SSL error: certificate verify failed</code></p> <p>Root Cause: Client cannot verify server certificate.</p> <p>Resolution: 1. Install Amazon Root CA 1 certificate properly 2. Set <code>PGSSLROOTCERT</code> environment variable to correct certificate file 3. Verify certificate file has correct permissions 4. Retry connection</p>"},{"location":"troubleshooting.html#unrecognized-ssl-error-code","title":"Unrecognized SSL Error Code","text":"<p>Error Message: <code>Unrecognized SSL error code: 6</code></p> <p>Root Cause: PostgreSQL client version below 14 lacks proper SSL support.</p> <p>Resolution: Upgrade PostgreSQL client to version 17 or later.</p>"},{"location":"troubleshooting.html#ssl-unregistered-scheme-windows","title":"SSL Unregistered Scheme (Windows)","text":"<p>Error Message: <code>SSL error: unregistered scheme (Windows)</code></p> <p>Root Cause: Known issue with Windows psql client using system certificates.</p> <p>Resolution: Use downloaded certificate file method for Windows connections instead of system certificates.</p>"},{"location":"what-is-amazon-aurora-dsql.html","title":"What is Amazon Aurora DSQL?","text":"\ud83d\udccb Copy Page"},{"location":"what-is-amazon-aurora-dsql.html#what-is-amazon-aurora-dsql","title":"What is Amazon Aurora DSQL?","text":""},{"location":"what-is-amazon-aurora-dsql.html#overview","title":"Overview","text":"<p>Amazon Aurora DSQL is a serverless, distributed relational database service optimized for transactional workloads. Amazon Aurora DSQL offers virtually unlimited scale and doesn't require you to manage infrastructure. The active-active highly available architecture provides 99.99% single-Region and 99.999% multi-Region availability.</p>"},{"location":"what-is-amazon-aurora-dsql.html#when-to-use-amazon-aurora-dsql","title":"When to Use Amazon Aurora DSQL","text":"<p>Aurora DSQL is optimized for transactional workloads that benefit from ACID transactions and a relational data model. Because it's serverless, Aurora DSQL is ideal for application patterns of microservice, serverless, and event-driven architectures. Aurora DSQL is PostgreSQL-compatible, so you can use familiar drivers, object-relational mappings (ORMs), frameworks, and SQL features.</p> <p>Aurora DSQL automatically manages system infrastructure and scales compute, I/O, and storage based on your workload. Because you have no servers to provision or manage, you don't have to worry about maintenance downtime related to provisioning, patching, or infrastructure upgrades.</p> <p>Aurora DSQL helps you to build and maintain enterprise applications that are always available at any scale. The active-active serverless design automates failure recovery, so you don't need to worry about traditional database failover. Your applications benefit from Multi-AZ and multi-Region availability, and you don't have to be concerned about eventual consistency or missing data related to failovers.</p>"},{"location":"what-is-amazon-aurora-dsql.html#key-features-in-amazon-aurora-dsql","title":"Key Features in Amazon Aurora DSQL","text":""},{"location":"what-is-amazon-aurora-dsql.html#distributed-architecture","title":"Distributed Architecture","text":"<p>Amazon Aurora DSQL is composed of the following multi-tenant components:</p> <ol> <li>Relay and connectivity</li> <li>Compute and databases</li> <li>Transaction log, concurrency control, and isolation</li> <li>Storage</li> </ol> <p>A control plane coordinates these components. Each component provides redundancy across three Availability Zones (AZs), with: - Automatic cluster scaling - Self-healing in case of component failures</p>"},{"location":"what-is-amazon-aurora-dsql.html#single-region-and-multi-region-clusters","title":"Single-Region and Multi-Region Clusters","text":"<p>Amazon Aurora DSQL clusters provide the following benefits:</p> <ul> <li>Synchronous data replication</li> <li>Consistent read operations</li> <li>Automatic failure recovery</li> <li>Data consistency across multiple AZs or Regions</li> </ul>"},{"location":"what-is-amazon-aurora-dsql.html#failure-recovery","title":"Failure Recovery","text":"<p>If an infrastructure component fails, Amazon Aurora DSQL automatically routes requests to healthy infrastructure without manual intervention. Amazon Aurora DSQL provides atomicity, consistency, isolation, and durability (ACID) transactions with: - Strong consistency - Snapshot isolation - Atomicity - Cross-AZ and cross-Region durability</p>"},{"location":"what-is-amazon-aurora-dsql.html#multi-region-capabilities","title":"Multi-Region Capabilities","text":"<p>Multi-Region peered clusters provide the same resilience and connectivity as single-Region clusters. But they improve availability by offering: - Two Regional endpoints (one in each peered cluster Region) - Both endpoints present a single logical database - Available for concurrent read and write operations - Strong data consistency</p> <p>You can build applications that run in multiple Regions at the same time for performance and resilience\u2014and know that readers always see the same data.</p>"},{"location":"what-is-amazon-aurora-dsql.html#compatibility-with-postgresql-databases","title":"Compatibility with PostgreSQL Databases","text":"<p>The distributed database layer (compute) in Amazon Aurora DSQL is based on a current major version of PostgreSQL. You can connect to Amazon Aurora DSQL with familiar PostgreSQL drivers and tools, such as <code>psql</code>.</p>"},{"location":"what-is-amazon-aurora-dsql.html#version-compatibility","title":"Version Compatibility","text":"<ul> <li>Amazon Aurora DSQL is currently compatible with PostgreSQL version 16</li> <li>Supports a subset of PostgreSQL features, expressions, and data types</li> </ul>"},{"location":"what-is-amazon-aurora-dsql.html#technical-specifications","title":"Technical Specifications","text":""},{"location":"what-is-amazon-aurora-dsql.html#availability-guarantees","title":"Availability Guarantees","text":"<ul> <li>Single-Region: 99.99% availability</li> <li>Multi-Region: 99.999% availability</li> </ul>"},{"location":"what-is-amazon-aurora-dsql.html#architecture-benefits","title":"Architecture Benefits","text":"<ul> <li>Serverless: No infrastructure management required</li> <li>Distributed: Built for high availability and scalability</li> <li>Active-Active: Highly available architecture</li> <li>ACID Compliant: Full transactional consistency</li> <li>PostgreSQL Compatible: Use familiar tools and syntax</li> </ul>"},{"location":"what-is-amazon-aurora-dsql.html#scaling-characteristics","title":"Scaling Characteristics","text":"<ul> <li>Virtually unlimited scale</li> <li>Automatic scaling of compute, I/O, and storage</li> <li>Multi-AZ redundancy</li> <li>Multi-Region support</li> </ul>"},{"location":"what-is-amazon-aurora-dsql.html#region-availability-for-amazon-aurora-dsql","title":"Region Availability for Amazon Aurora DSQL","text":"<p>With Amazon Aurora DSQL, you can deploy database instances across multiple AWS Regions to support global applications and meet data residency requirements. Region availability determines where you can create and manage Aurora DSQL database clusters. Database administrators and application architects who need to design highly available, globally distributed database systems often need to understand Region support for their workloads. Common use cases include setting up cross-Region disaster recovery, serving users from geographically closer database instances to reduce latency, and maintaining data copies in specific locations for compliance.</p> <p>The following table shows the AWS Regions where Aurora DSQL is currently available and the endpoint for each AWS Region:</p>"},{"location":"what-is-amazon-aurora-dsql.html#supported-aws-regions","title":"Supported AWS Regions","text":"Region Name Region Code Endpoint Protocol US East (Ohio) us-east-2 dsql.us-east-2.api.awsdsql-fips.us-east-2.api.aws HTTPSHTTPS US East (N. Virginia) us-east-1 dsql.us-east-1.api.awsdsql-fips.us-east-1.api.aws HTTPSHTTPS US West (Oregon) us-west-2 dsql.us-west-2.api.awsdsql-fips.us-west-2.api.aws HTTPSHTTPS Asia Pacific (Osaka) ap-northeast-3 dsql.ap-northeast-3.api.aws HTTPS Asia Pacific (Seoul) ap-northeast-2 dsql.ap-northeast-2.api.aws HTTPS Asia Pacific (Tokyo) ap-northeast-1 dsql.ap-northeast-1.api.aws HTTPS Europe (Frankfurt) eu-central-1 dsql.eu-central-1.api.aws HTTPS Europe (Ireland) eu-west-1 dsql.eu-west-1.api.aws HTTPS Europe (London) eu-west-2 dsql.eu-west-2.api.aws HTTPS Europe (Paris) eu-west-3 dsql.eu-west-3.api.aws HTTPS"},{"location":"what-is-amazon-aurora-dsql.html#multi-region-cluster-availability-for-amazon-aurora-dsql","title":"Multi-Region Cluster Availability for Amazon Aurora DSQL","text":"<p>You can create Aurora DSQL multi-Region clusters within specific AWS Region sets. Each Region set groups geographically related Regions that can work together in a multi-Region cluster.</p>"},{"location":"what-is-amazon-aurora-dsql.html#us-regions","title":"US Regions","text":"<ul> <li>US East (N. Virginia)</li> <li>US East (Ohio)</li> <li>US West (Oregon)</li> </ul>"},{"location":"what-is-amazon-aurora-dsql.html#asia-pacific-regions","title":"Asia Pacific Regions","text":"<ul> <li>Asia Pacific (Osaka)</li> <li>Asia Pacific (Seoul)</li> <li>Asia Pacific (Tokyo)</li> </ul>"},{"location":"what-is-amazon-aurora-dsql.html#european-regions","title":"European Regions","text":"<ul> <li>Europe (Frankfurt)</li> <li>Europe (Ireland)</li> <li>Europe (London)</li> <li>Europe (Paris)</li> </ul>"},{"location":"what-is-amazon-aurora-dsql.html#important-limitations","title":"Important Limitations","text":"<p>Multi-Region clusters must be created within a single Region set. For example, you can't create a cluster that includes both US East (N. Virginia) and Europe (Ireland) Regions.</p> <p>Important: Aurora DSQL currently doesn't support cross-continent multi-Region clusters.</p>"},{"location":"what-is-amazon-aurora-dsql.html#pricing","title":"Pricing","text":"<p>For cost information, see Amazon Aurora DSQL pricing.</p>"},{"location":"what-is-amazon-aurora-dsql.html#next-steps","title":"Next Steps","text":"<p>For information about the core components in Amazon Aurora DSQL and to get started with the service, see the following:</p> <ol> <li>Getting Started - Complete guide to creating your first Aurora DSQL cluster</li> <li>PostgreSQL Compatibility - Detailed information about supported SQL features</li> <li>Accessing Aurora DSQL - Methods for connecting to your clusters with PostgreSQL-compatible clients</li> <li>Working with Aurora DSQL - Advanced cluster management and operations</li> </ol>"},{"location":"what-is-amazon-aurora-dsql.html#cross-references","title":"Cross-References","text":"<ul> <li>High Availability Architecture: Learn more about how the distributed architecture supports high availability</li> <li>SQL Feature Compatibility: Understand the subset of PostgreSQL features, expressions, and data types supported</li> </ul>"},{"location":"generative-ai/jupyterlab.html","title":"Using JupyterLab with Aurora DSQL","text":"\ud83d\udccb Copy Page"},{"location":"generative-ai/jupyterlab.html#query-editors-using-jupyterlab-with-aurora-dsql","title":"Query Editors: Using JupyterLab with Aurora DSQL","text":"<p>This guide provides step-by-step instructions on how to connect and query Amazon Aurora DSQL using JupyterLab with Python. JupyterLab is a popular interactive computing environment that combines code, text, and visualizations in a single document. It's widely used for data science and research applications.</p> <p>The instructions below will cover the basics of Aurora DSQL usage in both a local installation of JupyterLab as well as using Amazon SageMaker AI, a fully-managed machine learning service that provides a hosted environment with a UI for data workflows.</p>"},{"location":"generative-ai/jupyterlab.html#getting-started","title":"Getting started","text":""},{"location":"generative-ai/jupyterlab.html#requirements","title":"Requirements","text":"<ul> <li>An Aurora DSQL cluster</li> <li>AWS credentials configured (local installation only)</li> <li>Python version 3.9 or greater (local installation only)</li> </ul>"},{"location":"generative-ai/jupyterlab.html#using-local-jupyterlab","title":"Using local JupyterLab","text":"<p>To get started with JupyterLab, users must first install the application using Python's pip:</p> <pre><code>pip install jupyterlab\n</code></pre> <p>JupyterLab can then be opened by running <code>jupyter lab</code>. This will open the JupyterLab application at localhost:8888, accessible in a browser. Ensure you have AWS credentials configured in your local environment before proceeding.</p>"},{"location":"generative-ai/jupyterlab.html#using-amazon-sagemaker-ai","title":"Using Amazon SageMaker AI","text":"<p>In the AWS console, proceed to the Amazon SageMaker AI console page and then to the Notebooks section under Applications and IDEs. From there you can select Create notebook instance to begin creating a SageMaker environment. Select an instance type and platform before clicking Create notebook instance.</p> <p>See Amazon SageMaker AI setup documentation for more information on setup and instance options.</p> <p>Note</p> <p>Warning: Using Amazon SageMaker AI may result in charges to your AWS account.</p> <p>Once the SageMaker instance becomes active, you can open it from the Notebook instances section with Open JupyterLab. Before getting started with Aurora DSQL in your notebook you must provide access to your DSQL cluster in the SageMaker instance's IAM role. The simplest way to do so is to follow the link to the IAM role in the notebook instance page. From there you can edit the Policies attached to your SageMaker IAM role. See Authentication and authorization for more information on configuring an IAM policy to allow access to Aurora DSQL.</p>"},{"location":"generative-ai/jupyterlab.html#connecting-to-aurora-dsql-using-jupyterlab","title":"Connecting to Aurora DSQL using JupyterLab","text":"<p>After you have set up a JupyterLab instance, the steps to connect to Aurora DSQL are the same locally and in SageMaker AI. Create an empty Python 3 notebook, in which you can add cells with Python code.</p> <p>In a Python cell, download the Amazon root certificate from the official trust store:</p> <pre><code>import urllib.request\nurllib.request.urlretrieve('https://www.amazontrust.com/repository/AmazonRootCA1.pem', 'root.pem')\n</code></pre> <p>To connect to Aurora DSQL, first install the Aurora DSQL Connector for Python and the Psycopg driver in a Python cell, and then import it:</p> <pre><code>pip install aurora_dsql_python_connector psycopg\n</code></pre> <pre><code>import aurora_dsql_psycopg as dsql\n</code></pre> <p>With the connector imported, you can then create a DSQL configuration and connect. The Aurora DSQL Python Connector will automatically handle creation of an authentication token on each connection.</p> <pre><code>config = {\n    'host': \"your-cluster.dsql.us-east-1.on.aws\",\n    'region': \"us-east-1\",\n    'user': \"admin\"\n}\n\nconn = dsql.connect(**config)\n</code></pre> <p>Upon running your code you should now have a Psycopg connection to Aurora DSQL. You can then run queries using the Psycopg cursor and providing your SQL query. See the Psycopg documentation for more information on using Psycopg with a Postgres-compatible database. This query will result in a list of tuples in <code>results_list</code>.</p> <pre><code>with conn:\n    with conn.cursor() as cur:\n        cur.execute(\"SELECT * FROM table\")\n        results_list = cur.fetchall()\n</code></pre> <p>You can then use Python frameworks like Pandas to analyze or visualize your query results, for example:</p> <pre><code>pip install pandas\n</code></pre> <pre><code>import pandas as pd\n\ndf = pd.DataFrame(tuples_list)\nprint(df)\nprint(f\"Total records: {len(df)}\")\n</code></pre>"},{"location":"generative-ai/jupyterlab.html#example-notebook","title":"Example notebook","text":"<p>A sample notebook using Aurora DSQL is available in the Aurora DSQL samples repository.</p>"},{"location":"generative-ai/jupyterlab.html#further-reading","title":"Further reading","text":"<ul> <li>Amazon SageMaker AI setup documentation</li> <li>Aurora DSQL Connector for Python</li> <li>Pandas documentation</li> </ul>"},{"location":"generative-ai/mcp-server.html","title":"AWS Labs Aurora DSQL MCP Server","text":"\ud83d\udccb Copy Page"},{"location":"generative-ai/mcp-server.html#aws-labs-aurora-dsql-mcp-server","title":"AWS Labs Aurora DSQL MCP Server","text":"<p>An AWS Labs Model Context Protocol (MCP) server for Aurora DSQL.</p>"},{"location":"generative-ai/mcp-server.html#features","title":"Features","text":"<ul> <li>Converting human-readable questions and commands into structured Postgres-compatible SQL queries and executing them against the configured Aurora DSQL database.</li> <li>Read-only by default, transactions enabled with <code>--allow-writes</code></li> <li>Connection reuse between requests for improved performance</li> <li>Built-in access to Aurora DSQL documentation, search, and best practice recommendations</li> </ul>"},{"location":"generative-ai/mcp-server.html#available-tools","title":"Available Tools","text":""},{"location":"generative-ai/mcp-server.html#database-operations","title":"Database Operations","text":"<ul> <li>readonly_query - Execute read-only SQL queries against your DSQL cluster</li> <li>transact - Execute write operations in a transaction (requires <code>--allow-writes</code>)</li> <li>get_schema - Retrieve table schema information</li> </ul>"},{"location":"generative-ai/mcp-server.html#documentation-and-recommendations","title":"Documentation and Recommendations","text":"<ul> <li>dsql_search_documentation - Search Aurora DSQL documentation</li> <li>Parameters: <code>search_phrase</code> (required), <code>limit</code> (optional)</li> <li>dsql_read_documentation - Read specific DSQL documentation pages</li> <li>Parameters: <code>url</code> (required), <code>start_index</code> (optional), <code>max_length</code> (optional)</li> <li>dsql_recommend - Get recommendations for DSQL best practices</li> <li>Parameters: <code>url</code> (required)</li> </ul>"},{"location":"generative-ai/mcp-server.html#prerequisites","title":"Prerequisites","text":"<ol> <li>An AWS account with an Aurora DSQL Cluster</li> <li>This MCP server can only be run locally on the same host as your LLM client.</li> <li>Set up AWS credentials with access to AWS services</li> <li>You need an AWS account with a role including these permissions:<ul> <li><code>dsql:DbConnectAdmin</code> - Connect to DSQL clusters as the admin user</li> <li><code>dsql:DbConnect</code> - Connect to DSQL clusters with custom database roles (only needed if using non-admin users)</li> </ul> </li> <li>Configure AWS credentials with <code>aws configure</code> or environment variables</li> </ol>"},{"location":"generative-ai/mcp-server.html#installation","title":"Installation","text":"<p>For most tools, updating the configuration by following the Default Installation instructions should be sufficient. </p> <p>Separate instructions are outlined for Claude Code and Codex. </p>"},{"location":"generative-ai/mcp-server.html#default-installation-updating-the-relevant-mcp-config-file","title":"Default Installation: Updating the Relevant MCP Config File","text":""},{"location":"generative-ai/mcp-server.html#using-uv","title":"Using <code>uv</code>","text":"<ol> <li>Install <code>uv</code> from Astral or the GitHub README</li> <li>Install Python using <code>uv python install 3.10</code></li> </ol> <p>Configure the MCP server in your MCP client configuration (Finding the MCP Config File)</p> <pre><code>{\n  \"mcpServers\": {\n    \"awslabs.aurora-dsql-mcp-server\": {\n      \"command\": \"uvx\",\n      \"args\": [\n        \"awslabs.aurora-dsql-mcp-server@latest\",\n        \"--cluster_endpoint\",\n        \"[your dsql cluster endpoint, e.g. abcdefghijklmnopqrst234567.dsql.us-east-1.on.aws]\",\n        \"--region\",\n        \"[your dsql cluster region, e.g. us-east-1]\",\n        \"--database_user\",\n        \"[your dsql username, e.g. admin]\",\n        \"--profile\",\n        \"default\"\n      ],\n      \"env\": {\n        \"FASTMCP_LOG_LEVEL\": \"ERROR\"\n      },\n      \"disabled\": false,\n      \"autoApprove\": []\n    }\n  }\n}\n</code></pre>"},{"location":"generative-ai/mcp-server.html#windows-installation","title":"Windows Installation","text":"<p>For Windows users, the MCP server configuration format is slightly different:</p> <pre><code>{\n  \"mcpServers\": {\n    \"awslabs.aurora-dsql-mcp-server\": {\n      \"disabled\": false,\n      \"timeout\": 60,\n      \"type\": \"stdio\",\n      \"command\": \"uv\",\n      \"args\": [\n        \"tool\",\n        \"run\",\n        \"--from\",\n        \"awslabs.aurora-dsql-mcp-server@latest\",\n        \"awslabs.aurora-dsql-mcp-server.exe\"\n      ],\n      \"env\": {\n        \"FASTMCP_LOG_LEVEL\": \"ERROR\",\n        \"AWS_PROFILE\": \"your-aws-profile\",\n        \"AWS_REGION\": \"us-east-1\"\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"generative-ai/mcp-server.html#finding-the-mcp-client-configuration-file","title":"Finding the MCP Client Configuration File","text":"<p>For some of the most common Agentic development tools, you can find your MCP client configurations  at the following file paths:</p> <ul> <li>Kiro:</li> <li>User Config: <code>~/.kiro/settings/mcp.json</code></li> <li>Workspace Config: <code>/path/to/workspace/.kiro/settings/mcp.json</code></li> <li>Claude Code: Refer to Claude Code Installation for detailed setup help</li> <li>User Config: <code>~/.claude.json</code> in <code>\"mcpServers\"</code></li> <li>Project Config: <code>/path/to/project/.mcp.json</code></li> <li>Local Config: <code>~/.claude.json</code> in <code>\"projects\" -&gt; \"path/to/project\" -&gt; \"mcpServers\"</code></li> <li>Cursor:</li> <li>Global: <code>~/.cursor/mcp.json</code></li> <li>Project: <code>/path/to/project/.cursor/mcp.json</code></li> <li>Codex: <code>~/.codex/config.toml</code></li> <li>Each MCP server is configured with a [mcp_servers.] table in the config file. Refer to     the Custom Codex Installation Instructions <li>Warp:</li> <li>File Editing: <code>~/.warp/mcp_settings.json</code></li> <li>Application Editor: <code>Settings &gt; AI &gt; Manage MCP Servers</code> and paste json</li> <li>Amazon Q Developer CLI: <code>~/.aws/amazonq/mcp.json</code></li> <li>Cline: Usually a nested VS Code path - <code>~/.vscode-server/path/to/cline_mcp_settings.json</code> </li>"},{"location":"generative-ai/mcp-server.html#claude-code","title":"Claude Code","text":""},{"location":"generative-ai/mcp-server.html#prerequisites_1","title":"Prerequisites","text":"<p>Important: MCP server management is only available through the Claude Code CLI terminal experience, not the VS Code native panel mode.</p> <p>Install the Claude Code CLI first by following Claude\u2019s native installation recommended process. </p>"},{"location":"generative-ai/mcp-server.html#choosing-the-right-scope","title":"Choosing the Right Scope","text":"<p>Claude Code offers 3 different scopes: local (default), project, and user and details which scope to choose based on credential sensitivity and need to share. Refer to the Claude Code documentation on MCP Installation Scopesfor more details.  </p> <ol> <li>Local-scoped servers represent the default configuration level and are stored in <code>~/.claude.json</code> under your project\u2019s path. They\u2019re both private to you and only accessible within the current project directory. This is the default <code>scope</code> when creating MCP servers. </li> <li>Project-scoped servers enable team collaboration while still only being accessible in a project directory. Project-scoped servers add a <code>.mcp.json</code> file at your project\u2019s root directory. This file is designed to be checked into version control, ensuring all team members have access to the same MCP tools and services. When you add a project-scoped server, Claude Code automatically creates or updates this file with the appropriate configuration structure.</li> <li>User-scoped servers are stored in <code>~/.claude.json</code> and provide cross-project accessibility, making them available across all projects on your machine while remaining private to your user account. </li> </ol>"},{"location":"generative-ai/mcp-server.html#using-the-claude-cli-recommended","title":"Using the Claude CLI (recommended)","text":"<p>Using an interactive <code>claude</code> CLI session enables an improved troubleshooting experience,  so this is the recommended path. </p> <pre><code>claude mcp add amazon-aurora-dsql \\\n  --scope [one of local, project, or user] \\\n  --env FASTMCP_LOG_LEVEL=\"ERROR\" \\\n  -- uvx \"awslabs.aurora-dsql-mcp-server@latest\" \\\n  --cluster_endpoint \"[dsql-cluster-id].dsql.[region].on.aws\" \\\n  --region \"[dsql cluster region, eg. us-east-1]\" \\\n  --database_user \"[your-username]\"\n</code></pre>"},{"location":"generative-ai/mcp-server.html#troubleshooting-using-claude-code-with-bedrock-on-a-different-aws-account","title":"Troubleshooting: Using Claude Code with Bedrock on a different AWS Account","text":"<p>If you've configured Claude Code with a Bedrock AWS account or profile that is distinct from the profile needed to connect to your dsql cluster, you'll need to  provide additional environment arguments:</p> <pre><code>  --env AWS_PROFILE=\"[dsql profile, eg. default]\" \\\n  --env AWS_REGION=\"[dsql cluster region, eg. us-east-1]\" \\\n</code></pre>"},{"location":"generative-ai/mcp-server.html#direct-modification-in-the-configuration-file","title":"Direct Modification in the Configuration File","text":"<p>Claude Code Requires alphanumeric naming, so we recommend naming your server: <code>aurora-dsql-mcp-server</code>. </p>"},{"location":"generative-ai/mcp-server.html#local-scope","title":"Local-Scope","text":"<p>Update <code>~/.claude.json</code> within the project-specific <code>mcpServers</code> field:</p> <pre><code>{\n  \"projects\": {\n    \"/path/to/project\": {\n      \"mcpServers\": {}\n    }\n  }\n}\n</code></pre>"},{"location":"generative-ai/mcp-server.html#project-scope","title":"Project-Scope","text":"<p>Update <code>/path/to/project/root/.mcp.json</code> in the <code>mcpServers</code> field:</p> <pre><code>{\n  \"mcpServers\": {}\n}\n</code></pre>"},{"location":"generative-ai/mcp-server.html#user-scope","title":"User-Scope","text":"<p>Update <code>~/.claude.json</code> within the project-specific <code>mcpServers</code> field:</p> <pre><code>{\n  \"mcpServers\": {}\n}\n</code></pre>"},{"location":"generative-ai/mcp-server.html#codex","title":"Codex","text":""},{"location":"generative-ai/mcp-server.html#option-1-codex-cli","title":"Option 1: Codex CLI","text":"<p>If you have the Codex CLI installed, you can use the codex mcp command to configure your MCP servers.</p> <pre><code>codex mcp add amazon-aurora-dsql \\\n  --env FASTMCP_LOG_LEVEL=\"ERROR\" \\\n  -- uvx \"awslabs.aurora-dsql-mcp-server@latest\" \\\n  --cluster_endpoint \"[dsql-cluster-id].dsql.[region].on.aws\" \\\n  --region \"[dsql cluster region, eg. us-east-1]\" \\\n  --database_user \"[your-username]\"\n</code></pre>"},{"location":"generative-ai/mcp-server.html#option-2-configtoml","title":"Option 2: config.toml","text":"<p>For more fine grained control over MCP server options, you can manually edit the ~/.codex/config.toml configuration file. Each MCP server is configured with a <code>[mcp_servers.&lt;server-name&gt;]</code> table in the config file.</p> <pre><code>[mcp_servers.amazon-aurora-dsql]\ncommand = \"uvx\"\nargs = [\n  \"awslabs.aurora-dsql-mcp-server@latest\",\n  \"--cluster_endpoint\", \"&lt;DSQL_CLUSTER_ID&gt;.dsql.&lt;AWS_REGION&gt;.on.aws\",\n  \"--region\", \"&lt;AWS_REGION&gt;\",\n  \"--database_user\", \"&lt;DATABASE_USERNAME&gt;\"\n]\n\n[mcp_servers.amazon-aurora-dsql.env]\nFASTMCP_LOG_LEVEL = \"ERROR\"\n</code></pre>"},{"location":"generative-ai/mcp-server.html#verifying-installation","title":"Verifying Installation","text":"<p>For Amazon Q Developer CLI, Kiro CLI, Claude CLI/TUI, or Codex CLI/TUI, run <code>/mcp</code> to see the status  of the MCP server.</p> <p>For the Kiro IDE, you can also navigate to the Kiro Panel's <code>MCP SERVERS</code> tab which shows  all configured MCP servers and their connection status indicators. </p>"},{"location":"generative-ai/mcp-server.html#server-configuration-options","title":"Server Configuration Options","text":""},{"location":"generative-ai/mcp-server.html#-allow-writes","title":"<code>--allow-writes</code>","text":"<p>By default, the dsql mcp server does not allow write operations (\"read-only mode\"). Any invocations of transact tool will fail in this mode. To use transact tool, allow writes by passing <code>--allow-writes</code> parameter.</p> <p>We recommend using least-privilege access when connecting to DSQL. For example, users should use a role that is read-only when possible. The read-only mode has a best-effort client-side enforcement to reject mutations.</p>"},{"location":"generative-ai/mcp-server.html#-cluster_endpoint","title":"<code>--cluster_endpoint</code>","text":"<p>This is mandatory parameter to specify the cluster to connect to. This should be the full endpoint of your cluster, e.g., <code>01abc2ldefg3hijklmnopqurstu.dsql.us-east-1.on.aws</code></p>"},{"location":"generative-ai/mcp-server.html#-database_user","title":"<code>--database_user</code>","text":"<p>This is a mandatory parameter to specify the user to connect as. For example <code>admin</code>, or <code>my_user</code>. Note that the AWS credentials you are using must have permission to login as that user. For more information on setting up and using database roles in DSQL, see Using database roles with IAM roles.</p>"},{"location":"generative-ai/mcp-server.html#-profile","title":"<code>--profile</code>","text":"<p>You can specify the aws profile to use for your credentials. Note that this is not supported for docker installation.</p> <p>Using the <code>AWS_PROFILE</code> environment variable in your MCP configuration is also supported:</p> <pre><code>\"env\": {\n  \"AWS_PROFILE\": \"your-aws-profile\"\n}\n</code></pre> <p>If neither is provided, the MCP server defaults to using the \"default\" profile in your AWS configuration file.</p>"},{"location":"generative-ai/mcp-server.html#-region","title":"<code>--region</code>","text":"<p>This is a mandatory parameter to specify the region of your DSQL database.</p>"},{"location":"generative-ai/mcp-server.html#-knowledge-server","title":"<code>--knowledge-server</code>","text":"<p>Optional parameter to specify the remote MCP server endpoint for DSQL knowledge tools (documentation search, reading, and recommendations). By default it is pre-configured.</p> <p>Example:</p> <pre><code>--knowledge-server https://custom-knowledge-server.example.com\n</code></pre> <p>Note: For security, only use trusted knowledge server endpoints. The server should be an HTTPS endpoint.</p>"},{"location":"generative-ai/mcp-server.html#-knowledge-timeout","title":"<code>--knowledge-timeout</code>","text":"<p>Optional parameter to specify the timeout in seconds for requests to the knowledge server.</p> <p>Default: <code>30.0</code></p> <p>Example:</p> <pre><code>--knowledge-timeout 60.0\n</code></pre> <p>Increase this value if you experience timeouts when accessing documentation on slow networks.</p>"},{"location":"generative-ai/query-editor.html","title":"Aurora DSQL Query Editor","text":"\ud83d\udccb Copy Page"},{"location":"generative-ai/query-editor.html#get-started-with-the-aurora-dsql-query-editor","title":"Get started with the Aurora DSQL Query Editor","text":"<p>With the Aurora DSQL Query Editor, you can securely connect to your Aurora DSQL clusters and run SQL queries directly from the AWS Management Console without installing or configuring external clients. It provides an intuitive workspace with built-in syntax highlighting, auto-completion, and intelligent code assistance. You can quickly explore schema objects, develop and execute SQL queries, and view results, all within a single interface.</p> <p>This topic walks you through the steps to connect to a cluster, run queries, view results, and explore advanced capabilities such as execution plans.</p> <p>Note</p> <p>The Query Editor is available in all Regions where Aurora DSQL is supported. For more information, see AWS Regional Services.</p>"},{"location":"generative-ai/query-editor.html#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure that you meet the following requirements:</p> <ul> <li>You have at least one Aurora DSQL cluster available. For more information, see Step 1: Create a Single-Region Cluster.</li> <li>Your cluster endpoint is publicly accessible. The Query Editor does not currently support clusters that have public access blocked by resource-based policies or clusters managed through VPC endpoints. For more information, see Blocking public access with resource-based policies in Aurora DSQL and Managing and connecting to Amazon Aurora DSQL clusters using AWS PrivateLink.</li> <li>Your IAM user or role has the required permissions to access and connect to the cluster. For more information, see Using database roles and IAM authentication.</li> </ul>"},{"location":"generative-ai/query-editor.html#working-with-the-query-editor","title":"Working with the Query Editor","text":""},{"location":"generative-ai/query-editor.html#open-the-query-editor","title":"Open the Query Editor","text":"<p>To open the Query Editor:</p> <ol> <li>Open the Aurora DSQL console.</li> <li>In the navigation pane, choose Query Editor.</li> </ol> <p>Alternatively, from the Clusters page, select the cluster you want to query and choose Connect with Query editor to launch the editor directly.</p> <p>Note</p> <p>Work and connection state are not saved. If you navigate away from the Aurora DSQL console, close the browser tab, or sign out, your connections, query text, and results are lost.</p>"},{"location":"generative-ai/query-editor.html#connect-to-a-cluster","title":"Connect to a cluster","text":"<p>To connect to a cluster:</p> <ol> <li>If no cluster connection exists, the editor displays No cluster has been connected. Choose Connect or select + (Add) in the Cluster Explorer pane to connect to an existing cluster.</li> <li>(Optional) Connect to multiple clusters or to the same cluster using different roles.</li> </ol>"},{"location":"generative-ai/query-editor.html#explore-cluster-objects","title":"Explore cluster objects","text":"<p>The Cluster Explorer displays all available cluster connections and lets you browse objects such as databases, schemas, tables, and views. It also provides common actions like Refresh, Create table, and other context-specific options.</p>"},{"location":"generative-ai/query-editor.html#run-queries","title":"Run queries","text":"<p>To run a query:</p> <ol> <li> <p>In the query editor tab pane, enter your SQL statement. For example:    <pre><code>SELECT * FROM public.orders LIMIT 10;\n</code></pre></p> </li> <li> <p>Verify the Active Cluster Context displayed on the upper right of the query tab. This indicates the cluster connection associated with the current query tab.</p> </li> <li> <p>(Optional) Use the connection dropdown to review all available connections or switch to a different cluster. Changing the connection updates where your queries in that tab are executed.</p> </li> <li> <p>Choose Run to execute the query.</p> </li> </ol> <p>Note</p> <p>Each query can return up to 10,000 rows in the results pane. For larger datasets, refine your query with filters or limits.</p>"},{"location":"generative-ai/query-editor.html#review-results-and-execution-plans","title":"Review results and execution plans","text":"<p>After the query runs, review the output in the Results panel at the bottom of the editor. By default, each query execution displays the Results (Table) tab, showing tabular query output.</p> <p>To get the query execution plan, run <code>EXPLAIN ANALYZE</code> or <code>EXPLAIN ANALYZE VERBOSE</code> to get additional insights into query performance. For more information, see Reading Aurora DSQL EXPLAIN plans.</p> <p>Tip</p> <p>The <code>EXPLAIN ANALYZE VERBOSE</code> command surfaces DPU usage estimates, including Compute, Read, Write, and Total DPU values, providing immediate visibility into the resources consumed by individual SQL statements.</p>"},{"location":"guides/getting-started/quickstart.html","title":"Getting Started","text":"\ud83d\udccb Copy Page"},{"location":"guides/getting-started/quickstart.html#getting-started-with-aurora-dsql","title":"Getting Started with Aurora DSQL","text":"<p>Learn how to create an Aurora DSQL cluster, connect to it, and run your first queries. This guide walks you through creating single-Region and multi-Region Aurora DSQL clusters, connecting to them, and running sample SQL commands using the AWS Console and PostgreSQL-compatible tools.</p>"},{"location":"guides/getting-started/quickstart.html#prerequisites","title":"Prerequisites","text":"<p>Before you begin using Aurora DSQL, ensure you meet the following prerequisites:</p> <ul> <li>Your IAM identity must have permission to sign in to the console</li> <li>Your IAM identity must meet the following criteria:</li> <li>Access to perform any action on any resource in your AWS account</li> <li><code>AmazonAuroraDSQLConsoleFullAccess</code> AWS managed policy is attached</li> </ul>"},{"location":"guides/getting-started/quickstart.html#step-1-create-a-single-region-cluster","title":"Step 1: Create a Single-Region Cluster","text":"<p>The basic unit of Aurora DSQL is the cluster, which is where you store your data. In this step, you create a cluster in a single AWS Region.</p>"},{"location":"guides/getting-started/quickstart.html#create-a-single-region-cluster","title":"Create a single-Region cluster","text":"<ol> <li> <p>Sign in to the AWS Console and open the Aurora DSQL console at https://console.aws.amazon.com/dsql</p> </li> <li> <p>Choose Create cluster and then Single-Region</p> </li> <li> <p>(Optional) Change the value of the default Name tag</p> </li> <li> <p>(Optional) Add additional Tags for this cluster</p> </li> <li> <p>(Optional) In Cluster settings, select any of the following options:</p> </li> <li>Select Customize encryption settings (advanced) to choose or create an AWS KMS key</li> <li>Select Enable deletion protection to prevent a delete operation from removing your cluster. By default, deletion protection is selected</li> <li> <p>Select Resource-based policy (advanced) to specify access control policies for this cluster</p> </li> <li> <p>Choose Create cluster</p> </li> <li> <p>The console returns you to the Clusters page. A notification banner appears indicating that the cluster is being created. Select the Cluster ID to open the cluster details view</p> </li> </ol>"},{"location":"guides/getting-started/quickstart.html#step-2-connect-to-your-cluster","title":"Step 2: Connect to Your Cluster","text":"<p>Aurora DSQL supports multiple ways to connect to your cluster, including the DSQL Query Editor, AWS CloudShell, the local psql client, and other PostgreSQL-compatible tools. In this step, you connect using the Aurora DSQL Query Editor, which provides a quick way to begin interacting with your new cluster.</p>"},{"location":"guides/getting-started/quickstart.html#connect-using-the-query-editor","title":"Connect using the Query Editor","text":"<ol> <li> <p>In the Aurora DSQL Console (https://console.aws.amazon.com/dsql), open the Clusters page and confirm that your cluster creation has completed and its status is Active</p> </li> <li> <p>Select your cluster from the list, or choose the Cluster ID to open the Cluster details page</p> </li> <li> <p>Choose Connect with Query editor</p> </li> <li> <p>Choose Connect as admin for the cluster that was just created</p> </li> <li>Optionally you can connect with a custom role see Using database roles and IAM authentication</li> </ol>"},{"location":"guides/getting-started/quickstart.html#step-3-run-sample-sql-commands","title":"Step 3: Run Sample SQL Commands","text":"<p>Test your Aurora DSQL cluster by running SQL statements. After opening the cluster in the Query Editor, select and run each sample query step by step.</p>"},{"location":"guides/getting-started/quickstart.html#create-a-schema","title":"Create a schema","text":"<p>Create a schema named <code>test</code>:</p> <pre><code>CREATE SCHEMA IF NOT EXISTS test;\n</code></pre>"},{"location":"guides/getting-started/quickstart.html#create-a-table","title":"Create a table","text":"<p>Create a hello_world table that uses an automatically generated UUID as the primary key:</p> <pre><code>CREATE TABLE IF NOT EXISTS test.hello_world (\n    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n    message VARCHAR(255) NOT NULL,\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n);\n</code></pre>"},{"location":"guides/getting-started/quickstart.html#insert-sample-data","title":"Insert sample data","text":"<p>Insert a sample row:</p> <pre><code>INSERT INTO test.hello_world (message)\nVALUES ('Hello, World!');\n</code></pre>"},{"location":"guides/getting-started/quickstart.html#query-the-data","title":"Query the data","text":"<p>Read the inserted values:</p> <pre><code>SELECT * FROM test.hello_world;\n</code></pre>"},{"location":"guides/getting-started/quickstart.html#clean-up-optional","title":"Clean up (Optional)","text":"<p>Optionally clean up the test resources:</p> <pre><code>DROP TABLE test.hello_world;\nDROP SCHEMA test;\n</code></pre>"},{"location":"guides/getting-started/quickstart.html#step-4-optional-create-a-multi-region-cluster","title":"Step 4 (Optional): Create a Multi-Region Cluster","text":"<p>When you create a multi-Region cluster, you specify the following Regions:</p>"},{"location":"guides/getting-started/quickstart.html#remote-region","title":"Remote Region","text":"<p>This is the Region in which you create a second cluster. You create a second cluster in this Region and peer it to your initial cluster. Aurora DSQL replicates all writes on the initial cluster to the remote cluster. You can read and write on any cluster.</p>"},{"location":"guides/getting-started/quickstart.html#witness-region","title":"Witness Region","text":"<p>This Region receives all data that is written to the multi-Region cluster. However, witness Regions don't host client endpoints and don't provide user data access. A limited window of the encrypted transaction log is maintained in witness Regions. This log facilitates recovery and supports transactional quorum if a Region becomes unavailable.</p>"},{"location":"guides/getting-started/quickstart.html#create-a-multi-region-cluster","title":"Create a multi-Region cluster","text":"<p>Use the following procedure to create an initial cluster, create a second cluster in a different Region, and then peer the two clusters to create a multi-Region cluster. It also demonstrates cross-Region write replication and consistent reads from both Regional endpoints.</p> <ol> <li> <p>Sign in to the Aurora DSQL console</p> </li> <li> <p>In the navigation pane, choose Clusters</p> </li> <li> <p>Choose Create cluster and then Multi-Region</p> </li> <li> <p>(Optional) Change the value of the default Name tag</p> </li> <li> <p>(Optional) Add additional Tags for this cluster</p> </li> <li> <p>In Multi-Region settings, choose the following options for your initial cluster:</p> </li> <li>In Witness Region, choose a Region. Currently, only US-based Regions are supported for witness Regions in multi-Region clusters</li> <li> <p>(Optional) In Remote Region cluster ARN, enter an ARN for an existing cluster in another Region. If no cluster exists to serve as the second cluster in your multi-Region cluster, complete setup after you create the initial cluster</p> </li> <li> <p>(Optional) In Cluster settings, select any of the following options for your initial cluster:</p> </li> <li>Select Customize encryption settings (advanced) to choose or create an AWS KMS key</li> <li>Select Enable deletion protection to prevent a delete operation from removing your cluster. By default, deletion protection is selected</li> <li> <p>Select Resource-based policy (advanced) to specify access control policies for this cluster</p> </li> <li> <p>Choose Create cluster to create your initial cluster. If you didn't enter an ARN in the previous step, the console shows the Cluster setup pending notification</p> </li> <li> <p>In the Cluster setup pending notification, choose Complete multi-Region cluster setup. This action initiates creation of a second cluster in another Region</p> </li> <li> <p>Choose one of the following options for your second cluster:</p> <ul> <li>Add remote Region cluster ARN \u2013 Choose this option if a cluster exists, and you want it to be the second cluster in your multi-Region cluster</li> <li>Create cluster in another Region \u2013 Choose this option to create a second cluster. In Remote Region, choose the Region for this second cluster</li> </ul> </li> <li> <p>Choose Create cluster in your-second-region, where <code>your-second-region</code> is the location of your second cluster. The console opens in your second Region</p> </li> <li> <p>(Optional) Choose cluster settings for your second cluster. For example, you can choose an AWS KMS key</p> </li> <li> <p>Choose Create cluster to create your second cluster</p> </li> <li> <p>Choose Peer in initial-cluster-region, where <code>initial-cluster-region</code> is the Region that hosts the first cluster that you created</p> </li> <li> <p>When prompted, choose Confirm. This step completes the creation of your multi-Region cluster</p> </li> </ol>"},{"location":"guides/getting-started/quickstart.html#connect-to-your-second-cluster","title":"Connect to your second cluster","text":"<ol> <li> <p>Open the Aurora DSQL console and choose the Region for your second cluster</p> </li> <li> <p>Choose Clusters</p> </li> <li> <p>Select the row for the second cluster in your multi-Region cluster</p> </li> <li> <p>Choose Connect with Query editor</p> </li> <li> <p>Choose Connect as admin</p> </li> <li> <p>Create a sample schema and table, and insert data by following the steps in Step 3: Run Sample SQL Commands</p> </li> </ol>"},{"location":"guides/getting-started/quickstart.html#query-data-across-regions","title":"Query data across Regions","text":"<p>To query data in the second cluster from the Region hosting your initial cluster:</p> <ol> <li> <p>In the Aurora DSQL console, choose the Region for your initial cluster</p> </li> <li> <p>Choose Clusters</p> </li> <li> <p>Select the row for the second cluster in your multi-Region cluster</p> </li> <li> <p>Choose Connect with Query editor</p> </li> <li> <p>Choose Connect as admin</p> </li> <li> <p>Query the data that you inserted into the second cluster:</p> </li> </ol> <pre><code>SELECT * FROM test.hello_world;\n</code></pre>"},{"location":"guides/getting-started/quickstart.html#troubleshooting","title":"Troubleshooting","text":"<p>See the Troubleshooting section of the Aurora DSQL documentation.</p>"},{"location":"guides/getting-started/quickstart.html#next-steps","title":"Next Steps","text":"<ul> <li>Learn about authentication and authorization</li> <li>Explore programming with Aurora DSQL</li> <li>Understand multi-Region clusters</li> <li>Review security best practices</li> </ul>"},{"location":"samples/index.html","title":"Aurora DSQL Samples","text":"<p>This repository contains code examples that demonstrate how to use the Aurora DSQL.</p> <p>To get started with Aurora DSQL, create clusters and more information, please refer to AWS Documentation</p>"},{"location":"samples/index.html#how-this-repository-is-organized","title":"How this repository is organized","text":"<p>The subdirectories contain code examples for connecting and using Aurora DSQL in each programming language and ORM framework. The examples demonstrate the most common uses, such as installing clients, handling authentication, performing CRUD operations, and more. Please refer to the documentation for a full list of differences and limitations.</p> Language Client / ORM C++ libpq C# (dotnet) Npgsql Go pgx Java HikariCP + pgJDBC Java Liquibase Java pgJDBC JavaScript AWS Lambda + node-postgres JavaScript node-postgres (standalone) JavaScript Postgres.js Python Jupyter Python psycopg Python psycopg2 Python SQLAlchemy Ruby Rails Ruby pg Rust sqlx Typescript Prisma Typescript Sequelize Typescript TypeORM Language Cluster Management C++ cluster_management C# (dotnet) cluster_management Go cluster_management Java cluster_management JavaScript cluster_management Python cluster_management Ruby cluster_management Rust cluster_management <p>Each example includes language and client-specific instructions as well as instructions to invoke example code.</p>"},{"location":"samples/index.html#security","title":"Security","text":"<p>See CONTRIBUTING for more information.</p>"},{"location":"samples/index.html#license","title":"License","text":"<p>This project is licensed under the MIT-0 License.</p>"},{"location":"samples/CODE_OF_CONDUCT.html","title":"CODE OF CONDUCT","text":""},{"location":"samples/CODE_OF_CONDUCT.html#code-of-conduct","title":"Code of Conduct","text":"<p>This project has adopted the Amazon Open Source Code of Conduct. For more information see the Code of Conduct FAQ or contact opensource-codeofconduct@amazon.com with any additional questions or comments.</p>"},{"location":"samples/CONTRIBUTING.html","title":"Contributing Guidelines","text":"<p>Thank you for your interest in contributing to our project. Whether it's a bug report, new feature, correction, or additional documentation, we greatly value feedback and contributions from our community.</p> <p>Please read through this document before submitting any issues or pull requests to ensure we have all the necessary information to effectively respond to your bug report or contribution.</p>"},{"location":"samples/CONTRIBUTING.html#reporting-bugsfeature-requests","title":"Reporting Bugs/Feature Requests","text":"<p>We welcome you to use the GitHub issue tracker to report bugs or suggest features.</p> <p>When filing an issue, please check existing open, or recently closed, issues to make sure somebody else hasn't already reported the issue. Please try to include as much information as you can. Details like these are incredibly useful:</p> <ul> <li>A reproducible test case or series of steps</li> <li>The version of our code being used</li> <li>Any modifications you've made relevant to the bug</li> <li>Anything unusual about your environment or deployment</li> </ul>"},{"location":"samples/CONTRIBUTING.html#contributing-via-pull-requests","title":"Contributing via Pull Requests","text":"<p>Contributions via pull requests are much appreciated. Before sending us a pull request, please ensure that:</p> <ol> <li>You are working against the latest source on the main branch.</li> <li>You check existing open, and recently merged, pull requests to make sure someone else hasn't addressed the problem already.</li> <li>You open an issue to discuss any significant work - we would hate for your time to be wasted.</li> </ol> <p>To send us a pull request, please:</p> <ol> <li>Fork the repository.</li> <li>Modify the source; please focus on the specific change you are contributing. If you also reformat all the code, it will be hard for us to focus on your change.</li> <li>Ensure local tests pass.</li> <li>Commit to your fork using clear commit messages.</li> <li>Send us a pull request, answering any default questions in the pull request interface.</li> <li>Pay attention to any automated CI failures reported in the pull request, and stay involved in the conversation.</li> </ol> <p>GitHub provides additional document on forking a repository and creating a pull request.</p>"},{"location":"samples/CONTRIBUTING.html#finding-contributions-to-work-on","title":"Finding contributions to work on","text":"<p>Looking at the existing issues is a great way to find something to contribute on. As our projects, by default, use the default GitHub issue labels (enhancement/bug/duplicate/help wanted/invalid/question/wontfix), looking at any 'help wanted' issues is a great place to start.</p>"},{"location":"samples/CONTRIBUTING.html#code-of-conduct","title":"Code of Conduct","text":"<p>This project has adopted the Amazon Open Source Code of Conduct. For more information see the Code of Conduct FAQ or contact opensource-codeofconduct@amazon.com with any additional questions or comments.</p>"},{"location":"samples/CONTRIBUTING.html#security-issue-notifications","title":"Security issue notifications","text":"<p>If you discover a potential security issue in this project we ask that you notify AWS/Amazon Security via our vulnerability reporting page. Please do not create a public github issue.</p>"},{"location":"samples/CONTRIBUTING.html#licensing","title":"Licensing","text":"<p>See the LICENSE file for our project's licensing. We will ask you to confirm the licensing of your contribution.</p>"},{"location":"samples/cpp/cluster_management/index.html","title":"Aurora DSQL CPP code examples","text":""},{"location":"samples/cpp/cluster_management/index.html#overview","title":"Overview","text":"<p>The code examples in this topic show you how to use the AWS CPP SDK with DSQL  to create, read, update, and delete single- and multi-Region clusters.</p> <p>Each *.cpp file in the src directory demonstrates a minimum working example for each operation. Each of the files can be independently compiled to produce an independent program that can be executed. The Example.cpp invokes each individual operation to create full examples of single- and multi-Region cluster lifecycles.</p>"},{"location":"samples/cpp/cluster_management/index.html#run-the-examples","title":"Run the examples","text":""},{"location":"samples/cpp/cluster_management/index.html#important","title":"\u26a0\ufe0f Important","text":"<ul> <li>Running this code might result in charges to your AWS account.</li> <li>We recommend that you grant your code least privilege. At most, grant only the   minimum permissions required to perform the task. For more information, see   Grant least privilege.</li> <li>This code is not tested in every AWS Region. For more information, see   AWS Regional Services.</li> </ul>"},{"location":"samples/cpp/cluster_management/index.html#prerequisites","title":"Prerequisites","text":""},{"location":"samples/cpp/cluster_management/index.html#c-compiler","title":"C++ compiler","text":"<p>A c++ compiler that supports c++11 standard or newer.</p>"},{"location":"samples/cpp/cluster_management/index.html#aws-sdk-for-c","title":"AWS SDK for C++","text":"<p>The AWS SDK for C++ installed</p> <ul> <li>The instructions for how to get and install the sdk can be found in the Official site</li> <li>The path to the AWS SDK libraries and include files will need to be specified for compilation</li> <li>The path to the AWS SDK libraries will need to be specified for execution</li> </ul> <p>Note If you're building the SDK from source and you only need it for dsql you may use the -DBUILD_ONLY=\"dsql\" flag to avoid building the entire sdk. For example:</p> <pre><code>cmake &lt;your_path&gt;/aws-sdk-cpp -DCMAKE_BUILD_TYPE=Release -DCMAKE_INSTALL_PREFIX=&lt;your_path_to_aws-sdk-install&gt; -DBUILD_ONLY=\"dsql\"\n\n# Note: Follow build and installation instructions on the official website. \n# This example is only meant to point to the -DBUILD_ONLY=\"dsql\" flag.\n</code></pre>"},{"location":"samples/cpp/cluster_management/index.html#build-the-example-program","title":"Build the example program","text":""},{"location":"samples/cpp/cluster_management/index.html#update-paths-in-the-makefile","title":"Update paths in the Makefile","text":"<p>Open the Makefile in the cluster_management/src directory and modify the path to aws sdk include files </p> <pre><code>AWS_INC_DIR=-I &lt;your_path_to_aws-sdk-install&gt;/include\n</code></pre> <p>and the path to aws sdk library files to match the path on your computer.</p> <pre><code>AWS_LIB_DIR=-L &lt;your_path_to_aws-sdk-install&gt;/lib\n</code></pre> <p>For Mac, depending on the location of the compiler files on your computer, you may or may not need to modify this variable as well:</p> <pre><code>COMPILER_INC_DIR_MAC\n\ne.g. \nCOMPILER_INC_DIR_MAC=-I /Library/Developer/CommandLineTools/SDKs/MacOSX14.x.sdk/usr/include/c++/v1\n\n# or \n\nCOMPILER_INC_DIR_MAC=-I /Library/Developer/CommandLineTools/SDKs/MacOSX15.x.sdk/usr/include/c++/v1\n</code></pre>"},{"location":"samples/cpp/cluster_management/index.html#build-the-example-program_1","title":"Build the example program","text":"<p>From the cluster_management/src directory run the following make command:</p>"},{"location":"samples/cpp/cluster_management/index.html#linux","title":"Linux","text":"<pre><code>make linux_example\n</code></pre>"},{"location":"samples/cpp/cluster_management/index.html#mac","title":"Mac","text":"<pre><code>make mac_example\n</code></pre> <p>This should result in the example executable program in the same directory.</p>"},{"location":"samples/cpp/cluster_management/index.html#build-the-programs-demonstrating-individual-operations","title":"Build the programs demonstrating individual operations","text":"<p>Each .cpp file, in addition to the full Example.cpp, in the src directory demonstrates a minimum working example for each operation. Each of the files can be independently compiled to produce an independent program that can be executed.</p> <p>Note Each of the *.cpp files illustrating a single operation contains the main() function.  However, the main() functions are wrapped around a conditional compilation variables like this:</p> <pre><code>//#define STANDALONE_MODE\n#ifdef STANDALONE_MODE\nint main() {}\n#endif // STANDALONE_MODE\n</code></pre> <p>In order to build the individual examples, open the corresponding .cpp file, and uncomment this line:</p> <pre><code>//#define STANDALONE_MODE\n</code></pre> <p>Then build the programs as follows</p>"},{"location":"samples/cpp/cluster_management/index.html#linux_1","title":"Linux","text":"<pre><code># to build all the small programs\nmake linux_all\n\n# to build a specific one run make corresponding to that operation, e.g. \n# to build CreateMultiRegion.cpp\nmake linux_create_multi\n\n# to build UpdateCluster.cpp\nmake linux_update\n</code></pre>"},{"location":"samples/cpp/cluster_management/index.html#mac_1","title":"Mac","text":"<pre><code># to build all the small programs\nmake mac_all\n\n# to build a specific one run make corresponding to that operation, e.g. \n# to build CreateMultiRegion.cpp\nmake mac_create_multi\n\n# to build UpdateCluster.cpp\nmake mac_update\n</code></pre>"},{"location":"samples/cpp/cluster_management/index.html#setup-test-running-environment","title":"Setup test running environment","text":"<p>Ensure you are authenticated with AWS credentials. </p>"},{"location":"samples/cpp/cluster_management/index.html#set-environment-variables-specifying-the-location-of-the-aws-sdk-libraries","title":"Set environment variables specifying the location of the aws sdk libraries","text":""},{"location":"samples/cpp/cluster_management/index.html#linux_2","title":"Linux","text":"<pre><code>export LD_LIBRARY_PATH=\"&lt;your_path_to_aws-sdk-install&gt;/lib:$LD_LIBRARY_PATH\"\n</code></pre>"},{"location":"samples/cpp/cluster_management/index.html#mac_2","title":"Mac","text":"<pre><code>export DYLD_FALLBACK_LIBRARY_PATH=&lt;your_path_to_aws-sdk-install&gt;/lib:$DYLD_FALLBACK_LIBRARY_PATH\n</code></pre>"},{"location":"samples/cpp/cluster_management/index.html#set-region-optional-and-cluster-environment-variables","title":"Set Region (optional) and cluster environment variables","text":""},{"location":"samples/cpp/cluster_management/index.html#programs-that-work-with-a-single-region-and-a-single-cluster","title":"Programs that work with a single region and a single cluster","text":"<pre><code># Defaults to 'us-east-1'\nexport CLUSTER_REGION=\"&lt;your region&gt;\"\n\n# e.g. \"foo0bar1baz2quux3quuux4\"\nexport CLUSTER_ID=\"&lt;your id&gt;\"\n</code></pre>"},{"location":"samples/cpp/cluster_management/index.html#programs-that-work-with-multi-region-clusters","title":"Programs that work with multi-region clusters","text":"<pre><code># Defaults to 'us-east-1'\nexport CLUSTER_1_REGION=\"&lt;your region 1&gt;\"\n\n# Defaults to 'us-east-2'\nexport CLUSTER_2_REGION=\"&lt;your region 2&gt;\"\n\n# Defaults to 'us-west-2'\nexport WITNESS_REGION=\"&lt;your witness region&gt;\"\n\n# e.g. \"foo0bar1baz2quux3quuux4\"\nexport CLUSTER_1_ID=\"&lt;your id 1&gt;\"\nexport CLUSTER_2_ID=\"&lt;your id 2&gt;\"\n</code></pre>"},{"location":"samples/cpp/cluster_management/index.html#run-the-example-program","title":"Run the example program","text":"<p>Note</p> <p>To execute the example code, you need to have valid AWS Credentials configured (e.g. AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY and AWS_SESSION_TOKEN)</p> <p>In a terminal run the following command from the cluster_management/src directory </p> <pre><code>./example\n</code></pre> <p>Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved. </p> <p>SPDX-License-Identifier: MIT-0</p>"},{"location":"samples/cpp/libpq/index.html","title":"Aurora DSQL with libpq","text":""},{"location":"samples/cpp/libpq/index.html#overview","title":"Overview","text":"<p>This code example demonstrates how to use the Libpq library to interact with Amazon Aurora DSQL (DSQL). The example shows you how to connect to an Aurora DSQL cluster and perform basic database operations.</p> <p>Aurora DSQL is a distributed SQL database service that provides high availability and scalability for your PostgreSQL-compatible applications. Libpq is a popular PostgreSQL library that allows you to interact with PostgreSQL databases using c/cpp code.</p>"},{"location":"samples/cpp/libpq/index.html#about-the-code-example","title":"About the code example","text":"<p>The example demonstrates a flexible connection approach that works for both admin and non-admin users:</p> <ul> <li>When connecting as an admin user, the example uses the <code>public</code> schema and generates an admin authentication   token.</li> <li>When connecting as a non-admin user, the example uses a custom <code>myschema</code> schema and generates a standard   authentication token. The <code>myschema</code> schema needs to be created prior to running the example and the non-admin user needs to be granted access to the schema.</li> </ul> <p>The code automatically detects the user type and adjusts its behavior accordingly. The example contains comments explaining the code and the operations being performed.</p>"},{"location":"samples/cpp/libpq/index.html#important","title":"\u26a0\ufe0f Important","text":"<ul> <li>Running this code might result in charges to your AWS account.</li> <li>We recommend that you grant your code least privilege. At most, grant only the   minimum permissions required to perform the task. For more information, see   Grant least privilege.</li> <li>This code is not tested in every AWS Region. For more information, see   AWS Regional Services.</li> </ul>"},{"location":"samples/cpp/libpq/index.html#tls-connection-configuration","title":"TLS connection configuration","text":"<p>This example uses direct TLS connections where supported, and verifies the server certificate is trusted. Verified SSL connections should be used where possible to ensure data security during transmission.</p> <ul> <li>Driver versions following the release of PostgreSQL 17 support direct TLS connections, bypassing the traditional   PostgreSQL connection preamble</li> <li>Direct TLS connections provide improved connection performance and enhanced security</li> <li>Not all PostgreSQL drivers support direct TLS connections yet, or only in recent versions following PostgreSQL 17</li> <li>Ensure your installed driver version supports direct TLS negotiation, or use a version that is at least as recent as   the one used in this sample</li> <li>If your driver doesn't support direct TLS connections, you may need to use the traditional preamble connection instead</li> </ul>"},{"location":"samples/cpp/libpq/index.html#run-the-example","title":"Run the example","text":""},{"location":"samples/cpp/libpq/index.html#prerequisites","title":"Prerequisites","text":"<ul> <li>You must have an AWS account, and have your default credentials and AWS Region   configured as described in the   Globally configuring AWS SDKs and tools   guide.</li> <li>You must have an Aurora DSQL cluster. For information about creating an Aurora DSQL cluster, see the   Getting started with Aurora DSQL   guide.</li> <li>If connecting as a non-admin user, ensure the user is linked to an IAM role and is granted access to the <code>myschema</code>   schema. See the   Using database roles with IAM roles   guide.</li> </ul>"},{"location":"samples/cpp/libpq/index.html#download-the-amazon-root-certificate-from-the-official-trust-store","title":"Download the Amazon root certificate from the official trust store","text":"<p>Download the Amazon root certificate from the official trust store:</p> <pre><code>wget https://www.amazontrust.com/repository/AmazonRootCA1.pem -O root.pem\n</code></pre>"},{"location":"samples/cpp/libpq/index.html#c-compiler","title":"C++ compiler","text":"<p>A c++ compiler that supports c++11 standard or newer.</p>"},{"location":"samples/cpp/libpq/index.html#aws-sdk-for-c","title":"AWS SDK for C++","text":"<p>The AWS SDK for C++ installed</p> <ul> <li>The instructions how to get and install the sdk can be found in the Official site</li> <li>The path to the AWS SDK libraries and include files will need to be specified for compilation</li> <li>The path to the AWS SDK libraries will need to be specified for execution</li> </ul> <p>Note If you're building the SDK from source and you only need it for dsql you may use the -DBUILD_ONLY=\"dsql\" flag to avoid building the entire sdk. For example:</p> <pre><code>cmake &lt;your_path&gt;/aws-sdk-cpp -DCMAKE_BUILD_TYPE=Release -DCMAKE_INSTALL_PREFIX=&lt;your_path_to_aws-sdk-install&gt; -DBUILD_ONLY=\"dsql\"\n\n# Note: Follow build and installation instructions on the official website. \n# This example is meant to point to the -DBUILD_ONLY=\"dsql\" flag.\n</code></pre>"},{"location":"samples/cpp/libpq/index.html#libpq-library-and-postgres-include-files","title":"Libpq library and Postgres include files","text":"<ul> <li>The path to the Libpq library and include files will need to be specified for compilation</li> <li>The path to the Libpq library will need to be specified for execution</li> <li>Obtaining Libpq library<ul> <li>It is installed with postgres installation. Therefore, if postgres is installed on the system the libpq is present in ../postgres_install_dir/lib, ../postgres_install_dir/include</li> <li>It is installed when psql client program is installed, similarly as with postgres installation</li> <li>On some systems libpq can be installed through package manager (if the package exists for the system) e.g.     <pre><code>sudo yum install libpq-devel\n</code></pre></li> <li>On Mac libpq can be installed using brew     <pre><code>brew install libpq\n</code></pre></li> <li>The official website may have a package for libpq or psql which bundles libpq</li> <li>The last resort, build from source which also can be obtained from official website </li> </ul> </li> </ul>"},{"location":"samples/cpp/libpq/index.html#ssl-libraries","title":"SSL Libraries","text":"<ul> <li>SSL libraries need to be installed</li> <li>For example on Amazon Linux run these commands:     <pre><code>  sudo yum install -y openssl-devel \n  sudo yum install -y  openssl11-libs \n</code></pre></li> <li>On some systems the SSL libraries can be installed using package managers<ul> <li>They can be downloaded from the official website</li> </ul> </li> </ul>"},{"location":"samples/cpp/libpq/index.html#build-the-example-program","title":"Build the example program","text":""},{"location":"samples/cpp/libpq/index.html#edit-the-makefile-file","title":"Edit the Makefile file","text":"<p>The Makefile is located in the libpq/src directory.</p>"},{"location":"samples/cpp/libpq/index.html#location-of-the-awd-sdk-cpp","title":"Location of the awd-sdk-cpp","text":"<p>Update the following variables with the paths to the aws-sdk-cpp include and library files on your computer:</p> <pre><code>AWS_INC_DIR=-I &lt;your_path_to_aws-sdk-install&gt;/include\nAWS_LIB_DIR=-L &lt;your_path_to_aws-sdk-install&gt;/lib\n</code></pre>"},{"location":"samples/cpp/libpq/index.html#linux","title":"Linux","text":"<p>Edit the variables specifying path to the postgres include files and path to the location of the libpq library.</p> <p>Relace the /usr/local/pgsql/include and /usr/local/pgsql/lib with locations on your computer:</p> <pre><code>PG_INC_DIR=-I /usr/local/pgsql/include\nLIBPQ_DIR=-L /usr/local/pgsql/lib\n</code></pre> <p>Note:</p> <p>If you have the pg_config utility installed you can use the following commands to retrieve the above directories:</p> <pre><code>pg_config --includedir\npg_config --libdir\n</code></pre>"},{"location":"samples/cpp/libpq/index.html#mac","title":"Mac","text":"<p>The Mac related variables are in the 'Mac' section of the Makefile. Edit the variables specifying path to the postgres include files and path to the location of the libpq library as well as the compiler include directory.</p> <p>Note: These are examples only. Replace them with your path.</p> <pre><code>(x86)\nPG_INC_DIR_MAC=-I /usr/local/opt/libpq/include\nLIBPQ_DIR_MAC=-L /usr/local/opt/libpq/lib\nOR\n(brew)\nPG_INC_DIR_MAC=-I /opt/homebrew/opt/postgresql@16/include\nLIBPQ_DIR_MAC=-L /opt/homebrew/opt/postgresql@16/lib\n\nCOMPILER_INC_DIR_MAC=-I /Library/Developer/CommandLineTools/SDKs/MacOSX14.x.sdk/usr/include/c++/v1\n\n# or could be \n\nCOMPILER_INC_DIR_MAC=-I /Library/Developer/CommandLineTools/SDKs/MacOSX15.x.sdk/usr/include/c++/v1\n</code></pre>"},{"location":"samples/cpp/libpq/index.html#build-the-program","title":"Build the program","text":"<p>From the libpq/src directory run make command:</p>"},{"location":"samples/cpp/libpq/index.html#linux_1","title":"Linux","text":"<pre><code>make libpq_example\n</code></pre>"},{"location":"samples/cpp/libpq/index.html#mac_1","title":"Mac","text":"<pre><code>make libpq_example_mac\n</code></pre> <p>This should result in the libpq_example executable program</p>"},{"location":"samples/cpp/libpq/index.html#run-the-example-program","title":"Run the example program","text":"<p>Note</p> <p>To execute the example code, you need to have valid AWS Credentials configured (e.g. AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY and AWS_SESSION_TOKEN)</p>"},{"location":"samples/cpp/libpq/index.html#set-environment-variables-specifying-the-location-of-the-libpq-and-aws-sdk-libraries","title":"Set environment variables specifying the location of the libpq and aws sdk libraries","text":"<p>Run the commands below. Replace the paths in the commands below with the path on your computer.</p>"},{"location":"samples/cpp/libpq/index.html#linux_2","title":"Linux","text":"<pre><code>export LD_LIBRARY_PATH=\"/usr/local/pgsql/lib:$LD_LIBRARY_PATH\"\nexport LD_LIBRARY_PATH=\"&lt;your_path_to_aws-sdk-install&gt;/lib:$LD_LIBRARY_PATH\"\n</code></pre>"},{"location":"samples/cpp/libpq/index.html#mac_2","title":"Mac","text":"<pre><code>export DYLD_FALLBACK_LIBRARY_PATH=&lt;your_path_to_aws-sdk-install&gt;/lib\n</code></pre>"},{"location":"samples/cpp/libpq/index.html#set-environment-variables-specifying-cluster-endpoint-and-region","title":"Set environment variables specifying cluster endpoint and region","text":"<pre><code># e.g. 'admin' or a custom user \nexport CLUSTER_USER=&lt;your cluster user&gt; \n\n# e.g. \"foo0bar1baz2quux3quuux4.dsql.us-east-1.on.aws\"\nexport CLUSTER_ENDPOINT=\"&lt;your cluster endpoint&gt;\"\n\n# e.g. \"us-east-1\"\nexport REGION=\"&lt;your cluster region&gt;\"\n</code></pre>"},{"location":"samples/cpp/libpq/index.html#run-the-example-program_1","title":"Run the example program","text":"<p>From the libpq/src directory run:</p> <pre><code>./libpq_example\n</code></pre>"},{"location":"samples/cpp/libpq/index.html#troubleshooting","title":"Troubleshooting","text":""},{"location":"samples/cpp/libpq/index.html#ssl-support-in-libpq","title":"SSL support in libpq","text":"<p>Aurora DSQL requires SSL when connecting to it. Therefore, the libpq library must have been built with SSL support.</p> <p>If this is not the case, you may see the following error message while executing the libpq_example program:</p> <p>Error while connecting to the database server: sslmode value \"require\" invalid when SSL support is not compiled in.</p> <p>Hopefully, the libpq library that is distributed with PostgreSQL installation as well as through package managers has the SSL built in. </p> <p>It may not be the case if you've built it yourself from sources. In this case make sure to include the SSL when building.</p> <p>Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.</p> <p>SPDX-License-Identifier: MIT-0</p>"},{"location":"samples/dotnet/cluster_management/index.html","title":"Aurora DSQL C# code examples","text":""},{"location":"samples/dotnet/cluster_management/index.html#overview","title":"Overview","text":"<p>The code examples in this directory show you how to use the AWS .NET SDK with DSQL to create, update, get, and delete single- and multi-Region clusters.</p> <p>Each project in the examples directory demonstrates a minimum working example for each operation and produces an independent executable that can be run.</p>"},{"location":"samples/dotnet/cluster_management/index.html#run-the-examples","title":"Run the examples","text":""},{"location":"samples/dotnet/cluster_management/index.html#important","title":"\u26a0\ufe0f Important","text":"<ul> <li>Running this code might result in charges to your AWS account.</li> <li>We recommend that you grant your code least privilege. At most, grant only the minimum permissions required to perform   the task. For more information,   see Grant least privilege.</li> <li>This code is not tested in every AWS Region. For more information,   see AWS Regional Services.</li> </ul>"},{"location":"samples/dotnet/cluster_management/index.html#prerequisites","title":"Prerequisites","text":"<ul> <li>You must have an AWS account, and have your default credentials and AWS Region configured as described in   the Globally configuring AWS SDKs and tools</li> <li>You must have .NET 9.0 SDK installed.</li> </ul>"},{"location":"samples/dotnet/cluster_management/index.html#configure-the-environment","title":"Configure the environment","text":"<p>Set environment variables with your cluster details as required.</p>"},{"location":"samples/dotnet/cluster_management/index.html#single-region-clusters","title":"Single-region clusters","text":"<pre><code># Only relevant for delete/get/update examples.\n# e.g. \"foo0bar1baz2quux3quuux4\"\nexport CLUSTER_ID=\"&lt;your id&gt;\"\n\n# Relevant for all examples.\n# e.g. \"us-east-1\"\nexport CLUSTER_REGION=\"&lt;your region&gt;\"\n</code></pre>"},{"location":"samples/dotnet/cluster_management/index.html#multi-region-clusters","title":"Multi-region clusters","text":"<pre><code># Only relevant for delete/get/update examples.\n# e.g. \"foo0bar1baz2quux3quuux4\"\nexport CLUSTER_1_ID=\"&lt;your id 1&gt;\"\nexport CLUSTER_2_ID=\"&lt;your id 2&gt;\"\n\n# Relevant for all examples.\n# e.g. \"us-east-1\"\nexport CLUSTER_1_REGION=\"&lt;your region 1&gt;\"\nexport CLUSTER_2_REGION=\"&lt;your region 2&gt;\"\n\n# Only relevant for create examples.\nexport WITNESS_REGION=\"&lt;your region 3&gt;\"\n</code></pre>"},{"location":"samples/dotnet/cluster_management/index.html#run-the-examples_1","title":"Run the examples","text":"<p>Each example is a standalone project that can be run independently:</p> <pre><code># Build all examples\ndotnet build\n\n# Run a specific example\ndotnet run --framework net9.0 --project examples/CreateSingleRegionCluster\ndotnet run --framework net9.0 --project examples/CreateMultiRegionClusters\ndotnet run --framework net9.0 --project examples/GetCluster\ndotnet run --framework net9.0 --project examples/UpdateCluster\ndotnet run --framework net9.0 --project examples/DeleteSingleRegionCluster\ndotnet run --framework net9.0 --project examples/DeleteMultiRegionClusters\n</code></pre>"},{"location":"samples/dotnet/cluster_management/index.html#run-the-tests","title":"Run the tests","text":"<p>The project includes unit tests that exercise the full lifecycle of both single-region and multi-region clusters:</p> <pre><code># Run all tests\ndotnet test\n\n# Run a specific test\ndotnet test --filter \"DisplayName~TestSingleRegionClusterLifecycle\"\ndotnet test --filter \"DisplayName~TestMultiRegionClusterLifecycle\"\n</code></pre> <p>These examples do not wait for cluster operations to complete. Clusters may still be changing state after an example has finished executing.</p> <p>Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.</p> <p>SPDX-License-Identifier: MIT-0</p>"},{"location":"samples/dotnet/npgsql/index.html","title":"Aurora DSQL with Npgsql","text":""},{"location":"samples/dotnet/npgsql/index.html#overview","title":"Overview","text":"<p>This code example demonstrates how to use Npgsql with Amazon Aurora SQL (DSQL). The example shows you how to connect to an Aurora DSQL cluster and perform basic database operations.</p> <p>Aurora DSQL is a distributed SQL database service that provides high availability and scalability for your PostgreSQL-compatible applications. Npgsql is a popular PostgreSQL adapter for .NET that allows you to interact with PostgreSQL databases using C# code.</p>"},{"location":"samples/dotnet/npgsql/index.html#about-the-code-example","title":"About the code example","text":"<p>The example demonstrates a flexible connection approach that works for both admin and non-admin users:</p> <ul> <li>When connecting as an admin user, the example uses the <code>public</code> schema and generates an admin authentication   token.</li> <li>When connecting as a non-admin user, the example uses a custom <code>myschema</code> schema and generates a standard   authentication token.</li> </ul> <p>The code automatically detects the user type and adjusts its behavior accordingly.</p>"},{"location":"samples/dotnet/npgsql/index.html#important","title":"\u26a0\ufe0f Important","text":"<ul> <li>Running this code might result in charges to your AWS account.</li> <li>We recommend that you grant your code least privilege. At most, grant only the   minimum permissions required to perform the task. For more information, see   Grant least privilege.</li> <li>This code is not tested in every AWS Region. For more information, see   AWS Regional Services.</li> </ul>"},{"location":"samples/dotnet/npgsql/index.html#tls-connection-configuration","title":"TLS connection configuration","text":"<p>This example uses direct TLS connections where supported, and verifies the server certificate is trusted. Verified SSL connections should be used where possible to ensure data security during transmission.</p> <ul> <li>Driver versions following the release of PostgreSQL 17 support direct TLS connections, bypassing the traditional   PostgreSQL connection preamble</li> <li>Direct TLS connections provide improved connection performance and enhanced security</li> <li>Not all PostgreSQL drivers support direct TLS connections yet, or only in recent versions following PostgreSQL 17</li> <li>Ensure your installed driver version supports direct TLS negotiation, or use a version that is at least as recent as   the one used in this sample</li> <li>If your driver doesn't support direct TLS connections, you may need to use the traditional preamble connection instead</li> </ul>"},{"location":"samples/dotnet/npgsql/index.html#run-the-example","title":"Run the example","text":""},{"location":"samples/dotnet/npgsql/index.html#prerequisites","title":"Prerequisites","text":"<ul> <li>You must have an AWS account, and have your default credentials and AWS Region   configured as described in the   Globally configuring AWS SDKs and tools   guide.</li> <li>.NET 9.0 SDK or later.</li> <li>You must have an Aurora DSQL cluster. For information about creating an Aurora DSQL cluster, see the   Getting started with Aurora DSQL   guide.</li> <li>If connecting as a non-admin user, ensure the user is linked to an IAM role and is granted access to the <code>myschema</code>   schema. See the   Using database roles with IAM roles   guide.</li> </ul>"},{"location":"samples/dotnet/npgsql/index.html#run-the-code","title":"Run the code","text":"<p>The example demonstrates the following operations:</p> <ul> <li>Opening a connection to an Aurora DSQL cluster</li> <li>Creating a table</li> <li>Inserting and querying data</li> </ul> <p>The example is designed to work with both admin and non-admin users:</p> <ul> <li>When run as an admin user, it uses the <code>public</code> schema</li> <li>When run as a non-admin user, it uses the <code>myschema</code> schema</li> </ul> <p>Note: running the example will use actual resources in your AWS account and may incur charges.</p> <p>Set environment variables for your cluster details:</p> <pre><code># e.g. \"admin\"\nexport CLUSTER_USER=\"&lt;your user&gt;\"\n\n# e.g. \"foo0bar1baz2quux3quuux4.dsql.us-east-1.on.aws\"\nexport CLUSTER_ENDPOINT=\"&lt;your endpoint&gt;\"\n\n# e.g. \"us-east-1\"\nexport REGION=\"&lt;your region&gt;\"\n</code></pre> <p>Run the example:</p> <pre><code>dotnet run --framework net9.0\n</code></pre> <p>The example contains comments explaining the code and the operations being performed.</p>"},{"location":"samples/dotnet/npgsql/index.html#additional-resources","title":"Additional resources","text":"<ul> <li>Amazon Aurora DSQL Documentation</li> <li>Npgsql Documentation</li> </ul> <p>Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.</p> <p>SPDX-License-Identifier: MIT-0</p>"},{"location":"samples/go/cluster_management/index.html","title":"Aurora DSQL Go SDK code examples","text":""},{"location":"samples/go/cluster_management/index.html#overview","title":"Overview","text":"<p>The code examples in this topic show you how to use the AWS Go SDK with DSQL to create, read, update, and delete clusters.</p>"},{"location":"samples/go/cluster_management/index.html#run-the-examples","title":"Run the examples","text":""},{"location":"samples/go/cluster_management/index.html#important","title":"\u26a0\ufe0f Important","text":"<ul> <li>Running this code might result in charges to your AWS account.</li> <li>We recommend that you grant your code least privilege. At most, grant only the   minimum permissions required to perform the task. For more information, see   Grant least privilege.</li> <li>This code is not tested in every AWS Region. For more information, see   AWS Regional Services.</li> </ul>"},{"location":"samples/go/cluster_management/index.html#prerequisites","title":"Prerequisites","text":"<ul> <li>Go version &gt;= 1.21</li> <li>Valid AWS credentials can be discovered by the default provider chain.</li> </ul>"},{"location":"samples/go/cluster_management/index.html#setup-test-running-environment","title":"Setup test running environment","text":"<p>Ensure you are authenticated with AWS credentials. No other setup is needed besides having Go installed.</p>"},{"location":"samples/go/cluster_management/index.html#run-the-example-tests","title":"Run the example tests","text":"<p>In a terminal run the following commands:</p>"},{"location":"samples/go/cluster_management/index.html#execute-tests-to-create-and-delete-clusters","title":"Execute tests to create and delete clusters","text":"<pre><code>make test\n</code></pre> <p>OR</p> <pre><code>go env -w GOPROXY=direct\ngo test -v -count=1 ./cmd/create_multi_region\ngo test -v -count=1 ./cmd/create_single_region\ngo test -v -count=1 ./cmd/get_cluster\ngo test -v -count=1 ./cmd/update_cluster\ngo test -v -count=1 ./cmd/delete_multi_region\ngo test -v -count=1 ./cmd/delete_single_region\n</code></pre> <p>Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.</p> <p>SPDX-License-Identifier: MIT-0</p>"},{"location":"samples/go/pgx/index.html","title":"Aurora DSQL with pgx","text":""},{"location":"samples/go/pgx/index.html#overview","title":"Overview","text":"<p>This code example demonstrates how to use the <code>pgx</code> driver with Amazon Aurora DSQL. The example shows you how to connect to an Aurora DSQL cluster and perform database operations using IAM authentication.</p> <p>Aurora DSQL is a distributed SQL database service that provides high availability and scalability for your PostgreSQL-compatible applications. <code>pgx</code> is a pure Go driver and toolkit for PostgreSQL that offers robust features including automatic connection pool management and authentication token refresh.</p>"},{"location":"samples/go/pgx/index.html#about-the-code-example","title":"About the code example","text":"<p>The example demonstrates a flexible connection approach using IAM authentication:</p> <ul> <li>Implements automatic token generation for new connections</li> <li>Handles secure IAM-based authentication token generation</li> <li>Provides connection pooling and management</li> <li>Demonstrates best practices for Aurora DSQL connectivity in Go applications</li> </ul>"},{"location":"samples/go/pgx/index.html#important","title":"\u26a0\ufe0f Important","text":"<ul> <li>Running this code might result in charges to your AWS account.</li> <li>We recommend that you grant your code least privilege. At most, grant only the   minimum permissions required to perform the task. For more information, see   Grant least privilege in the AWS IAM User Guide.</li> <li>This code is not tested in every AWS Region. For more information, see   AWS Regional Services.</li> </ul>"},{"location":"samples/go/pgx/index.html#tls-connection-configuration","title":"TLS connection configuration","text":"<p>This example uses direct TLS connections where supported, and verifies the server certificate is trusted. Verified SSL connections should be used where possible to ensure data security during transmission.</p> <ul> <li>Driver versions following the release of PostgreSQL 17 support direct TLS connections, bypassing the traditional   PostgreSQL connection preamble</li> <li>Direct TLS connections provide improved connection performance and enhanced security</li> <li>Not all PostgreSQL drivers support direct TLS connections yet, or only in recent versions following PostgreSQL 17</li> <li>Ensure your installed driver version supports direct TLS negotiation, or use a version that is at least as recent as   the one used in this sample</li> <li>If your driver doesn't support direct TLS connections, you may need to use the traditional preamble connection instead</li> </ul>"},{"location":"samples/go/pgx/index.html#configuration","title":"Configuration","text":"<p>The following environment variables can be used to configure the connection parameter in this example:</p> <ul> <li><code>CLUSTER_ENDPOINT</code>: Your Aurora DSQL cluster endpoint (required)</li> <li><code>CLUSTER_USER</code>: Database user (required)</li> <li><code>REGION</code>: AWS region where your cluster is located (required)</li> <li><code>DB_PORT</code>: Database port (defaults to 5432)</li> <li><code>DB_NAME</code>: Database name (defaults to \"postgres\")</li> </ul>"},{"location":"samples/go/pgx/index.html#run-the-examples","title":"Run the examples","text":""},{"location":"samples/go/pgx/index.html#prerequisites","title":"Prerequisites","text":"<ul> <li>You must have an AWS account, and have your default credentials and AWS Region   configured as described in the   Globally configuring AWS SDKs and tools   guide.</li> <li>You must have an Aurora DSQL cluster. For information about creating an Aurora DSQL cluster, see the   Getting started with Aurora DSQL   guide.</li> <li>If connecting as a non-admin user, ensure the user is linked to an IAM role and is granted access to the <code>myschema</code>   schema. See the   Using database roles with IAM roles   guide.</li> <li>Go version &gt;= 1.24</li> </ul>"},{"location":"samples/go/pgx/index.html#setup-test-running-environment","title":"Setup test running environment","text":"<p>Ensure you are authenticated with AWS credentials. No other setup is needed besides having Go installed.</p>"},{"location":"samples/go/pgx/index.html#environment-variables","title":"Environment Variables","text":"<p>Set the following required environment variables:</p> <pre><code># Your cluster endpoint (e.g., \"cluster-name.cluster-xxx.region.rds.amazonaws.com\")\nexport CLUSTER_ENDPOINT=\"&lt;your cluster endpoint&gt;\"\n\n# Your AWS region (e.g., \"us-east-1\")\nexport REGION=\"&lt;your cluster region&gt;\"\n</code></pre>"},{"location":"samples/go/pgx/index.html#run-the-example-tests","title":"Run the example tests","text":"<p>In a terminal run the following commands:</p> <pre><code># Run the unit tests\ngo env -w GOPROXY=direct\ngo test\n\n# Run the example directly\ngo build -o example\n./example\n</code></pre>"},{"location":"samples/go/pgx/index.html#token-generation","title":"Token Generation","text":"<p>The implementation includes an automatic token generation mechanism for new connections. This ensures continuous database connectivity for the pool. Lazily created connections will generate a new authentication token before connecting. Similarly, replacement connections for those exceeding the pool connection lifetime will generate their own fresh authentication token.</p>"},{"location":"samples/go/pgx/index.html#additional-resources","title":"Additional resources","text":"<ul> <li>Amazon Aurora DSQL Documentation</li> <li>pgx Documentation</li> </ul> <p>Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.</p> <p>SPDX-License-Identifier: MIT-0</p>"},{"location":"samples/java/cluster_management/index.html","title":"Aurora DSQL Java code examples","text":""},{"location":"samples/java/cluster_management/index.html#overview","title":"Overview","text":"<p>The code examples in this topic show you how to use the AWS Java SDK v2 with DSQL to create, update, get, and delete single- and multi-Region clusters.</p> <p>Each file in the /example directory demonstrates a minimal working example for each operation. The <code>example()</code> method for each operation is invoked in <code>DsqlClusterManagementTest.java</code>.</p>"},{"location":"samples/java/cluster_management/index.html#run-the-examples","title":"Run the examples","text":""},{"location":"samples/java/cluster_management/index.html#important","title":"\u26a0\ufe0f Important","text":"<ul> <li>Running this code might result in charges to your AWS account.</li> <li>We recommend that you grant your code least privilege. At most, grant only the   minimum permissions required to perform the task. For more information, see   Grant least privilege.</li> <li>This code is not tested in every AWS Region. For more information, see   AWS Regional Services.</li> </ul>"},{"location":"samples/java/cluster_management/index.html#prerequisites","title":"Prerequisites","text":"<ul> <li>java version &gt;= 17 is installed.</li> <li>Valid AWS credentials can be discovered by the default provider chain.</li> </ul>"},{"location":"samples/java/cluster_management/index.html#execute-tests-to-create-and-delete-clusters","title":"Execute tests to create and delete clusters","text":"<p>Optionally configure the regions for cluster creation and run with <code>mvn test</code>:</p> <pre><code># Optional: Single-Region examples will execute in CLUSTER_REGION. Defaults to 'us-east-1'.\nexport CLUSTER_REGION=\"us-east-1\"\n\n# Optional: Multi-Region examples will create clusters in CLUSTER_1_REGION and CLUSTER_2_REGION\n# with WITNESS_REGION as witness for both. Defaults to 'us-east-2' for CLUSTER_2_REGION\n# and 'us-west-2' for WITNESS_REGION.\nexport CLUSTER_1_REGION=\"us-east-1\"\nexport CLUSTER_2_REGION=\"us-east-2\"\nexport WITNESS_REGION=\"us-west-2\"\n\n# Will create, update, read, then delete clusters\nmvn test\n</code></pre> <p>Test execution will take around five minutes as it waits for clusters to complete activation and deletion.</p>"},{"location":"samples/java/cluster_management/index.html#executing-single-operations","title":"Executing single operations","text":"<p>Files in src/../example/ each have a <code>main()</code> method that let you exercise single operations.</p> <p>The build process will produce a single <code>.jar</code> that can be invoked as:</p> <pre><code># Build the project if this has not been done yet with 'mvn test'\nmvn clean compile assembly:single\n\n# Check each operation for its expected environment variables\nCLUSTER_REGION=\"us-east-1\" CLUSTER_ID=\"&lt;your cluster id&gt;\" \\\n  java \\\n  -cp target/AuroraDSQLClusterCrudExample-1.0-SNAPSHOT-jar-with-dependencies.jar \\\n  org.example.GetCluster\n</code></pre> <p>Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.</p> <p>SPDX-License-Identifier: MIT-0</p>"},{"location":"samples/java/liquibase/index.html","title":"Aurora DSQL with Liquibase","text":""},{"location":"samples/java/liquibase/index.html#overview","title":"Overview","text":"<p>This code example demonstrates how to use Liquibase with Amazon Aurora DSQL. The example shows you how to connect to an Aurora DSQL cluster and manage database schema changes using Liquibase's database migration capabilities.</p> <p>Aurora DSQL is a distributed SQL database service that provides high availability and scalability for your PostgreSQL-compatible applications. Liquibase is a database schema change management tool that allows you to track, version, and deploy database changes in a structured and repeatable way.</p>"},{"location":"samples/java/liquibase/index.html#about-the-code-example","title":"About the code example","text":"<p>The example includes both SQL and JSON changelog formats demonstrating:</p> <ul> <li>Creating Pet-Clinic tables with UUID primary keys</li> <li>Creating indexes with DSQL's <code>ASYNC</code> syntax</li> <li>Inserting sample data</li> <li>Proper rollback configurations for each changeset</li> </ul>"},{"location":"samples/java/liquibase/index.html#configuration","title":"Configuration","text":"<p>Liquibase can be configured using Maven, Gradle, a liquibase.properties file, Docker, Spring Boot, or Java classes, and  can be incorporated into a larger application or AWS Lambda. The Liquibase options available are similar in all.</p> <p>A dependency manager like Maven or Gradle is strongly recommended to use the Aurora DSQL Connector for JDBC with Liquibase in order to include the connector's dependencies.</p> <p>This example includes a <code>pom.xml</code> file that configures the Liquibase Maven plugin with all connection details embedded directly in the configuration:</p> <ul> <li>DSQL JDBC Connector: Uses <code>software.amazon.dsql:aurora-dsql-jdbc-connector</code> for Aurora DSQL connectivity</li> <li>Connection URL: Dynamically constructed using the <code>CLUSTER_ENDPOINT</code> environment variable with the format <code>jdbc:aws-dsql:postgresql://${env.CLUSTER_ENDPOINT}:5432/postgres</code></li> <li>Authentication: Configured for the <code>admin</code> user with IAM-based authentication</li> <li>Changelog: Defaults to <code>changelog.sql</code> but can be overridden via command line</li> </ul>"},{"location":"samples/java/liquibase/index.html#important","title":"\u26a0\ufe0f Important","text":"<ul> <li>Running this code might result in charges to your AWS account.</li> <li>We recommend that you grant your code least privilege. At most, grant only the   minimum permissions required to perform the task. For more information, see   Grant least privilege.</li> <li>This code is not tested in every AWS Region. For more information, see   AWS Regional Services.</li> </ul>"},{"location":"samples/java/liquibase/index.html#run-the-example","title":"Run the example","text":""},{"location":"samples/java/liquibase/index.html#prerequisites","title":"Prerequisites","text":"<ul> <li>You must have an AWS account, and have your default credentials and AWS Region   configured as described in the   Globally configuring AWS SDKs and tools   guide.</li> <li>Java 11 or later.</li> <li>Apache Maven 3.6 or later.</li> <li>You must have an Aurora DSQL cluster. For information about creating an Aurora DSQL cluster, see the   Getting started with Aurora DSQL   guide.</li> </ul>"},{"location":"samples/java/liquibase/index.html#set-environment-variables","title":"Set environment variables","text":"<p>Set the cluster endpoint environment variable:</p> <pre><code># e.g. \"foo0bar1baz2quux3quuux4.dsql.us-east-1.on.aws\"\nexport CLUSTER_ENDPOINT=\"&lt;your endpoint&gt;\"\n</code></pre>"},{"location":"samples/java/liquibase/index.html#run-database-migrations","title":"Run database migrations","text":"<p>Apply the database migrations using the SQL changelog:</p> <pre><code># Apply all pending changesets\nmvn liquibase:update\n</code></pre> <p>Or use the JSON changelog:</p> <pre><code># Apply JSON format changesets\nmvn liquibase:update -Dliquibase.changeLogFile=changelog.json\n</code></pre>"},{"location":"samples/java/liquibase/index.html#view-migration-status","title":"View migration status","text":"<p>Check the status of your migrations:</p> <pre><code># Show migration status\nmvn liquibase:status\n</code></pre>"},{"location":"samples/java/liquibase/index.html#rollback-changes","title":"Rollback changes","text":"<p>Rollback one or more changesets:</p> <pre><code># Rollback the most recent changeset\nmvn liquibase:rollback -Dliquibase.rollbackCount=1\n</code></pre>"},{"location":"samples/java/liquibase/index.html#liquibase-considerations-with-aurora-dsql","title":"Liquibase considerations with Aurora DSQL","text":"<p>When using Liquibase with Aurora DSQL, be aware of the following considerations and limitations:</p>"},{"location":"samples/java/liquibase/index.html#transactions","title":"Transactions","text":"<ul> <li>Running multiple DDL statements in a single transaction will result in an error</li> <li>Use <code>runInTransaction:false</code> when running multiple DDL statements</li> <li>Or separate DDL statements into different changesets</li> </ul>"},{"location":"samples/java/liquibase/index.html#index-creation","title":"Index Creation","text":"<ul> <li>Use <code>CREATE INDEX ASYNC</code> instead of <code>CREATE INDEX</code> for DSQL compatibility</li> <li>Structured Liquibase index creation doesn't support the <code>ASYNC</code> keyword, so use raw SQL instead</li> </ul>"},{"location":"samples/java/liquibase/index.html#primary-keys","title":"Primary Keys","text":"<ul> <li>DSQL doesn't support <code>ALTER TABLE ADD CONSTRAINT</code> for primary keys</li> <li>Define primary keys inline during table creation rather than as separate changesets</li> </ul>"},{"location":"samples/java/liquibase/index.html#changelog-formats","title":"Changelog formats","text":"<p>The example includes two changelog formats:</p>"},{"location":"samples/java/liquibase/index.html#sql-format-changelogsql","title":"SQL Format (<code>changelog.sql</code>)","text":"<ul> <li>Uses Liquibase's SQL changelog format</li> <li>Includes DDL and DML statements</li> <li>Demonstrates proper rollback SQL</li> </ul>"},{"location":"samples/java/liquibase/index.html#json-format-changelogjson","title":"JSON Format (<code>changelog.json</code>)","text":"<ul> <li>Uses structured Liquibase JSON format</li> <li>Provides type safety and validation</li> </ul>"},{"location":"samples/java/liquibase/index.html#additional-resources","title":"Additional resources","text":"<ul> <li>Amazon Aurora DSQL Documentation</li> <li>Liquibase Documentation</li> <li>Aurora DSQL Connector for JDBC</li> </ul> <p>Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.</p> <p>SPDX-License-Identifier: MIT-0</p>"},{"location":"samples/java/pgjdbc/index.html","title":"Aurora DSQL with pgJDBC","text":""},{"location":"samples/java/pgjdbc/index.html#overview","title":"Overview","text":"<p>This code example demonstrates how to use <code>pgJDBC</code> with Amazon Aurora DSQL. The example shows you how to connect to an Aurora DSQL cluster and perform basic database operations.</p> <p>Aurora DSQL is a distributed SQL database service that provides high availability and scalability for your PostgreSQL-compatible applications. <code>pgJDBC</code> is a popular PostgreSQL adapter for Java that allows you to interact with PostgreSQL databases using Java code.</p> <p>This example uses the Aurora DSQL JDBC Connector to handle IAM authentication automatically.</p>"},{"location":"samples/java/pgjdbc/index.html#about-the-code-example","title":"About the code example","text":"<p>The example demonstrates a flexible connection approach that works for both admin and non-admin users:</p> <ul> <li>When connecting as an admin user, the example uses the <code>public</code> schema.</li> <li>When connecting as a non-admin user, the example uses a custom <code>myschema</code> schema.</li> </ul> <p>The code automatically detects the user type and adjusts its behavior accordingly.</p>"},{"location":"samples/java/pgjdbc/index.html#important","title":"\u26a0\ufe0f Important","text":"<ul> <li>Running this code might result in charges to your AWS account.</li> <li>We recommend that you grant your code least privilege. At most, grant only the   minimum permissions required to perform the task. For more information, see   Grant least privilege.</li> <li>This code is not tested in every AWS Region. For more information, see   AWS Regional Services.</li> </ul>"},{"location":"samples/java/pgjdbc/index.html#tls-connection-configuration","title":"TLS connection configuration","text":"<p>This example uses direct TLS connections where supported, and verifies the server certificate is trusted. Verified SSL connections should be used where possible to ensure data security during transmission.</p> <ul> <li>Driver versions following the release of PostgreSQL 17 support direct TLS connections, bypassing the traditional   PostgreSQL connection preamble</li> <li>Direct TLS connections provide improved connection performance and enhanced security</li> <li>Not all PostgreSQL drivers support direct TLS connections yet, or only in recent versions following PostgreSQL 17</li> <li>Ensure your installed driver version supports direct TLS negotiation, or use a version that is at least as recent as   the one used in this sample</li> <li>If your driver doesn't support direct TLS connections, you may need to use the traditional preamble connection instead</li> </ul>"},{"location":"samples/java/pgjdbc/index.html#run-the-example","title":"Run the example","text":""},{"location":"samples/java/pgjdbc/index.html#prerequisites","title":"Prerequisites","text":"<ul> <li>You must have an AWS account, and have your default credentials and AWS Region   configured as described in the   Globally configuring AWS SDKs and tools   guide.</li> <li>Java Development Kit (JDK): Ensure you have JDK 17+ installed.</li> </ul> <p>To verify the java is installed, you can run    ```bash    java -version</p> <ul> <li>Gradle: This example uses the Gradle wrapper included in the repository, so no separate installation is required.</li> <li>AWS SDK: Ensure that you setup the latest version of the AWS Java SDK official website</li> <li>You must have an Aurora DSQL cluster. For information about creating an Aurora DSQL cluster, see the   Getting started with Aurora DSQL   guide.</li> <li>If connecting as a non-admin user, ensure the user is linked to an IAM role and is granted access to the <code>myschema</code>   schema. See the   Using database roles with IAM roles   guide.</li> </ul>"},{"location":"samples/java/pgjdbc/index.html#run-the-code","title":"Run the code","text":"<p>The example demonstrates the following operations:</p> <ul> <li>Opening a connection to an Aurora DSQL cluster</li> <li>Creating a table</li> <li>Inserting and querying data</li> </ul> <p>The example is designed to work with both admin and non-admin users:</p> <ul> <li>When run as an admin user, it uses the <code>public</code> schema</li> <li>When run as a non-admin user, it uses the <code>myschema</code> schema</li> </ul> <p>Note: running the example will use actual resources in your AWS account and may incur charges.</p> <p>Set environment variables for your cluster details:</p> <pre><code># e.g. \"admin\"\nexport CLUSTER_USER=\"&lt;your user&gt;\"\n\n# e.g. \"foo0bar1baz2quux3quuux4.dsql.us-east-1.on.aws\"\nexport CLUSTER_ENDPOINT=\"&lt;your endpoint&gt;\"\n</code></pre> <p>Run the example:</p> <pre><code>./gradlew run\n</code></pre> <p>Run the tests:</p> <pre><code>./gradlew test\n</code></pre> <p>The example contains comments explaining the code and the operations being performed.</p>"},{"location":"samples/java/pgjdbc/index.html#additional-resources","title":"Additional resources","text":"<ul> <li>Amazon Aurora DSQL Documentation</li> <li>Amazon Aurora DSQL JDBC Connector</li> <li>pgJDBC Documentation</li> <li>AWS SDK for Java Documentation</li> </ul> <p>Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.</p> <p>SPDX-License-Identifier: Apache-2.0</p>"},{"location":"samples/java/pgjdbc/src/main/java/software/amazon/dsql/examples/alternatives/index.html","title":"Alternative Examples","text":"<p>The recommended approach is <code>ExamplePreferred.java</code> in the parent directory, which uses HikariCP connection pool with the Aurora DSQL JDBC Connector.</p>"},{"location":"samples/java/pgjdbc/src/main/java/software/amazon/dsql/examples/alternatives/index.html#why-connection-pooling-with-the-connector","title":"Why Connection Pooling with the Connector?","text":"<p>Aurora DSQL has specific connection characteristics: - 60-minute max connection lifetime - connections are terminated after 1 hour - 15-minute token expiry - IAM auth tokens must be refreshed - Optimized for concurrency - more concurrent connections with smaller batches yields better throughput</p> <p>The connector + pool combination handles this automatically: - Generates fresh IAM tokens per connection - Recycles connections before the 60-minute limit (via <code>maxLifetime &lt; 3600000</code>) - Reuses warmed connections for better performance</p>"},{"location":"samples/java/pgjdbc/src/main/java/software/amazon/dsql/examples/alternatives/index.html#alternatives","title":"Alternatives","text":""},{"location":"samples/java/pgjdbc/src/main/java/software/amazon/dsql/examples/alternatives/index.html#no_connection_pool","title":"<code>no_connection_pool/</code>","text":"<p>Examples without pooling: - <code>ExampleWithNoConnectionPool.java</code> - Single connection with connector - <code>ExampleWithNoConnector.java</code> - Single connection without connector (uses AWS SDK directly)</p>"},{"location":"samples/java/spring_boot/index.html","title":"Aurora DSQL with Spring Boot","text":""},{"location":"samples/java/spring_boot/index.html#overview","title":"Overview","text":"<p>This example demonstrates how to connect to Aurora DSQL using Spring Boot with HikariCP connection pooling. The example shows you how to connect to an Aurora DSQL cluster and perform basic database operations.</p> <p>Aurora DSQL is a distributed SQL database service that provides high availability and scalability for your PostgreSQL-compatible applications.</p> <p>This example uses the Aurora DSQL JDBC Connector to handle IAM authentication automatically.</p>"},{"location":"samples/java/spring_boot/index.html#about-the-code-example","title":"About the code example","text":"<p>The example demonstrates a flexible connection approach that works for both admin and non-admin users:</p> <ul> <li>When connecting as an admin user, the example uses the <code>public</code> schema.</li> <li>When connecting as a non-admin user, the example uses a custom <code>myschema</code> schema.</li> </ul> <p>The code automatically detects the user type and adjusts its behavior accordingly.</p>"},{"location":"samples/java/spring_boot/index.html#important","title":"\u26a0\ufe0f Important","text":"<ul> <li>Running this code might result in charges to your AWS account.</li> <li>We recommend that you grant your code least privilege. At most, grant only the   minimum permissions required to perform the task. For more information, see   Grant least privilege.</li> <li>This code is not tested in every AWS Region. For more information, see   AWS Regional Services.</li> </ul>"},{"location":"samples/java/spring_boot/index.html#tls-connection-configuration","title":"TLS connection configuration","text":"<p>This example uses direct TLS connections where supported, and verifies the server certificate is trusted. Verified SSL connections should be used where possible to ensure data security during transmission.</p> <ul> <li>Driver versions following the release of PostgreSQL 17 support direct TLS connections, bypassing the traditional   PostgreSQL connection preamble</li> <li>Direct TLS connections provide improved connection performance and enhanced security</li> <li>Not all PostgreSQL drivers support direct TLS connections yet, or only in recent versions following PostgreSQL 17</li> <li>Ensure your installed driver version supports direct TLS negotiation, or use a version that is at least as recent as   the one used in this sample</li> <li>If your driver doesn't support direct TLS connections, you may need to use the traditional preamble connection instead</li> </ul>"},{"location":"samples/java/spring_boot/index.html#run-the-example","title":"Run the example","text":""},{"location":"samples/java/spring_boot/index.html#prerequisites","title":"Prerequisites","text":"<ul> <li>You must have an AWS account, and have your default credentials and AWS Region   configured as described in the   Globally configuring AWS SDKs and tools   guide.</li> <li>Java Development Kit (JDK): Ensure you have JDK 17+ installed.</li> </ul> <p>To verify Java is installed, run:</p> <pre><code>java -version\n</code></pre> <ul> <li>Build Tool (Maven or Gradle)<ul> <li>Maven: Ensure Maven is installed if that is your preferred option. You can download it from   the official website.</li> <li>Gradle: A Gradle wrapper is included with the example. If you prefer to use a system installation of Gradle, you   can download it from the official website.</li> </ul> </li> <li>AWS SDK: Ensure that you set up the latest version of the AWS Java   SDK from the official website.</li> <li>You must have an Aurora DSQL cluster. For information about creating an Aurora DSQL cluster, see the   Getting started with Aurora DSQL   guide.</li> <li>If connecting as a non-admin user, ensure the user is linked to an IAM role and is granted access to the <code>myschema</code>   schema. See the   Using database roles with IAM roles   guide.</li> </ul>"},{"location":"samples/java/spring_boot/index.html#configuration","title":"Configuration","text":"<p>Set the following environment variables:</p> <pre><code># e.g. \"admin\"\nexport CLUSTER_USER=\"&lt;your user&gt;\"\n\n# e.g. \"foo0bar1baz2quux3quuux4.dsql.us-east-1.on.aws\"\nexport CLUSTER_ENDPOINT=\"&lt;your endpoint&gt;\"\n\n# Optional: Only necessary if you want the application to exit after running the\n# example code instead of continuing to serve the HTTP API.\nexport EXIT_AFTER_TEST=\"true\"\n</code></pre>"},{"location":"samples/java/spring_boot/index.html#run-the-code","title":"Run the code","text":"<p>The example demonstrates the following operations:</p> <ul> <li>Opening a connection to an Aurora DSQL cluster</li> <li>Creating a table</li> <li>Inserting and querying data</li> </ul> <p>The example is designed to work with both admin and non-admin users:</p> <ul> <li>When run as an admin user, it uses the <code>public</code> schema</li> <li>When run as a non-admin user, it uses the <code>myschema</code> schema</li> </ul> <p>Note: running the example will use actual resources in your AWS account and may incur charges.</p> <p>Using Maven:</p> <pre><code>mvn spring-boot:run\n</code></pre> <p>Using Gradle:</p> <pre><code>./gradlew bootRun\n</code></pre>"},{"location":"samples/java/spring_boot/index.html#additional-resources","title":"Additional resources","text":"<ul> <li>Amazon Aurora DSQL Documentation</li> <li>Amazon Aurora DSQL JDBC Connector</li> <li>Spring Boot Documentation</li> <li>Spring Data JDBC</li> <li>HikariCP</li> <li>AWS SDK for Java Documentation</li> </ul> <p>Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.</p> <p>SPDX-License-Identifier: MIT-0</p>"},{"location":"samples/javascript/cluster_management/index.html","title":"Aurora DSQL JavaScript code examples","text":""},{"location":"samples/javascript/cluster_management/index.html#overview","title":"Overview","text":"<p>The code examples in this topic show you how to use the AWS JavaScript SDK v3 with DSQL to create, update, get, and delete single- and multi-Region clusters.</p> <p>Each file in the /src directory demonstrates a minimal working example for each operation.</p>"},{"location":"samples/javascript/cluster_management/index.html#run-the-examples","title":"Run the examples","text":""},{"location":"samples/javascript/cluster_management/index.html#important","title":"\u26a0\ufe0f Important","text":"<ul> <li>Running this code might result in charges to your AWS account.</li> <li>We recommend that you grant your code least privilege. At most, grant only the   minimum permissions required to perform the task. For more information, see   Grant least privilege.</li> <li>This code is not tested in every AWS Region. For more information, see   AWS Regional Services.</li> </ul>"},{"location":"samples/javascript/cluster_management/index.html#prerequisites","title":"Prerequisites","text":"<ul> <li>Node 18.0.0 or later.</li> <li>Valid AWS credentials can be discovered by the default provider chain.</li> </ul>"},{"location":"samples/javascript/cluster_management/index.html#configure-the-environment","title":"Configure the environment","text":"<p>Set environment variables with your cluster details as required.</p>"},{"location":"samples/javascript/cluster_management/index.html#single-region-clusters","title":"Single-region clusters","text":"<pre><code># Only relevant for delete/get/update examples.\n# e.g. \"foo0bar1baz2quux3quuux4\"\nexport CLUSTER_ID=\"&lt;your id&gt;\"\n\n# Relevant for all examples.\n# e.g. \"us-east-1\"\nexport CLUSTER_REGION=\"&lt;your region&gt;\"\n</code></pre>"},{"location":"samples/javascript/cluster_management/index.html#multi-region-clusters","title":"Multi-region clusters","text":"<pre><code># Only relevant for delete/get/update examples.\n# e.g. \"foo0bar1baz2quux3quuux4\" and \"foo5bar6baz7quux8quuux9\"\nexport CLUSTER_1_ID=\"&lt;your id 1&gt;\"\nexport CLUSTER_2_ID=\"&lt;your id 2&gt;\"\n\n# Relevant for all examples.\n# e.g. \"us-east-1\" and \"us-east-2\"\nexport CLUSTER_1_REGION=\"&lt;your region 1&gt;\"\nexport CLUSTER_2_REGION=\"&lt;your region 2&gt;\"\n\n# Only relevant for create examples.\n# e.g. \"us-west-2\"\nexport WITNESS_REGION=\"&lt;your region 3&gt;\"\n</code></pre>"},{"location":"samples/javascript/cluster_management/index.html#execute-tests-to-create-and-delete-clusters","title":"Execute tests to create and delete clusters","text":"<pre><code>npm install\n\nnpm test\n</code></pre>"},{"location":"samples/javascript/cluster_management/index.html#executing-single-operations","title":"Executing single operations","text":"<p>Files in the /src directory have a main() method that lets you exercise single operations.</p> <pre><code># Check each operation for its expected environment variables\nCLUSTER_REGION=\"us-east-1\" CLUSTER_ID=\"&lt;your cluster id&gt;\" \\\nnode ./src/get_cluster.js\n</code></pre> <p>Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.</p> <p>SPDX-License-Identifier: MIT-0</p>"},{"location":"samples/javascript/node-postgres/index.html","title":"Aurora DSQL with node-postgres","text":""},{"location":"samples/javascript/node-postgres/index.html#overview","title":"Overview","text":"<p>This code example demonstrates how to use <code>node-postgres</code> with Amazon Aurora DSQL. The example shows you how to connect to an Aurora DSQL cluster and perform basic database operations.</p> <p>Aurora DSQL is a distributed SQL database service that provides high availability and scalability for your PostgreSQL-compatible applications. <code>node-postgres</code> is a popular PostgreSQL adapter for Node.js that allows you to interact with PostgreSQL databases using JavaScript code.</p>"},{"location":"samples/javascript/node-postgres/index.html#about-the-code-example","title":"About the code example","text":"<p>This example uses the Aurora DSQL Node.js Connector which automatically handles IAM token generation for authentication.</p> <p>The example demonstrates a flexible connection approach that works for both admin and non-admin users:</p> <ul> <li>When connecting as an admin user, the example uses the <code>public</code> schema and generates an admin authentication token.</li> <li>When connecting as a non-admin user, the example uses a custom <code>myschema</code> schema and generates a standard authentication token.</li> </ul> <p>The code automatically detects the user type and adjusts its behavior accordingly.</p>"},{"location":"samples/javascript/node-postgres/index.html#important","title":"\u26a0\ufe0f Important","text":"<ul> <li>Running this code might result in charges to your AWS account.</li> <li>We recommend that you grant your code least privilege. At most, grant only the   minimum permissions required to perform the task. For more information, see   Grant least privilege.</li> <li>This code is not tested in every AWS Region. For more information, see   AWS Regional Services.</li> </ul>"},{"location":"samples/javascript/node-postgres/index.html#run-the-example","title":"Run the example","text":""},{"location":"samples/javascript/node-postgres/index.html#prerequisites","title":"Prerequisites","text":"<ul> <li>You must have an AWS account, and have your default credentials and AWS Region   configured as described in the   Globally configuring AWS SDKs and tools   guide.</li> <li>Node.js: Ensure you have Node.js 18+ installed.</li> </ul> <pre><code>node --version\n</code></pre> <p>It should output something similar to <code>v18.x</code> or higher.</p> <ul> <li>You must have an Aurora DSQL cluster. For information about creating an Aurora DSQL cluster, see the   Getting started with Aurora DSQL   guide.</li> <li>If connecting as a non-admin user, ensure the user is linked to an IAM role and is granted access to the <code>myschema</code>   schema. See the   Using database roles with IAM roles   guide.</li> </ul>"},{"location":"samples/javascript/node-postgres/index.html#run-the-code","title":"Run the code","text":"<p>The example demonstrates the following operations:</p> <ul> <li>Opening a connection to an Aurora DSQL cluster</li> <li>Creating a table</li> <li>Inserting and querying data</li> </ul> <p>The example is designed to work with both admin and non-admin users:</p> <ul> <li>When run as an admin user, it uses the <code>public</code> schema</li> <li>When run as a non-admin user, it uses the <code>myschema</code> schema</li> </ul> <p>Note: running the example will use actual resources in your AWS account and may incur charges.</p> <p>Set environment variables for your cluster details:</p> <pre><code># e.g. \"admin\"\nexport CLUSTER_USER=\"&lt;your user&gt;\"\n\n# e.g. \"foo0bar1baz2quux3quuux4.dsql.us-east-1.on.aws\"\nexport CLUSTER_ENDPOINT=\"&lt;your endpoint&gt;\"\n</code></pre> <p>Run the example:</p> <pre><code>npm install\nnpm test\n</code></pre> <p>The example contains comments explaining the code and the operations being performed.</p>"},{"location":"samples/javascript/node-postgres/index.html#additional-resources","title":"Additional resources","text":"<ul> <li>Amazon Aurora DSQL Documentation</li> <li>Aurora DSQL Node.js Connector</li> <li>node-postgres Documentation</li> </ul> <p>Note: The connector automatically extracts the region from the cluster endpoint and defaults to the <code>postgres</code> database.</p> <p>Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.</p> <p>SPDX-License-Identifier: MIT-0</p>"},{"location":"samples/javascript/node-postgres/src/alternatives/index.html","title":"Alternative Examples","text":"<p>The recommended approach is <code>example_preferred.js</code> in the parent directory, which uses a concurrent connection pool with the Aurora DSQL connector.</p>"},{"location":"samples/javascript/node-postgres/src/alternatives/index.html#why-connection-pooling-with-the-connector","title":"Why Connection Pooling with the Connector?","text":"<p>Aurora DSQL has specific connection characteristics: - 60-minute max connection lifetime - connections are terminated after 1 hour - 15-minute token expiry - IAM auth tokens must be refreshed - Optimized for concurrency - more concurrent connections with smaller batches yields better throughput</p> <p>The connector + pool combination handles this automatically: - Generates fresh IAM tokens per connection - Recycles connections before the 60-minute limit - Reuses warmed connections (2-24ms faster than new connections)</p>"},{"location":"samples/javascript/node-postgres/src/alternatives/index.html#alternatives","title":"Alternatives","text":""},{"location":"samples/javascript/node-postgres/src/alternatives/index.html#pool","title":"<code>pool/</code>","text":"<p>Other pool configurations using the connector: - <code>example_with_nonconcurrent_connection_pool.js</code> - Simple pool without concurrency</p>"},{"location":"samples/javascript/node-postgres/src/alternatives/index.html#no_connection_pool","title":"<code>no_connection_pool/</code>","text":"<p>Examples without pooling: - <code>example_with_no_connection_pool.js</code> - Single connection with connector - <code>example_with_no_connector.js</code> - SDK-only, for environments where the connector cannot be used (requires manual token management)</p>"},{"location":"samples/javascript/postgres-js/index.html","title":"Aurora DSQL with Postgres.js","text":""},{"location":"samples/javascript/postgres-js/index.html#overview","title":"Overview","text":"<p>This code example demonstrates how to use <code>Postgres.js</code> with Amazon Aurora DSQL. The example shows you how to connect to an Aurora DSQL cluster and perform basic database operations.</p> <p>Aurora DSQL is a distributed SQL database service that provides high availability and scalability for your PostgreSQL-compatible applications. <code>Postgres.js</code> is a lightweight PostgreSQL client for Node.js that allows you to interact with PostgreSQL databases using JavaScript code.</p>"},{"location":"samples/javascript/postgres-js/index.html#about-the-code-example","title":"About the code example","text":"<p>This example uses the Aurora DSQL Node.js Connector which automatically handles IAM token generation for authentication.</p> <p>The example demonstrates a flexible connection approach that works for both admin and non-admin users:</p> <ul> <li>When connecting as an admin user, the example uses the <code>public</code> schema and generates an admin authentication token.</li> <li>When connecting as a non-admin user, the example uses a custom <code>myschema</code> schema and generates a standard authentication token.</li> </ul> <p>The code automatically detects the user type and adjusts its behavior accordingly.</p>"},{"location":"samples/javascript/postgres-js/index.html#important","title":"\u26a0\ufe0f Important","text":"<ul> <li>Running this code might result in charges to your AWS account.</li> <li>We recommend that you grant your code least privilege. At most, grant only the   minimum permissions required to perform the task. For more information, see   Grant least privilege.</li> <li>This code is not tested in every AWS Region. For more information, see   AWS Regional Services.</li> </ul>"},{"location":"samples/javascript/postgres-js/index.html#run-the-example","title":"Run the example","text":""},{"location":"samples/javascript/postgres-js/index.html#prerequisites","title":"Prerequisites","text":"<ul> <li>You must have an AWS account, and have your default credentials and AWS Region   configured as described in the   Globally configuring AWS SDKs and tools   guide.</li> <li>Node.js: Ensure you have Node.js 18+ installed.</li> </ul> <pre><code>node --version\n</code></pre> <p>It should output something similar to <code>v18.x</code> or higher.</p> <ul> <li>You must have an Aurora DSQL cluster. For information about creating an Aurora DSQL cluster, see the   Getting started with Aurora DSQL   guide.</li> <li>If connecting as a non-admin user, ensure the user is linked to an IAM role and is granted access to the <code>myschema</code>   schema. See the   Using database roles with IAM roles   guide.</li> </ul>"},{"location":"samples/javascript/postgres-js/index.html#run-the-code","title":"Run the code","text":"<p>The example demonstrates the following operations:</p> <ul> <li>Opening a connection to an Aurora DSQL cluster</li> <li>Creating a table</li> <li>Inserting and querying data</li> </ul> <p>The example is designed to work with both admin and non-admin users:</p> <ul> <li>When run as an admin user, it uses the <code>public</code> schema</li> <li>When run as a non-admin user, it uses the <code>myschema</code> schema</li> </ul> <p>Note: running the example will use actual resources in your AWS account and may incur charges.</p> <p>Set environment variables for your cluster details:</p> <pre><code># e.g. \"admin\"\nexport CLUSTER_USER=\"&lt;your user&gt;\"\n\n# e.g. \"foo0bar1baz2quux3quuux4.dsql.us-east-1.on.aws\"\nexport CLUSTER_ENDPOINT=\"&lt;your endpoint&gt;\"\n</code></pre> <p>Run the example:</p> <pre><code>npm install\nnpm test\n</code></pre> <p>The example contains comments explaining the code and the operations being performed.</p>"},{"location":"samples/javascript/postgres-js/index.html#connection-pooling","title":"Connection pooling","text":"<p>Postgres.js uses connection pooling by default. The maximum pool size, and maximum lifespan of connections is configurable  when the client is created using the options <code>max</code> and <code>max_lifetime</code> respectively. Note that connections are created lazily only when a database call occurs, not when the client is created. See Postgres.js documentation here for more information. There are no guarantees as to which connection will be used when executing a command, except within a single transaction. This means users cannot rely on commands like <code>SET SESSION search_path=schema</code> to be applied correctly across multiple database interactions.</p>"},{"location":"samples/javascript/postgres-js/index.html#additional-resources","title":"Additional resources","text":"<ul> <li>Amazon Aurora DSQL Documentation</li> <li>Aurora DSQL Node.js Connector</li> <li>Postgres.js Documentation</li> </ul> <p>Note: The connector automatically extracts the region from the cluster endpoint and defaults to the <code>postgres</code> database.</p> <p>Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.</p> <p>SPDX-License-Identifier: MIT-0</p>"},{"location":"samples/javascript/postgres-js/src/alternatives/index.html","title":"Alternative Examples","text":"<p>The recommended approach is <code>example_preferred.js</code> in the parent directory, which uses a concurrent connection pool with the Aurora DSQL connector.</p>"},{"location":"samples/javascript/postgres-js/src/alternatives/index.html#why-connection-pooling-with-the-connector","title":"Why Connection Pooling with the Connector?","text":"<p>Aurora DSQL has specific connection characteristics: - 60-minute max connection lifetime - connections are terminated after 1 hour - 15-minute token expiry - IAM auth tokens must be refreshed - Optimized for concurrency - more concurrent connections with smaller batches yields better throughput</p> <p>The connector + pool combination handles this automatically: - Generates fresh IAM tokens per connection - Recycles connections before the 60-minute limit - Reuses warmed connections (2-24ms faster than new connections)</p>"},{"location":"samples/javascript/postgres-js/src/alternatives/index.html#alternatives","title":"Alternatives","text":""},{"location":"samples/javascript/postgres-js/src/alternatives/index.html#no_connection_pool","title":"<code>no_connection_pool/</code>","text":"<p>Examples without pooling: - <code>example_with_no_connection_pool.js</code> - Single connection with connector - <code>example_with_no_connector.js</code> - SDK-only, for environments where the connector cannot be used (requires manual token management)</p>"},{"location":"samples/lambda/index.html","title":"Using AWS Lambda with Amazon Aurora DSQL","text":"<p>This document describes how to use AWS Lambda to access Aurora DSQL. In this sample, a CDK application is created with a DSQL cluster, and a Lambda function that accesses the cluster to create a table, read and write values to that table, and then finally delete the table. The sample code is using Node.js, but it could be created similarly in other languages as well.</p>"},{"location":"samples/lambda/index.html#pre-requisites","title":"Pre-requisites","text":"<ul> <li>Permissions to create DSQL clusters, AWS Lambdas, and IAM policies</li> <li>You must have installed npm <code>v8.5.3</code> or higher.</li> <li>You must have installed aws-cdk toolkit <code>v2.1018.0</code> or higher.</li> <li>You must have AWS credentials configured in your local environment.</li> </ul>"},{"location":"samples/lambda/index.html#important","title":"\u26a0\ufe0f Important","text":"<ul> <li>Running this code might result in charges to your AWS account.</li> <li>We recommend that you grant your code least privilege. At most, grant only the   minimum permissions required to perform the task. For more information, see   Grant least privilege.</li> <li>This code is not tested in every AWS Region. For more information, see   AWS Regional Services.</li> <li>This sample uses an admin role to minimize prerequisite steps to get started. It is not a good practice to use an admin database role for your production applications. See Using database roles with IAM roles to learn how to create custom database roles with authorization that has the fewest permissions to your database.</li> </ul>"},{"location":"samples/lambda/index.html#how-to-use-aurora-dsql-with-aws-lambda","title":"How-to use Aurora DSQL with AWS Lambda","text":""},{"location":"samples/lambda/index.html#create-a-new-dsql-cluster-in-cdk","title":"Create a new DSQL cluster in CDK","text":"<p>A DSQL cluster can be added to a deployment stack using the CloudFormation DSQL resource. It is created with the same parameters as a cluster would be created using the CLI or an SDK.</p> <pre><code>const dsqlCluster = new aws_dsql.CfnCluster(this, 'DsqlCluster', {\n  deletionProtectionEnabled: false,\n  tags: [{\n    key: 'Name', value: 'Lambda sample single region cluster',\n  }, {\n    key: 'Repo', value: 'aws-samples/aurora-dsql-samples',\n  }],\n});\n</code></pre>"},{"location":"samples/lambda/index.html#create-a-lambda-function-in-cdk-that-accesses-the-dsql-cluster","title":"Create a Lambda function in CDK that accesses the DSQL cluster","text":"<p>The following shows a CDK configuration for a basic Lambda function that can interact with DSQL. It provides the DSQL endpoint directly from the cluster creation in the previous step.</p> <pre><code>const dsqlFunction = new Function(this, 'DsqlSample', {\n  runtime: Runtime.NODEJS_22_X,\n  handler: 'lambda.handler',\n  code: Code.fromAsset('sample'),\n  timeout: Duration.seconds(30),\n  memorySize: 256,\n  environment: {\n    CLUSTER_ENDPOINT: `${dsqlCluster.attrIdentifier}.dsql.${region}.on.aws`,\n    CLUSTER_REGION: region\n  }\n});\n</code></pre>"},{"location":"samples/lambda/index.html#authorize-your-lambda-execution-role-to-connect-to-your-cluster","title":"Authorize your Lambda execution role to connect to your cluster","text":"<p>The Lambda function created in the previous step will need permissions to access DSQL, otherwise it will fail to connect. The following shows a basic way to add DSQL permissions:</p> <pre><code>dsqlFunction.addToRolePolicy(new PolicyStatement({\n  effect: Effect.ALLOW,\n  actions: ['dsql:DbConnectAdmin', 'dsql:DbConnect'],\n  resources: [dsqlCluster.attrResourceArn]\n}));\n</code></pre>"},{"location":"samples/lambda/index.html#create-the-code-package-that-the-lambda-function-can-run","title":"Create the code package that the Lambda function can run","text":"<p>The Lambda function specifies the <code>code: Code.from('sample')</code> property indicating where the Lambda's code is located. The path specified, in this case <code>sample</code>, must contain all the code needed to run, including dependencies in node_modules. Almost any Node.js code could be run here, but the key block is the Lambda handler, which is the invocation point for Lambda executions:</p> <pre><code>// https://docs.aws.amazon.com/lambda/latest/dg/nodejs-handler.html\nexport const handler = async (event) =&gt; {\n    const endpoint = process.env.CLUSTER_ENDPOINT\n    const region = process.env.CLUSTER_REGION\n    const responseCode = await dsql_sample(endpoint, region);\n\n    const response = {\n        statusCode: responseCode,\n        endpoint: endpoint,\n    };\n    return response;\n};\n</code></pre> <p>From the handler you can write any database operations. This example uses Node-postgres to connect to DSQL using a connection pool, create a table, insert data, and read back that data. It also includes timers to show that the initial connection to DSQL from the Lambda will be the slowest, but future usages will be considerably faster. It uses a connection pool that can be shared between Lambda invocations by preserving the pool in memory, which greatly reduces connection time.</p> <pre><code>import {DsqlSigner} from \"@aws-sdk/dsql-signer\";\nimport {Pool} from \"pg\";\n\nlet pool;\n\nasync function dsql_sample(clusterEndpoint, region) {\n  await getOrCreatePool(clusterEndpoint, region)\n  await createTable(pool);\n  for (let i = 0; i &lt; 10; i++) {\n    await insertAndReadData(pool);\n  }\n  await dropTable(pool)\n}\n\nasync function getOrCreatePool(endpoint, region) {\n  if (pool === undefined) {\n    console.log(\"Creating connection pool.\")\n    const signer = new DsqlSigner({\n      hostname: endpoint,\n      region,\n    });\n    // &lt;https://node-postgres.com/apis/client&gt;\n    // By default `rejectUnauthorized` is true in TLS options\n    // &lt;https://nodejs.org/api/tls.html#tls_tls_connect_options_callback&gt;\n    // The config does not offer any specific parameter to set sslmode to verify-full\n    // Settings are controlled either via connection string or by setting\n    // rejectUnauthorized to false in ssl options\n    pool = new Pool({\n      host: endpoint,\n      port: 5432,\n      database: \"postgres\",\n      user: \"admin\",\n      password: async function () {\n        return await signer.getDbConnectAdminAuthToken()\n      },\n      ssl: true,\n      max: 20,\n      idleTimeoutMillis: 30000,\n      connectionTimeoutMillis: 5000,\n    });\n  }\n}\n\nasync function createTable(pool) {\n  let client;\n  try {\n    client = await getClientFromPool(pool);\n    // Create a new table\n    let start = Date.now();\n    await client.query(`CREATE TABLE IF NOT EXISTS sample (\n              id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n              val INTEGER NOT NULL\n            )`);\n    console.log(`Created table Sample - Time elapsed: ${Date.now() - start} ms`);\n  } finally {\n    if (client !== undefined) client.release();\n  }\n}\n\nasync function dropTable(pool) {\n  let client;\n  try {\n    client = await getClientFromPool(pool);\n    let start = Date.now();\n    await client.query(`DROP TABLE IF EXISTS sample`);\n    console.log(`Dropped table Sample - Time elapsed: ${Date.now() - start} ms`);\n  } finally {\n    if (client !== undefined) client.release();\n  }\n}\n\nasync function insertAndReadData(pool) {\n  let client;\n  try {\n    let val = Math.floor(Math.random() * 1000000);\n    client = await getClientFromPool(pool);\n    let start = Date.now();\n    await client.query(\"INSERT INTO sample(val) VALUES($1)\", [val]);\n    console.log(`Inserted data to Sample - Time elapsed: ${Date.now() - start} ms`);\n\n    // Check that data is inserted by reading it back\n    start = Date.now();\n    const result = await client.query(\"SELECT id, val FROM sample WHERE val = $1\", [val]);\n    console.log(`Retrieved row: ID=${result.rows[0].id}, Val=${result.rows[0].val}`);\n    console.log(`Retrieved row from Sample - Time elapsed: ${Date.now() - start} ms`);\n  } finally {\n    if (client !== undefined) client.release();\n  }\n}\n\nasync function getClientFromPool(pool) {\n  let start = Date.now();\n  const client = await pool.connect();\n  console.log(`Retrieved DSQL connection - Time elapsed: ${Date.now() - start} ms`);\n  return client;\n}\n\n// https://docs.aws.amazon.com/lambda/latest/dg/nodejs-handler.html\nexport const handler = async (event) =&gt; {\n  const endpoint = process.env.CLUSTER_ENDPOINT\n  const region = process.env.CLUSTER_REGION\n  const responseCode = await dsql_sample(endpoint, region);\n\n  const response = {\n    statusCode: responseCode,\n    endpoint: endpoint,\n  };\n  return response;\n};\n</code></pre>"},{"location":"samples/lambda/index.html#running-the-example","title":"Running the example","text":"<p>From the root directory of the Lambda example (<code>aurora-dsql-samples/lambda</code>) the CDK can be deployed as follows:</p> <ol> <li> <p>Install dependencies for the project: <pre><code>npm install &amp;&amp; npm --prefix ./sample install\n</code></pre></p> </li> <li> <p>Bootstrap the CDK deployment: <pre><code>cdk bootstrap\n</code></pre></p> </li> <li>Provide environment variables for deployment: <pre><code>export REGION=us-east-1 # Or any other DSQL supported region\n</code></pre></li> <li>Deploy the CDK configuration: <pre><code>cdk deploy\n</code></pre></li> </ol> <p>From the AWS console you should now be able to visit the Lambda console and see your function. If you test this function you should hopefully see the following as a result: <pre><code>{statusCode\": 200, \"endpoint\": \"your_cluster_endpoint\"}\n</code></pre></p> <p>If the database returns an error or if the connection to the database fails, the Lambda function execution response returns the error that occurred.</p>"},{"location":"samples/lambda/index.html#additional-resources","title":"Additional resources","text":"<ul> <li>Amazon Aurora DSQL Documentation</li> <li>AWS CDK Documentation</li> <li>AWS Lambda Documentation</li> <li>node-postgres Documentation</li> </ul> <p>Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.</p> <p>SPDX-License-Identifier: MIT-0</p>"},{"location":"samples/python/asyncpg/index.html","title":"Aurora DSQL with asyncpg","text":"<p>This example demonstrates how to use the Aurora DSQL Python Connector with asyncpg to connect to Amazon Aurora DSQL clusters and perform basic database operations.</p> <p>Aurora DSQL is a distributed SQL database service that provides high availability and scalability for your PostgreSQL-compatible applications. Asyncpg is a popular PostgreSQL database library for Python that allows you to interact with PostgreSQL databases using Python code.</p>"},{"location":"samples/python/asyncpg/index.html#about-the-code-example","title":"About the code example","text":"<p>The example demonstrates a flexible connection approach that works for both admin and non-admin users:</p> <ul> <li>When connecting as an admin user, the example uses the <code>public</code> schema and generates an admin authentication   token.</li> <li>When connecting as a non-admin user, the example uses a custom <code>myschema</code> schema and generates a standard   authentication token.</li> </ul> <p>The code automatically detects the user type and adjusts its behavior accordingly.</p>"},{"location":"samples/python/asyncpg/index.html#important","title":"\u26a0\ufe0f Important","text":"<ul> <li>Running this code might result in charges to your AWS account.</li> <li>We recommend that you grant your code least privilege. At most, grant only the   minimum permissions required to perform the task. For more information, see   Grant least privilege.</li> <li>This code is not tested in every AWS Region. For more information, see   AWS Regional Services.</li> </ul>"},{"location":"samples/python/asyncpg/index.html#tls-connection-configuration","title":"TLS connection configuration","text":"<p>This example uses direct TLS connections where supported, and verifies the server certificate is trusted. Verified SSL connections should be used where possible to ensure data security during transmission.</p> <ul> <li>Driver versions following the release of PostgreSQL 17 support direct TLS connections, bypassing the traditional   PostgreSQL connection preamble</li> <li>Direct TLS connections provide improved connection performance and enhanced security</li> <li>Not all PostgreSQL drivers support direct TLS connections yet, or only in recent versions following PostgreSQL 17</li> <li>Ensure your installed driver version supports direct TLS negotiation, or use a version that is at least as recent as   the one used in this sample</li> <li>If your driver doesn't support direct TLS connections, you may need to use the traditional preamble connection instead</li> </ul>"},{"location":"samples/python/asyncpg/index.html#prerequisites","title":"Prerequisites","text":"<ul> <li>You must have an AWS account, and have your default credentials and AWS Region   configured as described in the   Globally configuring AWS SDKs and tools   guide.</li> <li>Python 3.10.0 or later.</li> <li>You must have an Aurora DSQL cluster. For information about creating an Aurora DSQL cluster, see the   Getting started with Aurora DSQL   guide.</li> <li>If connecting as a non-admin user, ensure the user is linked to an IAM role and is granted access to the <code>myschema</code>   schema. See the   Using database roles with IAM roles   guide.</li> </ul>"},{"location":"samples/python/asyncpg/index.html#download-the-amazon-root-certificate-from-the-official-trust-store","title":"Download the Amazon root certificate from the official trust store","text":"<p>Download the Amazon root certificate from the official trust store:</p> <pre><code>wget https://www.amazontrust.com/repository/AmazonRootCA1.pem -O root.pem\n</code></pre>"},{"location":"samples/python/asyncpg/index.html#set-up-environment-for-examples","title":"Set up environment for examples","text":"<ol> <li>Create and activate a Python virtual environment:</li> </ol> <pre><code>python3 -m venv .venv\nsource .venv/bin/activate  # Linux, macOS\n# or\n.venv\\Scripts\\activate     # Windows\n</code></pre> <ol> <li>Install the required packages for running the examples:</li> </ol> <pre><code>pip install -e .\n\n# Install optional dependencies for tests\npip install -e \".[test]\"\n</code></pre>"},{"location":"samples/python/asyncpg/index.html#run-the-code","title":"Run the code","text":"<p>The example demonstrates the following operations:</p> <ul> <li>Opening a connection to an Aurora DSQL cluster</li> <li>Creating a table</li> <li>Inserting and querying data</li> </ul> <p>The example is designed to work with both admin and non-admin users:</p> <ul> <li>When run as an admin user, it uses the <code>public</code> schema</li> <li>When run as a non-admin user, it uses the <code>myschema</code> schema</li> </ul> <p>Note: running the example will use actual resources in your AWS account and may incur charges.</p> <p>The connetion pool examples demonstrate: - Creating a connection pool for Aurora DSQL - Using async context managers for connection management - Performing database operations through the pool - Running multiple concurrent database operations - Using asyncio.gather() for parallel execution - Proper resource management with connection pools</p> <p>Set environment variables for your cluster details:</p> <pre><code># e.g. \"admin\"\nexport CLUSTER_USER=\"&lt;your user&gt;\"\n\n# e.g. \"foo0bar1baz2quux3quuux4.dsql.us-east-1.on.aws\"\nexport CLUSTER_ENDPOINT=\"&lt;your endpoint&gt;\"\n</code></pre> <p>Run the example:</p> <pre><code># Run example directly\npython src/example.py\n\n# Run example using pytest\npytest ./test/test_example.py\n\n# Run all using pytest\npytest ./test\n</code></pre> <p>The example contains comments explaining the code and the operations being performed.</p>"},{"location":"samples/python/asyncpg/index.html#connection-defaults","title":"Connection defaults","text":"<p>The connector automatically handles the following parameters:</p> Parameter Default Notes <code>database</code> <code>postgres</code> Aurora DSQL's default database <code>port</code> <code>5432</code> Standard PostgreSQL port <code>region</code> Extracted from endpoint Parsed from <code>*.dsql.&lt;region&gt;.on.aws</code> <p>You can override any of these defaults by explicitly passing them in your connection parameters.</p>"},{"location":"samples/python/asyncpg/index.html#additional-resources","title":"Additional resources","text":"<ul> <li>Amazon Aurora DSQL Documentation</li> <li>Asyncpg Documentation</li> <li>AWS SDK for Python (Boto3) Documentation</li> </ul> <p>Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.</p> <p>SPDX-License-Identifier: MIT-0</p>"},{"location":"samples/python/asyncpg/src/alternatives/index.html","title":"Alternative Examples","text":"<p>The recommended approach is <code>example_preferred.py</code> in the parent directory, which uses asyncpg connection pool with the Aurora DSQL Python Connector.</p>"},{"location":"samples/python/asyncpg/src/alternatives/index.html#why-connection-pooling-with-the-connector","title":"Why Connection Pooling with the Connector?","text":"<p>Aurora DSQL has specific connection characteristics: - 60-minute max connection lifetime - connections are terminated after 1 hour - 15-minute token expiry - IAM auth tokens must be refreshed - Optimized for concurrency - more concurrent connections with smaller batches yields better throughput</p> <p>The connector + pool combination handles this automatically: - Generates fresh IAM tokens per connection - Recycles connections before the 60-minute limit - Reuses warmed connections for better performance</p>"},{"location":"samples/python/asyncpg/src/alternatives/index.html#alternatives","title":"Alternatives","text":""},{"location":"samples/python/asyncpg/src/alternatives/index.html#pool","title":"<code>pool/</code>","text":"<p>Other pool configurations: - <code>example_with_async_connection_pool.py</code> - Async pool usage - <code>example_with_nonconcurrent_connection_pool.py</code> - Sequential pool usage</p>"},{"location":"samples/python/asyncpg/src/alternatives/index.html#no_connection_pool","title":"<code>no_connection_pool/</code>","text":"<p>Examples without pooling: - <code>example_with_no_connection_pool.py</code> - Single connection with connector - <code>example_async_with_no_connection_pool.py</code> - Async single connection with connector</p>"},{"location":"samples/python/cluster_management/index.html","title":"Aurora DSQL Python code examples","text":""},{"location":"samples/python/cluster_management/index.html#overview","title":"Overview","text":"<p>The code examples in this topic show you how to use the AWS Python SDK with DSQL to create, update, get, and delete single- and multi-Region clusters.</p> <p>Each file in the /src directory demonstrates a minimum working example for each operation. The example function for each operation is invoked in <code>test_dsql_cluster_management.py</code>.</p>"},{"location":"samples/python/cluster_management/index.html#run-the-examples","title":"Run the examples","text":""},{"location":"samples/python/cluster_management/index.html#important","title":"\u26a0\ufe0f Important","text":"<ul> <li>Running this code might result in charges to your AWS account.</li> <li>We recommend that you grant your code least privilege. At most, grant only the   minimum permissions required to perform the task. For more information, see   Grant least privilege.</li> <li>This code is not tested in every AWS Region. For more information, see   AWS Regional Services.</li> </ul>"},{"location":"samples/python/cluster_management/index.html#prerequisites","title":"Prerequisites","text":"<ul> <li>Python version &gt;= 3.10 is installed.</li> <li>Valid AWS credentials can be discovered by the default provider chain.</li> </ul>"},{"location":"samples/python/cluster_management/index.html#execute-tests-to-create-and-delete-clusters","title":"Execute tests to create and delete clusters","text":"<pre><code># Optional: Single-Region examples will execute in CLUSTER_REGION. Defaults to 'us-east-1'.\nexport CLUSTER_REGION=\"us-east-1\"\n\n# Optional: Multi-Region examples will create clusters in CLUSTER_1_REGION and CLUSTER_2_REGION\n# with WITNESS_REGION as witness for both. Defaults to 'us-east-2' for CLUSTER_2_REGION\n# and 'us-west-2' for WITNESS_REGION.\nexport CLUSTER_1_REGION=\"us-east-1\"\nexport CLUSTER_2_REGION=\"us-east-2\"\nexport WITNESS_REGION=\"us-west-2\"\n\npython3 -m venv .venv &amp;&amp; source .venv/bin/activate\npip install -r requirements.txt\n\n# Will create, update, read, then delete clusters, use -s to see print statements when running tests\npytest test/test_dsql_cluster_management.py [-s]\n</code></pre> <p>Test execution will take around five minutes as it waits for clusters to complete activation and deletion.</p>"},{"location":"samples/python/cluster_management/index.html#executing-single-operations","title":"Executing single operations","text":"<p>Files in src/ each have a <code>main()</code> function that let you exercise single operations.</p> <pre><code># Check each operation for its expected environment variables\nCLUSTER_REGION=\"us-east-1\" CLUSTER_ID=\"&lt;your cluster id&gt;\" \\\n  python src/get_cluster.py\n</code></pre> <p>Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.</p> <p>SPDX-License-Identifier: MIT-0</p>"},{"location":"samples/python/jupyter/index.html","title":"Query Editors: Using JupyterLab with Aurora DSQL","text":"<p>This guide provides step-by-step instructions on how to connect and query Amazon Aurora DSQL using JupyterLab with Python. JupyterLab is a popular interactive computing environment that combines code, text, and visualizations in a single document. It\u2019s widely used for data science and research applications.</p> <p>The instructions below will cover the basics of Aurora DSQL usage in both a local installation of JupyterLab as well as using Amazon SageMaker AI, a fully-managed machine learning service that provides a hosted environment with a UI for data workflows.</p>"},{"location":"samples/python/jupyter/index.html#getting-started","title":"Getting started","text":""},{"location":"samples/python/jupyter/index.html#requirements","title":"Requirements","text":"<ul> <li>An Aurora DSQL cluster</li> <li>AWS credentials configured (local installation only)</li> <li>Python version 3.10 or greater (local installation only)</li> </ul>"},{"location":"samples/python/jupyter/index.html#using-local-jupyterlab","title":"Using local JupyterLab","text":"<p>To get started with JupyterLab, users must first install the application using Python\u2019s <code>pip</code>:</p> <pre><code>pip install jupyterlab\n</code></pre> <p>JupyterLab can then be opened by running <code>jupyter lab</code>. This will open the JupyterLab application at localhost:8888, accessible in a browser. Ensure you have AWS credentials configured in your local environment before proceeding.</p>"},{"location":"samples/python/jupyter/index.html#using-amazon-sagemaker-ai","title":"Using Amazon SageMaker AI","text":"<p>In the AWS console, proceed to the Amazon SageMaker AI console page and then to the \u2018Notebooks\u2019 section under \u2018Applications and IDEs\u2019. From there you can select \u2018Create notebook instance\u2019 to begin creating a SageMaker environment. Select an instance type and platform before clicking \u2018Create notebook instance\u2019.</p> <p>See Amazon SageMaker AI setup documentation for more information on setup and instance options.</p> <p>Warning: Using Amazon SageMaker AI may result in charges to your AWS account.</p> <p>Once the SageMaker instance becomes active, you can open it from the \u2018Notebook instances\u2019 section with \u2018Open JupyterLab\u2019 . Before getting started with Aurora DSQL in your notebook you must provide access to your DSQL cluster in the SageMaker instance\u2019s IAM role. The simplest way to do so is to follow the link to the IAM role in the notebook instance page. From there you can edit the Policies attached to your SageMaker IAM role. See Authentication and authorization for more information on configuring an IAM policy to allow access to Aurora DSQL.</p>"},{"location":"samples/python/jupyter/index.html#connecting-to-aurora-dsql-using-jupyterlab","title":"Connecting to Aurora DSQL using JupyterLab","text":"<p>Once you\u2019ve set up a JupyterLab instance, the steps to connect to Aurora DSQL are the same locally and in SageMaker AI. Create an empty Python 3 notebook, in which you can add cells with Python code.</p> <p>In a Python cell, download the Amazon root certificate from the official trust store:</p> <pre><code>import urllib.request\nurllib.request.urlretrieve('https://www.amazontrust.com/repository/AmazonRootCA1.pem', 'root.pem')\n</code></pre> <p>To connect to Aurora DSQL, first install the Aurora DSQL Connector for Python and the psycopg driver in a Python cell, and then import it:</p> <pre><code>pip install aurora_dsql_python_connector psycopg\n</code></pre> <pre><code>import aurora_dsql_psycopg2 as dsql\n</code></pre> <p>With the connector imported, you can then create a DSQL configuration and connect. The Aurora DSQL Python Connector will automatically handle creation of an authentication token on each connection.</p> <pre><code>config = {\n    'host': \"your-cluster.dsql.us-east-1.on.aws\",\n    'region': \"us-east-1\",\n    'user': \"admin\",\n    'sslmode': 'verify-full',\n    'sslrootcert': './root.pem'\n}\n\nconn = dsql.connect(**config)\n</code></pre> <p>Upon running your code you should now have a Psycopg connection to Aurora DSQL. You can then run queries using the Psycopg cursor and providing your SQL query. See the Psycopg documentation for more information on using Psycopg with a Postgres-compatible database. This query will result in a list of tuples in <code>results_list</code>.</p> <pre><code>with conn:\n    with conn.cursor() as cur:\n        cur.execute(\"SELECT * FROM table\")\n        results_list = cur.fetchall()\n</code></pre> <p>You can then use Python frameworks like Pandas to analyze or visualize your query results, for example:</p> <pre><code>pip install pandas\n</code></pre> <pre><code>import pandas as pd\n\ndf = pd.DataFrame(results_list)\nprint(df)\nprint(f\"Total records: {len(df)}\")\n</code></pre>"},{"location":"samples/python/jupyter/index.html#further-reading","title":"Further reading","text":"<p>Amazon SageMaker AI setup documentation</p> <p>Aurora DSQL Connector for Python</p> <p>Pandas documentation</p>"},{"location":"samples/python/psycopg/index.html","title":"Aurora DSQL with Psycopg","text":""},{"location":"samples/python/psycopg/index.html#overview","title":"Overview","text":"<p>This code example demonstrates how to use Psycopg (version 3) with Amazon Aurora SQL (DSQL). The example shows you how to connect to an Aurora DSQL cluster and perform basic database operations.</p> <p>Aurora DSQL is a distributed SQL database service that provides high availability and scalability for your PostgreSQL-compatible applications. Psycopg is a popular PostgreSQL adapter for Python that allows you to interact with PostgreSQL databases using Python code.</p>"},{"location":"samples/python/psycopg/index.html#about-the-code-example","title":"About the code example","text":"<p>This example uses the Aurora DSQL Python Connector which automatically handles IAM token generation for authentication.</p> <p>The example demonstrates a flexible connection approach that works for both admin and non-admin users:</p> <ul> <li>When connecting as an admin user, the example uses the <code>public</code> schema and generates an admin authentication token.</li> <li>When connecting as a non-admin user, the example uses a custom <code>myschema</code> schema and generates a standard authentication token.</li> </ul> <p>The code automatically detects the user type and adjusts its behavior accordingly.</p>"},{"location":"samples/python/psycopg/index.html#important","title":"\u26a0\ufe0f Important","text":"<ul> <li>Running this code might result in charges to your AWS account.</li> <li>We recommend that you grant your code least privilege. At most, grant only the   minimum permissions required to perform the task. For more information, see   Grant least privilege.</li> <li>This code is not tested in every AWS Region. For more information, see   AWS Regional Services.</li> </ul>"},{"location":"samples/python/psycopg/index.html#run-the-example","title":"Run the example","text":""},{"location":"samples/python/psycopg/index.html#prerequisites","title":"Prerequisites","text":"<ul> <li>You must have an AWS account, and have your default credentials and AWS Region   configured as described in the   Globally configuring AWS SDKs and tools   guide.</li> <li>Python 3.8.0 or later.</li> <li>You must have an Aurora DSQL cluster. For information about creating an Aurora DSQL cluster, see the   Getting started with Aurora DSQL   guide.</li> <li>If connecting as a non-admin user, ensure the user is linked to an IAM role and is granted access to the <code>myschema</code>   schema. See the   Using database roles with IAM roles   guide.</li> </ul>"},{"location":"samples/python/psycopg/index.html#set-up-environment-for-examples","title":"Set up environment for examples","text":"<ol> <li>Create and activate a Python virtual environment:</li> </ol> <pre><code>python3 -m venv .venv\nsource .venv/bin/activate  # Linux, macOS\n# or\n.venv\\Scripts\\activate     # Windows\n</code></pre> <ol> <li>Install the required packages for running the examples:</li> </ol> <pre><code>pip install -r requirements.txt\n</code></pre>"},{"location":"samples/python/psycopg/index.html#run-the-code","title":"Run the code","text":"<p>The example demonstrates the following operations:</p> <ul> <li>Opening a connection to an Aurora DSQL cluster</li> <li>Creating a table</li> <li>Inserting and querying data</li> </ul> <p>The example is designed to work with both admin and non-admin users:</p> <ul> <li>When run as an admin user, it uses the <code>public</code> schema</li> <li>When run as a non-admin user, it uses the <code>myschema</code> schema</li> </ul> <p>Note: running the example will use actual resources in your AWS account and may incur charges.</p> <p>Set environment variables for your cluster details:</p> <pre><code># e.g. \"admin\"\nexport CLUSTER_USER=\"&lt;your user&gt;\"\n\n# e.g. \"foo0bar1baz2quux3quuux4.dsql.us-east-1.on.aws\"\nexport CLUSTER_ENDPOINT=\"&lt;your endpoint&gt;\"\n</code></pre> <p>Run the example:</p> <pre><code>python src/example_preferred.py\n</code></pre> <p>The example contains comments explaining the code and the operations being performed.</p>"},{"location":"samples/python/psycopg/index.html#additional-resources","title":"Additional resources","text":"<ul> <li>Amazon Aurora DSQL Documentation</li> <li>Aurora DSQL Python Connector</li> <li>Psycopg Documentation</li> </ul> <p>Note: The connector automatically extracts the region from the cluster endpoint, defaults to the <code>postgres</code> database, and handles SSL configuration.</p> <p>Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.</p> <p>SPDX-License-Identifier: MIT-0</p>"},{"location":"samples/python/psycopg/src/alternatives/index.html","title":"Alternative Examples","text":"<p>The recommended approach is <code>example_preferred.py</code> in the parent directory, which uses a concurrent connection pool with the Aurora DSQL connector.</p>"},{"location":"samples/python/psycopg/src/alternatives/index.html#why-connection-pooling-with-the-connector","title":"Why Connection Pooling with the Connector?","text":"<p>Aurora DSQL has specific connection characteristics: - 60-minute max connection lifetime - connections are terminated after 1 hour - 15-minute token expiry - IAM auth tokens must be refreshed - Optimized for concurrency - more concurrent connections with smaller batches yields better throughput</p> <p>The connector + pool combination handles this automatically: - Generates fresh IAM tokens per connection - Recycles connections before the 60-minute limit (via <code>max_lifetime &lt; 3600</code>) - Reuses warmed connections (2-24ms faster than new connections)</p>"},{"location":"samples/python/psycopg/src/alternatives/index.html#alternatives","title":"Alternatives","text":""},{"location":"samples/python/psycopg/src/alternatives/index.html#pool","title":"<code>pool/</code>","text":"<p>Other pool configurations using the connector: - <code>example_with_async_connection_pool.py</code> - Async pool for asyncio applications - <code>example_with_nonconcurrent_connection_pool.py</code> - Simple pool without concurrency</p>"},{"location":"samples/python/psycopg/src/alternatives/index.html#no_connection_pool","title":"<code>no_connection_pool/</code>","text":"<p>Examples without pooling: - <code>example_with_no_connection_pool.py</code> - Single connection with connector - <code>example_async_with_no_connection_pool.py</code> - Async single connection with connector - <code>example_with_no_connector.py</code> - SDK-only, for environments where the connector cannot be used (requires manual token management)</p>"},{"location":"samples/python/psycopg2/index.html","title":"Aurora DSQL with Psycopg2","text":""},{"location":"samples/python/psycopg2/index.html#overview","title":"Overview","text":"<p>This code example demonstrates how to use Psycopg (version 2) with Amazon Aurora DSQL. The example shows you how to connect to an Aurora DSQL cluster and perform basic database operations.</p> <p>Aurora DSQL is a distributed SQL database service that provides high availability and scalability for your PostgreSQL-compatible applications. Psycopg2 is a popular PostgreSQL adapter for Python that allows you to interact with PostgreSQL databases using Python code.</p>"},{"location":"samples/python/psycopg2/index.html#about-the-code-example","title":"About the code example","text":"<p>This example uses the Aurora DSQL Python Connector which automatically handles IAM token generation for authentication.</p> <p>The example demonstrates a flexible connection approach that works for both admin and non-admin users:</p> <ul> <li>When connecting as an admin user, the example uses the <code>public</code> schema and generates an admin authentication token.</li> <li>When connecting as a non-admin user, the example uses a custom <code>myschema</code> schema and generates a standard authentication token.</li> </ul> <p>The code automatically detects the user type and adjusts its behavior accordingly.</p>"},{"location":"samples/python/psycopg2/index.html#important","title":"\u26a0\ufe0f Important","text":"<ul> <li>Running this code might result in charges to your AWS account.</li> <li>We recommend that you grant your code least privilege. At most, grant only the   minimum permissions required to perform the task. For more information, see   Grant least privilege.</li> <li>This code is not tested in every AWS Region. For more information, see   AWS Regional Services.</li> </ul>"},{"location":"samples/python/psycopg2/index.html#run-the-example","title":"Run the example","text":""},{"location":"samples/python/psycopg2/index.html#prerequisites","title":"Prerequisites","text":"<ul> <li>You must have an AWS account, and have your default credentials and AWS Region   configured as described in the   Globally configuring AWS SDKs and tools   guide.</li> <li>Python 3.8.0 or later.</li> <li>You must have an Aurora DSQL cluster. For information about creating an Aurora DSQL cluster, see the   Getting started with Aurora DSQL   guide.</li> <li>If connecting as a non-admin user, ensure the user is linked to an IAM role and is granted access to the <code>myschema</code>   schema. See the   Using database roles with IAM roles   guide.</li> </ul>"},{"location":"samples/python/psycopg2/index.html#set-up-environment-for-examples","title":"Set up environment for examples","text":"<ol> <li>Create and activate a Python virtual environment:</li> </ol> <pre><code>python3 -m venv .venv\nsource .venv/bin/activate  # Linux, macOS\n# or\n.venv\\Scripts\\activate     # Windows\n</code></pre> <ol> <li>Install the required packages for running the examples:</li> </ol> <pre><code>pip install -r requirements.txt\n</code></pre>"},{"location":"samples/python/psycopg2/index.html#run-the-code","title":"Run the code","text":"<p>The example demonstrates the following operations:</p> <ul> <li>Opening a connection to an Aurora DSQL cluster</li> <li>Creating a table</li> <li>Inserting and querying data</li> </ul> <p>The example is designed to work with both admin and non-admin users:</p> <ul> <li>When run as an admin user, it uses the <code>public</code> schema</li> <li>When run as a non-admin user, it uses the <code>myschema</code> schema</li> </ul> <p>Note: running the example will use actual resources in your AWS account and may incur charges.</p> <p>Set environment variables for your cluster details:</p> <pre><code># e.g. \"admin\"\nexport CLUSTER_USER=\"&lt;your user&gt;\"\n\n# e.g. \"foo0bar1baz2quux3quuux4.dsql.us-east-1.on.aws\"\nexport CLUSTER_ENDPOINT=\"&lt;your endpoint&gt;\"\n</code></pre> <p>Run the example:</p> <pre><code>python src/example_preferred.py\n</code></pre> <p>The example contains comments explaining the code and the operations being performed.</p>"},{"location":"samples/python/psycopg2/index.html#additional-resources","title":"Additional resources","text":"<ul> <li>Amazon Aurora DSQL Documentation</li> <li>Aurora DSQL Python Connector</li> <li>Psycopg2 Documentation</li> </ul> <p>Note: The connector automatically extracts the region from the cluster endpoint, defaults to the <code>postgres</code> database, and handles SSL configuration.</p> <p>Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.</p> <p>SPDX-License-Identifier: MIT-0</p>"},{"location":"samples/python/psycopg2/src/alternatives/index.html","title":"Alternative Examples","text":"<p>The recommended approach is <code>example_preferred.py</code> in the parent directory, which uses a concurrent connection pool with the Aurora DSQL connector.</p>"},{"location":"samples/python/psycopg2/src/alternatives/index.html#why-connection-pooling-with-the-connector","title":"Why Connection Pooling with the Connector?","text":"<p>Aurora DSQL has specific connection characteristics: - 60-minute max connection lifetime - connections are terminated after 1 hour - 15-minute token expiry - IAM auth tokens must be refreshed - Optimized for concurrency - more concurrent connections with smaller batches yields better throughput</p> <p>The connector + pool combination handles this automatically: - Generates fresh IAM tokens per connection - Recycles connections before the 60-minute limit - Reuses warmed connections (2-24ms faster than new connections)</p>"},{"location":"samples/python/psycopg2/src/alternatives/index.html#alternatives","title":"Alternatives","text":""},{"location":"samples/python/psycopg2/src/alternatives/index.html#pool","title":"<code>pool/</code>","text":"<p>Other pool configurations using the connector: - <code>example_with_nonconcurrent_connection_pool.py</code> - Simple pool without concurrency</p>"},{"location":"samples/python/psycopg2/src/alternatives/index.html#no_connection_pool","title":"<code>no_connection_pool/</code>","text":"<p>Examples without pooling: - <code>example_with_no_connection_pool.py</code> - Single connection with connector - <code>example_with_no_connector.py</code> - SDK-only, for environments where the connector cannot be used (requires manual token management)</p>"},{"location":"samples/python/sqlalchemy/index.html","title":"SQLAlchemy with Aurora DSQL","text":""},{"location":"samples/python/sqlalchemy/index.html#overview","title":"Overview","text":"<p>This code example demonstrates how to use SQLAlchemy with Amazon Aurora DSQL. The example shows you how to connect to an Aurora DSQL cluster with SQLAlchemy using Psycopg2, create entities, and read and write to those entity tables.</p> <p>Aurora DSQL is a distributed SQL database service that provides high availability and scalability for your PostgreSQL-compatible applications. SQLAlchemy is a popular object-relational mapping framework for Python that allows you to persist Python objects to a database while abstracting the database interactions.</p> <p>Note</p> <p>Note that SQLAlchemy with Psycopg3 does not work with Aurora DSQL. SQLAlchemy with Psycopg3 uses nested transactions which rely on savepoints as part of the connection setup. Savepoints are not supported by Aurora DSQL.</p>"},{"location":"samples/python/sqlalchemy/index.html#about-the-code-example","title":"About the code example","text":"<p>The example demonstrates a flexible connection approach that works for both admin and non-admin users:</p> <ul> <li>When connecting as an admin user, the example uses the <code>public</code> schema and generates an admin authentication   token.</li> <li>When connecting as a non-admin user, the example uses a custom <code>myschema</code> schema and generates a standard   authentication token.</li> </ul> <p>The code automatically detects the user type and adjusts its behavior accordingly.</p>"},{"location":"samples/python/sqlalchemy/index.html#important","title":"\u26a0\ufe0f Important","text":"<ul> <li>Running this code might result in charges to your AWS account.</li> <li>We recommend that you grant your code least privilege. At most, grant only the   minimum permissions required to perform the task. For more information, see   Grant least privilege.</li> <li>This code is not tested in every AWS Region. For more information, see   AWS Regional Services.</li> </ul>"},{"location":"samples/python/sqlalchemy/index.html#tls-connection-configuration","title":"TLS connection configuration","text":"<p>This example uses direct TLS connections where supported, and verifies the server certificate is trusted. Verified SSL connections should be used where possible to ensure data security during transmission.</p> <ul> <li>Driver versions following the release of PostgreSQL 17 support direct TLS connections, bypassing the traditional   PostgreSQL connection preamble</li> <li>Direct TLS connections provide improved connection performance and enhanced security</li> <li>Not all PostgreSQL drivers support direct TLS connections yet, or only in recent versions following PostgreSQL 17</li> <li>Ensure your installed driver version supports direct TLS negotiation, or use a version that is at least as recent as   the one used in this sample</li> <li>If your driver doesn't support direct TLS connections, you may need to use the traditional preamble connection instead</li> </ul>"},{"location":"samples/python/sqlalchemy/index.html#run-the-example","title":"Run the example","text":""},{"location":"samples/python/sqlalchemy/index.html#prerequisites","title":"Prerequisites","text":"<ul> <li>You must have an AWS account, and have your default credentials and AWS Region   configured as described in the   Globally configuring AWS SDKs and tools   guide.</li> <li>Python 3.8.0 or later.</li> <li>You must have an Aurora DSQL cluster. For information about creating an Aurora DSQL cluster, see the   Getting started with Aurora DSQL   guide.</li> <li>If connecting as a non-admin user, ensure the user is linked to an IAM role and is granted access to the <code>myschema</code>   schema. See the   Using database roles with IAM roles   guide.</li> </ul>"},{"location":"samples/python/sqlalchemy/index.html#download-the-amazon-root-certificate-from-the-official-trust-store","title":"Download the Amazon root certificate from the official trust store","text":"<p>Download the Amazon root certificate from the official trust store:</p> <pre><code>wget https://www.amazontrust.com/repository/AmazonRootCA1.pem -O root.pem\n</code></pre>"},{"location":"samples/python/sqlalchemy/index.html#set-up-environment-for-examples","title":"Set up environment for examples","text":"<ol> <li>Create and activate a Python virtual environment:</li> </ol> <pre><code>python3 -m venv .venv\nsource .venv/bin/activate  # Linux, macOS\n# or\n.venv\\Scripts\\activate     # Windows\n</code></pre> <ol> <li>Install the required packages for running the examples:</li> </ol> <pre><code>pip install \"psycopg2-binary&gt;=2.9\"\npip install \"sqlalchemy\"\npip install \"boto3&gt;=1.35.74\"\n</code></pre>"},{"location":"samples/python/sqlalchemy/index.html#run-the-code","title":"Run the code","text":"<p>The example demonstrates the following operations:</p> <ul> <li>Opening a connection pool to an Aurora DSQL cluster using a SQLAlchemy </li> <li>Creating several SQLAlchemy entities</li> <li>Creating and querying objects that are persisted in DSQL</li> </ul> <p>The example is designed to work with both admin and non-admin users:</p> <ul> <li>When run as an admin user, it uses the <code>public</code> schema</li> <li>When run as a non-admin user, it uses the <code>myschema</code> schema</li> </ul> <p>Note: running the example will use actual resources in your AWS account and may incur charges.</p> <p>Set environment variables for your cluster details:</p> <pre><code># e.g. \"admin\"\nexport CLUSTER_USER=\"&lt;your user&gt;\"\n\n# e.g. \"foo0bar1baz2quux3quuux4.dsql.us-east-1.on.aws\"\nexport CLUSTER_ENDPOINT=\"&lt;your endpoint&gt;\"\n\n# e.g. \"us-east-1\"\nexport REGION=\"&lt;your region&gt;\"\n</code></pre> <p>Run the example:</p> <pre><code>python src/example.py\n</code></pre> <p>The example contains comments explaining the code and the operations being performed.</p>"},{"location":"samples/python/sqlalchemy/index.html#sqlalchemy-pet-clinic-with-dsql","title":"SQLAlchemy Pet Clinic with DSQL","text":""},{"location":"samples/python/sqlalchemy/index.html#connect-to-an-aurora-dsql-cluster","title":"Connect to an Aurora DSQL cluster","text":"<p>The example below shows how to create an Aurora DSQL engine in SQLAlchemy and connect to a cluster. It handles token generation, creating a new token for each connection to DSQL. This ensures that the token is always valid. This is done using SQLAlchemy's event annotation to create a listener to the engine that creates a new token when connections are created.</p> <pre><code>import os\nimport boto3\nfrom sqlalchemy import create_engine, select, event\nfrom sqlalchemy.engine import URL\n\ndef create_dsql_engine():\n    cluster_user = os.environ.get(\"CLUSTER_USER\", None)\n    assert cluster_user is not None, \"CLUSTER_USER environment variable is not set\"\n\n    cluster_endpoint = os.environ.get(\"CLUSTER_ENDPOINT\", None)\n    assert cluster_endpoint is not None, \"CLUSTER_ENDPOINT environment variable is not set\"\n\n    region = os.environ.get(\"REGION\", None)\n    assert region is not None, \"REGION environment variable is not set\"\n\n    client = boto3.client(\"dsql\", region_name=region)\n\n    # Create the URL, note that the password token is added when connections are created.\n    url = URL.create(\n        \"postgresql\", \n        username=cluster_user, \n        host=cluster_endpoint, \n        database=\"postgres\"\n    )\n\n    # Create the engine\n    engine = create_engine(\n        url, \n        connect_args={\"sslmode\": \"verify-full\", \"sslrootcert\": \"./root.pem\"},\n    )\n\n    # Adds a listener that creates a new token every time a new connection is created in the SQLAlchemy engine\n    @event.listens_for(engine, \"do_connect\")\n    def add_token_to_params(dialect, conn_rec, cargs, cparams):\n        # Generate a fresh token for this connection\n        fresh_token = generate_token(client, cluster_user, cluster_endpoint, region)        \n        # Update the password in connection parameters\n        cparams[\"password\"] = fresh_token\n\n    # If we are using the non-admin user, we need to set the search path to use 'myschema' instead of public whenever a connection is created.\n    @event.listens_for(engine, \"connect\", insert=True)\n    def set_search_path(dbapi_connection, connection_record):\n        if cluster_user == ADMIN: return\n        existing_autocommit = dbapi_connection.autocommit\n        dbapi_connection.autocommit = True\n        cursor = dbapi_connection.cursor()\n        cursor.execute(\"SET SESSION search_path='%s'\" % NON_ADMIN_SCHEMA)\n        cursor.close()\n        dbapi_connection.autocommit = existing_autocommit\n\n    return engine\n\ndef generate_token(client, cluster_user, cluster_endpoint, region):\n    if (cluster_user == ADMIN):\n        return client.generate_db_connect_admin_auth_token(cluster_endpoint, region)\n    else:\n        return client.generate_db_connect_auth_token(cluster_endpoint, region)\n</code></pre>"},{"location":"samples/python/sqlalchemy/index.html#connection-pooling","title":"Connection Pooling","text":"<p>In SQLAlchemy, connection pooling is  enabled by default when the engine is created and each engine is automatically associated with a connection pool.  In the example above, a new token is created for each connection opened in the connection pool. Note that DSQL connections will automatically close after one hour. The connection pool will open new connections as needed.</p>"},{"location":"samples/python/sqlalchemy/index.html#create-models","title":"Create models","text":""},{"location":"samples/python/sqlalchemy/index.html#using-uuid-as-primary-key","title":"Using UUID as Primary Key","text":"<p>DSQL does not support serialized primary keys or identity columns (auto-incrementing integers) that are commonly used in traditional relational databases. Instead, it is recommended to use UUID (Universally Unique Identifier) as the primary key for your entities.</p> <p>Here's how to define a UUID primary key in your entity class: <pre><code>    id = Column(\"id\", UUID, primary_key=True, default=text('gen_random_uuid()'))\n</code></pre></p>"},{"location":"samples/python/sqlalchemy/index.html#model-definitions","title":"Model definitions","text":"<pre><code>from sqlalchemy import String\nfrom sqlalchemy.orm import DeclarativeBase\nfrom sqlalchemy.orm import relationship\nfrom sqlalchemy import Column, Date\nfrom sqlalchemy.dialects.postgresql import UUID\nfrom sqlalchemy.sql import text\n\nclass Base(DeclarativeBase):\n    pass\n\n# Define a Owner table\nclass Owner(Base):\n    __tablename__ = \"owner\"\n\n    id = Column(\n                \"id\", UUID, primary_key=True, default=text('gen_random_uuid()')\n            )\n    name = Column(\"name\", String(30), nullable=False)\n    city = Column(\"city\", String(80), nullable=False)\n    telephone = Column(\"telephone\", String(20), nullable=True, default=None)\n\n# Define a Pet table\nclass Pet(Base):\n    __tablename__ = \"pet\"\n\n    id = Column(\n                \"id\", UUID, primary_key=True, default=text('gen_random_uuid()')\n            )\n    name = Column(\"name\", String(30), nullable=False)\n    birth_date = Column(\"birth_date\", Date(), nullable=False)\n    owner_id = Column(\n                \"owner_id\", UUID, nullable=True\n    )\n    # One to many\n    owner = relationship(\"Owner\", foreign_keys=[owner_id], primaryjoin=\"Owner.id == Pet.owner_id\")\n\n# Define an association table for Vet and Specialty, this is an intermediate table\n# that lets us define the many-to-many mapping\nclass VetSpecialties(Base):\n    __tablename__ = \"vetSpecialties\"\n\n    id = Column(\n                \"id\", UUID, primary_key=True, default=text('gen_random_uuid()')\n            )\n    vet_id = Column(\n                \"vet_id\", UUID, nullable=True\n    )\n    specialty_id = Column(\n                \"specialty_id\", String(80), nullable=True\n    )\n\n# Define a Specialty table\nclass Specialty(Base):\n    __tablename__ = \"specialty\"\n    id = Column(\n                \"name\", String(80), primary_key=True\n            )\n\n# Define a Vet table\nclass Vet(Base):\n    __tablename__ = \"vet\"\n\n    id = Column(\n                \"id\", UUID, primary_key=True, default=text('gen_random_uuid()')\n            )\n    name = Column(\"name\", String(30), nullable=False)\n    # Many-to-Many mapping\n    specialties = relationship(\"Specialty\", secondary=VetSpecialties.__table__,\n        primaryjoin=\"foreign(VetSpecialties.vet_id)==Vet.id\",\n        secondaryjoin=\"foreign(VetSpecialties.specialty_id)==Specialty.id\")\n</code></pre>"},{"location":"samples/python/sqlalchemy/index.html#additional-resources","title":"Additional resources","text":"<ul> <li>Amazon Aurora DSQL Documentation</li> <li>SQLAlchemy Documentation</li> <li>Psycopg Documentation</li> <li>AWS SDK for Python (Boto3) Documentation</li> </ul> <p>Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved. </p> <p>SPDX-License-Identifier: MIT-0</p>"},{"location":"samples/ruby/cluster_management/index.html","title":"Aurora DSQL Ruby code examples","text":""},{"location":"samples/ruby/cluster_management/index.html#overview","title":"Overview","text":"<p>The code examples in this topic show you how to use the AWS Ruby SDK with DSQL to create, update, get, and delete single- and multi-Region clusters.</p> <p>Each file in the /lib directory demonstrates a minimal working example for each operation. The example function for each operation is invoked in <code>dsql_cluster_management_spec.rb</code>.</p>"},{"location":"samples/ruby/cluster_management/index.html#run-the-examples","title":"Run the examples","text":""},{"location":"samples/ruby/cluster_management/index.html#important","title":"\u26a0\ufe0f Important","text":"<ul> <li>Running this code might result in charges to your AWS account.</li> <li>We recommend that you grant your code least privilege. At most, grant only the   minimum permissions required to perform the task. For more information, see   Grant least privilege.</li> <li>This code is not tested in every AWS Region. For more information, see   AWS Regional Services.</li> </ul>"},{"location":"samples/ruby/cluster_management/index.html#prerequisites","title":"Prerequisites","text":"<ul> <li>Ruby version &gt;= 3.3 is installed.</li> <li>MacOS Optional: Use rbenv to manage Ruby version</li> </ul> <pre><code># Optional use rbenv\nrbenv install 3.3.5\nrbenv local 3.3.5\n\nruby --version\n</code></pre> <ul> <li>Valid AWS credentials can be discovered by   the default provider chain.</li> </ul>"},{"location":"samples/ruby/cluster_management/index.html#execute-tests-to-create-and-delete-clusters","title":"Execute tests to create and delete clusters","text":"<pre><code># Optional: Single-Region examples will execute in CLUSTER_REGION. Defaults to 'us-east-1'.\nexport CLUSTER_REGION=\"us-east-1\"\n\n# Optional: Multi-Region examples will create clusters in CLUSTER_1_REGION and CLUSTER_2_REGION\n# with WITNESS_REGION as witness for both. Defaults to 'us-east-1' for CLUSTER_1_REGION, 'us-east-2' \n# for CLUSTER_2_REGION and 'us-west-2' for WITNESS_REGION.\nexport CLUSTER_1_REGION=\"us-east-1\"\nexport CLUSTER_2_REGION=\"us-east-2\"\nexport WITNESS_REGION=\"us-west-2\"\n\nbundle install\n\n# Will create, update, read, then delete clusters\nrspec\n</code></pre> <p>Test execution will take around five minutes as it waits for clusters to complete activation and deletion.</p>"},{"location":"samples/ruby/cluster_management/index.html#executing-single-operations","title":"Executing single operations","text":"<p>Files in lib/ each have a <code>main()</code> function that let you exercise single operations.</p> <pre><code># Check each operation for its expected environment variables\nCLUSTER_REGION=\"us-east-1\" CLUSTER_ID=\"&lt;your cluster id&gt;\" \\\n  ruby lib/create_single_region_cluster.rb\n</code></pre> <p>Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.</p> <p>SPDX-License-Identifier: MIT-0</p>"},{"location":"samples/ruby/rails/index.html","title":"Aurora DSQL with Ruby on Rails","text":"<p>This example demonstrates how to use an Aurora DSQL cluster with a Ruby On Rails application. Aurora DSQL only supports token-based authentication so we extend the <code>pg-aws_rds_iam</code> plugin to generate Aurora DSQL auth tokens when required.</p> <p>It also includes changes to ActiveRecord behavior to be compatible with Aurora DSQL supported features.</p>"},{"location":"samples/ruby/rails/index.html#running-this-example","title":"Running this example","text":"<p>See <code>petclinic/README.md</code>.</p>"},{"location":"samples/ruby/rails/index.html#using-aurora-dsql-authentication-tokens-with-rails","title":"Using Aurora DSQL authentication tokens with Rails","text":"<p>These are the changes to make to your Rails application to be compatible with Aurora DSQL.</p>"},{"location":"samples/ruby/rails/index.html#add-a-token-generator","title":"Add a token generator","text":"<p>To modify your Rails application to work with Aurora DSQL you should reproduce the <code>DsqlAuthTokenGenerator</code> in <code>adapter.rb</code>.</p> <pre><code>require \"aws-sdk-dsql\"\n\nclass DsqlAuthTokenGenerator\n  def call(host:, port:, user:)\n    # e.g. host == \"&lt;clusterID&gt;.dsql.us-east-1.on.aws\"\n    region = host.split(\".\")[2]\n    raise \"Unable to extract AWS region from host '#{host}'\" unless region =~ /[\\w\\d-]+/\n\n    token_generator = Aws::DSQL::AuthTokenGenerator.new(\n      credentials: Aws::CredentialProviderChain.new.resolve,\n    )\n\n    auth_token_params = {\n      endpoint: host,\n      region: region,\n      expires_in: 15 * 60 # 15 minutes, optional\n    }\n\n    case user\n    when \"admin\"\n      token_generator.generate_db_connect_admin_auth_token(auth_token_params)\n    else\n      token_generator.generate_db_connect_auth_token(auth_token_params)\n    end\n  end\nend\n</code></pre> <p><code>call</code> will be invoked when a new database connection is requested. It will: 1. Retrieve credentials for the running environment. The <code>Aws::CredentialProviderChain</code> discovers credentials    in the order described in these docs. 1. Determine which token type to generate based on the database user.</p> <p>The retrieved credentials will need permission to <code>dsql:DbConnectAdmin</code> for the <code>admin</code> user or <code>dsql:DbConnect</code> for a custom user. See Aurora DSQL documentation for IAM role connect and authentication token generation for more details.</p> <p>Finally, register the adapter with the <code>pg-aws_rds_iam</code> plugin. <pre><code>PG::AWS_RDS_IAM.auth_token_generators.add :dsql do\n  DsqlAuthTokenGenerator.new\nend\n</code></pre></p>"},{"location":"samples/ruby/rails/index.html#alter-activerecord-behavior","title":"Alter ActiveRecord behavior","text":"<p>Disable features not supported by Aurora DSQL. The example includes this in <code>adapter.rb</code>.</p> <pre><code>require \"active_record/connection_adapters/postgresql/schema_statements\"\n\nmodule ActiveRecord::ConnectionAdapters::PostgreSQL::SchemaStatements\n  # DSQL does not support setting min_messages in the connection parameters\n  def client_min_messages=(level); end\nend\n\nrequire \"active_record/connection_adapters/postgresql_adapter\"\n\nclass ActiveRecord::ConnectionAdapters::PostgreSQLAdapter\n  def set_standard_conforming_strings; end\n\n  # Avoid error running multiple DDL or DDL + DML statements in the same transaction\n  def supports_ddl_transactions?\n    false\n  end\nend\n</code></pre>"},{"location":"samples/ruby/rails/index.html#use-the-adapter-in-the-database-configuration","title":"Use the adapter in the database configuration","text":"<p>Refer to <code>database.yml</code>.</p> <pre><code>development:\n  &lt;&lt;: *default\n\n  # Always the database name for Aurora DSQL\n  database: postgres\n\n  # eg: admin or other postgres users\n  username: &lt;postgres username&gt;\n\n  # Set this value based on the access of the configured user,\n  # or omit if running as 'admin' and using the 'public' schema.\n  schema_search_path: myschema\n\n  # Set to Aurora DSQL instance endpoint\n  # Use environment variables, etc for production values!\n  # e.g. {clusterId}.dsql.{region}.on.aws\n  host: foo0bar1baz2quux3quuux4.dsql.us-east-1.on.aws\n\n  # Use the custom token generator we created\n  aws_rds_iam_auth_token_generator: dsql\n\n  # Provide the path to the root certificate. \n  # Amazon's root certs can be fetched from https://www.amazontrust.com/repository/\n  sslrootcert: &lt;replace with the path to root certificate&gt;\n  sslmode: verify-full\n\n  # More DSQL compatibility tweaks\n  advisory_locks: false\n  prepared_statements: false\n</code></pre> <p>Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.</p> <p>SPDX-License-Identifier: MIT-0</p>"},{"location":"samples/ruby/rails/petclinic/index.html","title":"Aurora DSQL Pet Clinic","text":"<p>This example demonstrates how to use an Aurora DSQL cluster with a Ruby On Rails application. Aurora DSQL only supports token-based authentication so we extend the <code>pg-aws_rds_iam</code> plugin to generate Aurora DSQL auth tokens when required.</p> <p>It also includes changes to ActiveRecord behavior to be compatible with Aurora DSQL supported features.</p>"},{"location":"samples/ruby/rails/petclinic/index.html#important","title":"\u26a0\ufe0f Important","text":"<ul> <li>Running this code might result in charges to your AWS account.</li> <li>Running the tests might result in charges to your AWS account.</li> <li>We recommend that you grant your code least privilege. At most, grant only the   minimum permissions required to perform the task. For more information, see   Grant least privilege.</li> <li>This code is not tested in every AWS Region. For more information, see   AWS Regional Services.</li> </ul>"},{"location":"samples/ruby/rails/petclinic/index.html#tls-connection-configuration","title":"TLS connection configuration","text":"<p>This example uses direct TLS connections where supported, and verifies the server certificate is trusted. Verified SSL connections should be used where possible to ensure data security during transmission.</p> <ul> <li>Driver versions following the release of PostgreSQL 17 support direct TLS connections, bypassing the traditional   PostgreSQL connection preamble</li> <li>Direct TLS connections provide improved connection performance and enhanced security</li> <li>Not all PostgreSQL drivers support direct TLS connections yet, or only in recent versions following PostgreSQL 17</li> <li>Ensure your installed driver version supports direct TLS negotiation, or use a version that is at least as recent as   the one used in this sample</li> <li>If your driver doesn't support direct TLS connections, you may need to use the traditional preamble connection instead</li> </ul>"},{"location":"samples/ruby/rails/petclinic/index.html#prerequisites","title":"Prerequisites","text":"<ul> <li>You must have an AWS account, and have your default credentials and AWS Region   configured as described in the   Globally configuring AWS SDKs and tools   guide.</li> <li>Ruby 3.3.5 or later.</li> <li>You must have an Aurora DSQL cluster. For information about creating an Aurora DSQL cluster, see the   Getting started with Aurora DSQL   guide.</li> <li>If connecting as a non-admin user, ensure the user is linked to an IAM role and is granted access to the <code>myschema</code>   schema. See the   Using database roles with IAM roles   guide.</li> </ul>"},{"location":"samples/ruby/rails/petclinic/index.html#quick-start","title":"Quick start","text":"<p>Install ruby 3.3.5 with rbenv: <pre><code>rbenv install 3.3.5\n</code></pre></p> <p>Install the Rails application: <pre><code>cd petclinic\n\n# Download the Amazon root certificate from the official trust store:\nwget https://www.amazontrust.com/repository/AmazonRootCA1.pem -O root.pem\n\nbundle install\n</code></pre></p> <p>Prepare the schema and open an interactive console: <pre><code>export CLUSTER_ENDPOINT=\"&lt;your cluster endpoint&gt;\"\n\n# Generate the schema from the model files in db/migrate.\nbin/rails db:migrate\n\n# Start the rails console\nbin/rails console\n</code></pre></p>"},{"location":"samples/ruby/rails/petclinic/index.html#working-with-the-example-model","title":"Working with the example model","text":""},{"location":"samples/ruby/rails/petclinic/index.html#1-create-owner-model","title":"1. Create Owner Model","text":"<p>Let's assume we are creating a table that stores list of pet owners. Create corresponding model using</p> <pre><code># Execute in the app root directory\nbin/rails generate model Owner name:string city:string telephone:string\n</code></pre> <p>This will create a model (<code>app/models/owner.rb</code>) file and a migration file (<code>db/migrate/&lt;time stamp&gt;_create_owners.rb</code>) Change the model file to explicitly specify the primary key of the table.  Unlike postgres, by default, Aurora DSQL creates a primary key index by including all columns of the table. This makes active record to search using all columns of the table instead of just primary key. So the <code>&lt;Entity&gt;.find(&lt;primary key&gt;)</code> will not work because active record tries to search using all columns in the primary key index. <code>.find_by(&lt;cloumn name&gt;: \"&lt;value&gt;\")</code> works fine. To make active record search only using primary key column by default, we must set the primary key column explicitly  in the model as shown below.</p> <pre><code>class Owner &lt; ApplicationRecord\n  self.primary_key = \"id\"\nend\n</code></pre> <p>Generate the schema from the model files in db/migrate.</p> <pre><code>bin/rails db:migrate\n</code></pre> <p>Finally, disable the <code>plpgsql</code> extension by modifying the <code>{app root directory}/db/schema.rb</code> . In order to disable the plpgsql extension, remove the <code>enable_extension \"plgsql\"</code> line.</p>"},{"location":"samples/ruby/rails/petclinic/index.html#2-create-owner","title":"2. Create Owner","text":"<pre><code>owner = Owner.new(name: \"John Doe\", city: \"Anytown\", telephone: \"555-555-0150\")\nowner.save\nowner\n</code></pre>"},{"location":"samples/ruby/rails/petclinic/index.html#3-read-owner","title":"3. Read Owner","text":"<pre><code>Owner.find(\"&lt;owner id&gt;\")\n</code></pre>"},{"location":"samples/ruby/rails/petclinic/index.html#4-update-owner","title":"4. Update Owner","text":"<pre><code>Owner.find(\"&lt;owner id&gt;\").update(telephone: \"555-555-0123\")\n</code></pre>"},{"location":"samples/ruby/rails/petclinic/index.html#5-delete-owner","title":"5. Delete Owner","text":"<pre><code>Owner.find(\"&lt;owner id&gt;\").destroy\n</code></pre>"},{"location":"samples/ruby/rails/petclinic/index.html#relational-mapping-examples","title":"Relational Mapping Examples","text":"<p>The pet clinic example code base contains some of the typical ralationships that are often used in an ORM type application. This includes representations of one-to-one, one-to-many and also many-to-many definitions.The following examples show how to support these scenarios within Aurora DSQL, and enable building relational structured models in this environment.  The various model definitions capturing the relationships can be found in the <code>app/models</code> directory.</p> <p>The following examples will reuse the same owner instantiation created here.</p> <pre><code>john_doe = Owner.new(name: \"John Doe\", city: \"Anytown\", telephone: \"555-555-0150\")\njohn_doe.save\n</code></pre>"},{"location":"samples/ruby/rails/petclinic/index.html#one-to-one-mapping","title":"One-to-One Mapping","text":"<p>For the pet clinic example app, there is a one-to-many relationship defined between the owner and pet model.  This can be observed in the code snippets below taken from the <code>app/models/owner.rb</code> model definition that shows the association.</p> <pre><code>class Owner &lt; ApplicationRecord\n  ...\n  has_one :vet\n</code></pre> <p>Create a vet instantiation, associate it with the owner, then read it back to test the association.</p> <pre><code>dr_carlos_salazar = Vet.create(name: \"Dr. Carlos Salazar\")\njohn_doe.vet=dr_carlos_salazar\njohn_doe.vet\n</code></pre>"},{"location":"samples/ruby/rails/petclinic/index.html#one-to-many-mapping","title":"One-to-Many Mapping","text":"<p>For the pet clinic example app, there is a one-to-many relationship defined between the owner and pet models.  This can be observed in the code snippets below taken from the <code>app/models/owner.rb</code> and the <code>app/models/pet.rb</code> model definitions respectively.</p> <pre><code>class Owner &lt; ApplicationRecord\n  has_many :pets, dependent: :destroy\n</code></pre> <pre><code>class Pet &lt; ApplicationRecord\n  belongs_to :owner\n</code></pre> <p>Create an owner with multiple pet instances, and then read the list of pets belonging to the owner. When the owner is deleted, the pets owned will be removed from the system.</p> <pre><code>pet1 = john_doe.pets.create(name: \"Pet-1\", birth_date: \"2022-01-17\")\npet2 = john_doe.pets.create(name: \"Pet-2\", birth_date: \"2023-10-01\")\njohn_doe.pets\n</code></pre>"},{"location":"samples/ruby/rails/petclinic/index.html#many-to-many-mapping","title":"Many-to-Many Mapping","text":"<p>For the pet clinic example app, there is a many-to-many relationship defined between a vet and a set of specialties that a particular vet has.  The relationship definition in this case makes use of an intermediary join table to map any number of vet instances to any number of skills that they possess. The definition for these relationships can be seen in the <code>app/models/vet.rb</code> and <code>app/models/specialty.rb</code> models, and in the <code>app/models/vet_specialty.rb</code> model which maintains the relationship data in a join table.</p> <pre><code>class Vet &lt; ApplicationRecord\n  has_many :vet_specialties , dependent: :delete_all\n  has_many :specialties, through: :vet_specialties\n</code></pre> <pre><code>class Specialty &lt; ApplicationRecord\n  has_many :vet_specialties\n  has_many :vets, through: :vet_specialties\n</code></pre> <pre><code>class VetSpecialty &lt; ApplicationRecord\n  belongs_to :vet\n  belongs_to :specialty\n</code></pre> <p>Create a set of specialties for a vet and read this list back.  The specialties created in this example will exist even after the vet has been removed from the system.  Only the relationship captured in the vet specialties table will be removed on vet deletion.</p> <pre><code>small_pets = Specialty.create(name: \"small pets\")\nminor_surgery = Specialty.create(name: \"minor surgery\")\ndr_carlos_salazar.specialties &lt;&lt; small_pets\ndr_carlos_salazar.specialties &lt;&lt; minor_surgery\ndr_carlos_salazar.specialties\n</code></pre> <p>In order to see the many-to-many relationship mapping between all vets and specialties, retrieve the contents of the three tables with the following commands.</p> <pre><code>Vet.all\nSpecialty.all\nVetSpecialty.all\n</code></pre> <p>Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.</p> <p>SPDX-License-Identifier: MIT-0</p>"},{"location":"samples/ruby/ruby-pg/index.html","title":"Aurora DSQL with Ruby-pg","text":""},{"location":"samples/ruby/ruby-pg/index.html#overview","title":"Overview","text":"<p>This code example demonstrates how to use Ruby-pg to interact with Amazon Aurora DSQL (DSQL). The example shows you how to connect to an Aurora DSQL cluster and perform basic database operations.</p> <p>Aurora DSQL is a distributed SQL database service that provides high availability and scalability for your PostgreSQL-compatible applications. Ruby-pg is a popular PostgreSQL adapter for Ruby that allows you to interact with PostgreSQL databases using Ruby code.</p>"},{"location":"samples/ruby/ruby-pg/index.html#about-the-code-example","title":"About the code example","text":"<p>The example demonstrates a flexible connection approach that works for both admin and non-admin users:</p> <ul> <li>When connecting as an admin user, the example uses the <code>public</code> schema and generates an admin authentication   token.</li> <li>When connecting as a non-admin user, the example uses a custom <code>myschema</code> schema and generates a standard   authentication token. The <code>myschema</code> schema needs to be created prior to running the example and the non-admin user needs to be granted access to the schema.</li> </ul> <p>The code automatically detects the user type and adjusts its behavior accordingly. The example contains comments explaining the code and the operations being performed.</p>"},{"location":"samples/ruby/ruby-pg/index.html#important","title":"\u26a0\ufe0f Important","text":"<ul> <li>Running this code might result in charges to your AWS account.</li> <li>We recommend that you grant your code least privilege. At most, grant only the   minimum permissions required to perform the task. For more information, see   Grant least privilege.</li> <li>This code is not tested in every AWS Region. For more information, see   AWS Regional Services.</li> </ul>"},{"location":"samples/ruby/ruby-pg/index.html#tls-connection-configuration","title":"TLS connection configuration","text":"<p>This example uses direct TLS connections where supported, and verifies the server certificate is trusted. Verified SSL connections should be used where possible to ensure data security during transmission.</p> <ul> <li>Driver versions following the release of PostgreSQL 17 support direct TLS connections, bypassing the traditional   PostgreSQL connection preamble</li> <li>Direct TLS connections provide improved connection performance and enhanced security</li> <li>Not all PostgreSQL drivers support direct TLS connections yet, or only in recent versions following PostgreSQL 17</li> <li>Ensure your installed driver version supports direct TLS negotiation, or use a version that is at least as recent as   the one used in this sample</li> <li>If your driver doesn't support direct TLS connections, you may need to use the traditional preamble connection instead</li> </ul>"},{"location":"samples/ruby/ruby-pg/index.html#run-the-example","title":"Run the example","text":""},{"location":"samples/ruby/ruby-pg/index.html#prerequisites","title":"Prerequisites","text":"<ul> <li>You must have an AWS account, and have your default credentials and AWS Region   configured as described in the   Globally configuring AWS SDKs and tools   guide.</li> <li>You must have an Aurora DSQL cluster. For information about creating an Aurora DSQL cluster, see the   Getting started with Aurora DSQL   guide.</li> <li>If connecting as a non-admin user, ensure the user is linked to an IAM role and is granted access to the <code>myschema</code>   schema. See the   Using database roles with IAM roles   guide.</li> </ul>"},{"location":"samples/ruby/ruby-pg/index.html#driver-dependencies","title":"Driver Dependencies","text":"<p>Before using the Ruby-pg driver, ensure you have the following prerequisites installed: Ruby: Ensure you have ruby v3+ installed from the official website.</p> <p>Verify install</p> <pre><code>ruby --version\n</code></pre>"},{"location":"samples/ruby/ruby-pg/index.html#libpq-library","title":"Libpq library","text":"<p>Libpq is required by Ruby-pg</p>"},{"location":"samples/ruby/ruby-pg/index.html#obtaining-the-libpq-library","title":"Obtaining the libpq library","text":"<ul> <li>It is installed with postgres installation. Therefore, if postgres is installed on the system the libpq is present in ../postgres_install_dir/lib, ../postgres_install_dir/include</li> <li>It is installed when psql client program is installed, similarily as with postgres installation. </li> <li>On some systems libpq can be installed through package manager  e.g.</li> <li>On Amazon Linux     <pre><code>sudo yum install libpq-devel\n</code></pre></li> <li>On Mac libpq can be installed using brew     <pre><code>brew install libpq\n</code></pre></li> <li>The official website may have a package for libpq or psql (which bundles libpq)</li> <li>Ultimately, build from source which also can be obtained from official website </li> </ul>"},{"location":"samples/ruby/ruby-pg/index.html#add-libpq-to-path","title":"Add libpq to PATH","text":"<p>In some cases, it may be necessary to add the location of the libpq/bin directory to PATH </p> <pre><code>export PATH=\"$PATH:&lt;your installed location&gt;/libpq/bin\"\n</code></pre>"},{"location":"samples/ruby/ruby-pg/index.html#install-ruby-pg-aurora-dsql-sdk-and-other-required-dependencies","title":"Install Ruby-pg, Aurora DSQL SDK and other required dependencies","text":"<ul> <li>All the required dependencies are present in the <code>Gemfile</code> file. To get all the required dependencies, run the following command from the directory where the <code>Gemfile</code> is present.</li> </ul> <pre><code>bundle install\n</code></pre>"},{"location":"samples/ruby/ruby-pg/index.html#download-the-amazon-root-certificate-from-the-official-trust-store","title":"Download the Amazon root certificate from the official trust store","text":"<p>Download the Amazon root certificate from the official trust store:</p> <pre><code>wget https://www.amazontrust.com/repository/AmazonRootCA1.pem -O root.pem\n</code></pre> <p>Place the root.pem file in the same directory as the hello_dsql.rb example file or modify the path to it in the example file.</p>"},{"location":"samples/ruby/ruby-pg/index.html#set-the-environmet-variables-specifying-cluster-endpoint-region-and-cluster-user","title":"Set the environmet variables specifying cluster endpoint, region and cluster user","text":"<pre><code># e.g. 'admin' or a custom user \nexport CLUSTER_USER=&lt;your cluster user&gt; \n\n# e.g. \"foo0bar1baz2quux3quuux4.dsql.us-east-1.on.aws\"\nexport CLUSTER_ENDPOINT=\"&lt;your cluster endpoint&gt;\"\n\n# e.g. \"us-east-1\"\nexport REGION=\"&lt;your cluster region&gt;\" \n</code></pre>"},{"location":"samples/ruby/ruby-pg/index.html#run-the-example_1","title":"Run the example","text":"<p>Execute the following command:</p> <pre><code>ruby hello_dsql.rb\n</code></pre> <p>Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved. </p> <p>SPDX-License-Identifier: MIT-0</p>"},{"location":"samples/rust/cluster_management/index.html","title":"Aurora DSQL Rust code examples","text":""},{"location":"samples/rust/cluster_management/index.html#overview","title":"Overview","text":"<p>The code examples in this directory show you how to use the AWS Rust SDK with DSQL to create, update, get, and delete single- and multi-Region clusters.</p> <p>Each file in the examples directory demonstrates a minimum working example for each operation and produces an independent binary that can be executed.</p>"},{"location":"samples/rust/cluster_management/index.html#run-the-examples","title":"Run the examples","text":""},{"location":"samples/rust/cluster_management/index.html#important","title":"\u26a0\ufe0f Important","text":"<ul> <li>Running this code might result in charges to your AWS account.</li> <li>We recommend that you grant your code least privilege. At most, grant only the minimum permissions required to perform   the task. For more information,   see Grant least privilege.</li> <li>This code is not tested in every AWS Region. For more information,   see AWS Regional Services.</li> </ul>"},{"location":"samples/rust/cluster_management/index.html#prerequisites","title":"Prerequisites","text":"<ul> <li>You must have an AWS account, and have your default credentials and AWS Region configured as described in   the Globally configuring AWS SDKs and tools</li> <li>You must have Rust &amp; Cargo installed.</li> </ul>"},{"location":"samples/rust/cluster_management/index.html#configure-the-environment","title":"Configure the environment","text":"<p>Set environment variables with your cluster details as required.</p>"},{"location":"samples/rust/cluster_management/index.html#single-region-clusters","title":"Single-region clusters","text":"<pre><code># Only relevant for delete/get/update examples.\n# e.g. \"foo0bar1baz2quux3quuux4\"\nexport CLUSTER_ID=\"&lt;your id&gt;\"\n\n# Relevant for all examples.\n# e.g. \"us-east-1\"\nexport CLUSTER_REGION=\"&lt;your region&gt;\"\n</code></pre>"},{"location":"samples/rust/cluster_management/index.html#multi-region-clusters","title":"Multi-region clusters","text":"<pre><code># Only relevant for delete/get/update examples.\n# e.g. \"foo0bar1baz2quux3quuux4\"\nexport CLUSTER_1_ID=\"&lt;your id 1&gt;\"\nexport CLUSTER_2_ID=\"&lt;your id 2&gt;\"\n\n# Relevant for all examples.\n# e.g. \"us-east-1\"\nexport CLUSTER_1_REGION=\"&lt;your region 1&gt;\"\nexport CLUSTER_2_REGION=\"&lt;your region 2&gt;\"\n\n# Only relevant for create examples.\nexport WITNESS_REGION=\"&lt;your region 3&gt;\"\n</code></pre>"},{"location":"samples/rust/cluster_management/index.html#run-the-examples_1","title":"Run the examples","text":"<p>Each example is compiled as a separate binary. You can run them individually:</p> <pre><code># Build all examples\ncargo build --release\n\n# Run a specific example\n./target/release/create_single_region_cluster\n./target/release/get_cluster\n./target/release/update_cluster\n./target/release/delete_single_region_cluster\n./target/release/create_multi_region_clusters\n./target/release/delete_multi_region_clusters\n</code></pre> <p>Alternatively, you can use cargo to run a specific example:</p> <pre><code>cargo run --bin create_single_region_cluster\ncargo run --bin get_cluster\n# etc.\n</code></pre>"},{"location":"samples/rust/cluster_management/index.html#run-the-tests","title":"Run the tests","text":"<p>The project includes unit tests that exercise the full lifecycle of both single-region and multi-region clusters:</p> <p>Since the tests can take up to 5 minutes to run, the <code>--nocapture</code> flag can be used to show more granular progress.</p> <pre><code># Run all tests\ncargo test -- --nocapture\n\n# Run a specific test\ncargo test --test single_region_test -- --nocapture\ncargo test --test multi_region_test -- --nocapture\n</code></pre> <p>Example execution will take around five minutes as it waits for clusters to complete activation and deletion.</p> <p>Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.</p> <p>SPDX-License-Identifier: MIT-0</p>"},{"location":"samples/rust/sqlx/index.html","title":"Aurora DSQL with Rust","text":""},{"location":"samples/rust/sqlx/index.html#overview","title":"Overview","text":"<p>This code example demonstrates how to use Rust SQLx with Amazon Aurora DSQL. The example shows you how to connect to an Aurora DSQL cluster and perform basic database operations.</p> <p>Aurora DSQL is a distributed SQL database service that provides high availability and scalability for your PostgreSQL-compatible applications. SQLx is a popular SQL toolkit for Rust that allows you to interact with PostgreSQL databases using Rust code.</p>"},{"location":"samples/rust/sqlx/index.html#about-the-code-example","title":"About the code example","text":"<p>The example demonstrates a flexible connection approach that works for both admin and non-admin users:</p> <ul> <li>When connecting as an admin user, the example uses the <code>public</code> schema and generates an admin authentication   token.</li> <li>When connecting as a non-admin user, the example uses a custom <code>myschema</code> schema and generates a standard   authentication token.</li> </ul> <p>The code automatically detects the user type and adjusts its behavior accordingly.</p>"},{"location":"samples/rust/sqlx/index.html#important","title":"\u26a0\ufe0f Important","text":"<ul> <li>Running this code might result in charges to your AWS account.</li> <li>We recommend that you grant your code least privilege. At most, grant only the   minimum permissions required to perform the task. For more information, see   Grant least privilege.</li> <li>This code is not tested in every AWS Region. For more information, see   AWS Regional Services.</li> </ul>"},{"location":"samples/rust/sqlx/index.html#run-the-example","title":"Run the example","text":""},{"location":"samples/rust/sqlx/index.html#prerequisites","title":"Prerequisites","text":"<ul> <li>You must have an AWS account, and have your default credentials and AWS Region   configured as described in the   Globally configuring AWS SDKs and tools   guide.</li> <li>Rust &amp; Cargo.</li> <li>You must have an Aurora DSQL cluster. For information about creating an Aurora DSQL cluster, see the   Getting started with Aurora DSQL   guide.</li> <li>If connecting as a non-admin user, ensure the user is linked to an IAM role and is granted access to the <code>myschema</code>   schema. See the   Using database roles with IAM roles   guide.</li> </ul>"},{"location":"samples/rust/sqlx/index.html#run-the-code","title":"Run the code","text":"<p>The example demonstrates the following operations:</p> <ul> <li>Opening a pooled connection to an Aurora DSQL cluster with periodic credential refresh</li> <li>Creating a table</li> <li>Inserting and querying data</li> </ul> <p>The example is designed to work with both admin and non-admin users:</p> <ul> <li>When run as an admin user, it uses the <code>public</code> schema</li> <li>When run as a non-admin user, it uses the <code>myschema</code> schema</li> </ul> <p>Note: running the example will use actual resources in your AWS account and may incur charges.</p> <p>Set environment variables for your cluster details:</p> <pre><code># e.g. \"admin\"\nexport CLUSTER_USER=\"&lt;your user&gt;\"\n\n# e.g. \"foo0bar1baz2quux3quuux4.dsql.us-east-1.on.aws\"\nexport CLUSTER_ENDPOINT=\"&lt;your endpoint&gt;\"\n\n# e.g. \"us-east-1\"\nexport REGION=\"&lt;your region&gt;\"\n</code></pre> <p>Run the example:</p> <pre><code>cargo run\n</code></pre> <p>The example contains comments explaining the code and the operations being performed.</p>"},{"location":"samples/rust/sqlx/index.html#additional-resources","title":"Additional resources","text":"<ul> <li>Amazon Aurora DSQL Documentation</li> <li>SQLx Documentation</li> </ul> <p>Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.</p> <p>SPDX-License-Identifier: MIT-0</p>"},{"location":"samples/typescript/prisma/index.html","title":"Aurora DSQL with Prisma","text":""},{"location":"samples/typescript/prisma/index.html#overview","title":"Overview","text":"<p>This package provides tools for using Prisma ORM with Amazon Aurora DSQL:</p> <ol> <li>Schema Validator - Validates Prisma schemas for DSQL compatibility</li> <li>Migration Transformer - Converts Prisma migrations to DSQL-compatible SQL</li> <li>DSQL Prisma Client - Prisma client with automatic IAM authentication</li> </ol> <p>Aurora DSQL is a distributed SQL database service that provides high availability and scalability for your PostgreSQL-compatible applications. Prisma is a modern database toolkit that provides type-safe database access, automated migrations, and an intuitive data model for TypeScript and JavaScript applications.</p>"},{"location":"samples/typescript/prisma/index.html#quick-start","title":"Quick Start","text":"<p>The fastest way to generate a DSQL-compatible migration:</p> <pre><code># One command: validates schema, generates migration, transforms for DSQL\nnpm run dsql-migrate prisma/schema.prisma -o prisma/migrations/001_init/migration.sql\n</code></pre> <p>If validation fails, fix your schema and re-run. That's it!</p> <p>For incremental migrations (after your initial deployment), see Incremental Migrations below.</p> <p>For more control, see the manual workflow below.</p>"},{"location":"samples/typescript/prisma/index.html#project-structure","title":"Project Structure","text":"<ul> <li><code>src/</code> - Sample code that you can copy into your own project</li> <li><code>helpers/</code> - Optional tooling (schema validator, migration transformer) - useful during development but not required in your final project</li> </ul>"},{"location":"samples/typescript/prisma/index.html#recommended-workflow","title":"Recommended Workflow","text":"<p>For more control over each step, you can run the tools separately:</p> <ol> <li> <p>Write your Prisma schema - Define your models in <code>prisma/schema.prisma</code></p> </li> <li> <p>Validate for DSQL compatibility - Run the validator to catch issues early:</p> <pre><code>npm run validate prisma/schema.prisma\n</code></pre> </li> <li> <p>Generate Prisma client - Generate the type-safe client:</p> <pre><code>npx prisma generate\n</code></pre> </li> <li> <p>Generate DSQL-compatible migrations - Use Prisma's diff tool with the transformer:</p> <pre><code># Generate and transform in one step\nnpx prisma migrate diff \\\n    --from-empty \\\n    --to-schema-datamodel prisma/schema.prisma \\\n    --script | npm run dsql-transform &gt; prisma/migrations/001_init/migration.sql\n</code></pre> </li> <li> <p>Apply migrations - Deploy your schema:</p> <pre><code>npm run prisma:migrate-up\n</code></pre> </li> <li> <p>Use the DSQL Prisma Client - Connect and query with automatic IAM auth</p> </li> </ol>"},{"location":"samples/typescript/prisma/index.html#schema-validator","title":"Schema Validator","text":"<p>Validate your Prisma schema for DSQL compatibility before runtime:</p> <pre><code>npm run validate prisma/schema.prisma\n</code></pre>"},{"location":"samples/typescript/prisma/index.html#what-the-validator-checks","title":"What the Validator Checks","text":"<p>Aurora DSQL has specific PostgreSQL compatibility limitations. The validator catches Prisma schema patterns that will fail at runtime:</p> Check Type DSQL Limitation Missing <code>relationMode = \"prisma\"</code> Error Foreign keys not supported <code>autoincrement()</code> Error Sequences not supported <code>@db.Serial</code> Error Sequences not supported <code>@db.SmallSerial</code> Error Sequences not supported <code>@db.BigSerial</code> Error Sequences not supported <code>@@fulltext</code> Error Full-text indexes not supported <code>Int @id</code> without autoincrement Warning Manual ID management needed <code>BigInt @id</code> Warning Typically requires sequences <code>gen_random_uuid()</code> without <code>@db.Uuid</code> Warning Should use proper UUID type <p>Note: This table reflects DSQL limitations as of December 2025. Check the linked docs for the latest.</p>"},{"location":"samples/typescript/prisma/index.html#example-output","title":"Example Output","text":"<pre><code>\u2717 autoincrement() is not supported in DSQL (line 12)\n  \u2192 Use @default(dbgenerated(\"gen_random_uuid()\")) @db.Uuid instead\n\n\u2717 Missing relationMode = \"prisma\" in datasource block (line 3)\n  \u2192 Add relationMode = \"prisma\" to your datasource block. DSQL does not support foreign key constraints.\n\n\u2717 Validation failed: 2 error(s), 0 warning(s)\n</code></pre>"},{"location":"samples/typescript/prisma/index.html#migration-transformer","title":"Migration Transformer","text":"<p>Transform Prisma-generated migrations to be DSQL-compatible:</p> <pre><code># Transform from file\nnpm run dsql-transform raw.sql -o migration.sql\n\n# Transform using pipes (recommended)\nnpx prisma migrate diff \\\n    --from-empty \\\n    --to-schema-datamodel prisma/schema.prisma \\\n    --script | npm run dsql-transform &gt; migration.sql\n</code></pre>"},{"location":"samples/typescript/prisma/index.html#what-the-transformer-does","title":"What the Transformer Does","text":"<p>The transformer automatically applies DSQL-required changes to Prisma's migration output:</p> Transformation Reason Wraps each statement in <code>BEGIN/COMMIT</code> DSQL requires one DDL statement per transaction Converts <code>CREATE INDEX</code> to <code>CREATE INDEX ASYNC</code> DSQL requires asynchronous index creation Removes foreign key constraints DSQL requires application-layer referential integrity (see warning) <p>Note: When foreign keys are removed, you'll see a warning reminding you to use <code>relationMode = \"prisma\"</code> in your schema. This tells Prisma to handle referential integrity in your application code rather than the database. See Prisma's relation mode documentation for details.</p>"},{"location":"samples/typescript/prisma/index.html#example","title":"Example","text":"<p>Input (Prisma output):</p> <pre><code>CREATE TABLE \"user\" (\n    \"id\" UUID NOT NULL DEFAULT gen_random_uuid(),\n    \"name\" VARCHAR(100),\n    PRIMARY KEY (\"id\")\n);\n\nCREATE INDEX \"user_name_idx\" ON \"user\"(\"name\");\n\nALTER TABLE \"post\" ADD CONSTRAINT \"post_authorId_fkey\"\n    FOREIGN KEY (\"authorId\") REFERENCES \"user\"(\"id\");\n</code></pre> <p>Output (DSQL-compatible):</p> <pre><code>BEGIN;\nCREATE TABLE \"user\" (\n    \"id\" UUID NOT NULL DEFAULT gen_random_uuid(),\n    \"name\" VARCHAR(100),\n    PRIMARY KEY (\"id\")\n);\nCOMMIT;\n\nBEGIN;\nCREATE INDEX ASYNC \"user_name_idx\" ON \"user\"(\"name\");\nCOMMIT;\n</code></pre> <p>Note: The foreign key constraint is automatically removed since DSQL doesn't support them.</p>"},{"location":"samples/typescript/prisma/index.html#incremental-migrations","title":"Incremental Migrations","text":"<p>After your initial deployment, when you need to make schema changes (add columns, tables, indexes), use the <code>--from-url</code> option to generate a migration that only includes the differences:</p> <pre><code>npm run dsql-migrate prisma/schema.prisma \\\n    -o prisma/migrations/002_add_email/migration.sql \\\n    --from-url \"$DATABASE_URL\"\n</code></pre> <p>This compares your updated schema against the live database and generates only the necessary changes.</p>"},{"location":"samples/typescript/prisma/index.html#migration-ordering","title":"Migration Ordering","text":"<p>Migrations must be applied in order. Use numbered prefixes to ensure correct ordering:</p> <pre><code>prisma/migrations/\n\u251c\u2500\u2500 001_init/\n\u2502   \u2514\u2500\u2500 migration.sql\n\u251c\u2500\u2500 002_add_email/\n\u2502   \u2514\u2500\u2500 migration.sql\n\u2514\u2500\u2500 003_add_index/\n    \u2514\u2500\u2500 migration.sql\n</code></pre> <p>If no changes are detected, the command will exit with a success message:</p> <pre><code>\u2713 No changes detected - schema is up to date\n</code></pre>"},{"location":"samples/typescript/prisma/index.html#handling-unsupported-statements","title":"Handling Unsupported Statements","text":"<p>Sometimes Prisma generates <code>DROP CONSTRAINT</code> statements when comparing against a live database (e.g., to recreate primary keys). DSQL doesn't support <code>DROP CONSTRAINT</code>, so the tool will fail by default:</p> <pre><code>\u2717 Migration contains unsupported DSQL statements:\n\n  ALTER TABLE \"vet\" DROP CONSTRAINT \"vet_pkey\"\n\nDSQL doesn't support ALTER TABLE DROP CONSTRAINT.\n</code></pre> <p>If the primary key isn't actually changing (Prisma is just being cautious), use <code>--force</code> to skip these statements:</p> <pre><code>npm run dsql-migrate prisma/schema.prisma \\\n    -o prisma/migrations/002_add_email/migration.sql \\\n    --from-url \"$DATABASE_URL\" \\\n    --force\n</code></pre> <p>Warning: Only use <code>--force</code> if you're certain the constraint changes are safe to skip. If you're actually changing a primary key, you'll need to recreate the table instead.</p>"},{"location":"samples/typescript/prisma/index.html#about-the-example","title":"About the Example","text":"<p>The example uses the Aurora DSQL Connector for automatic IAM authentication and connection pooling. It demonstrates:</p> <ul> <li>Opening a connection to an Aurora DSQL cluster using Prisma</li> <li>Inserting and querying data using Prisma's type-safe client</li> <li>Managing relationships between entities (owners, pets, veterinarians, and specialties)</li> </ul> <p>The example is designed to work with both admin and non-admin users:</p> <ul> <li>When run as an admin user, it uses the <code>public</code> schema</li> <li>When run as a non-admin user, it uses the <code>myschema</code> schema</li> </ul> <p>The code automatically detects the user type and adjusts its behavior accordingly.</p>"},{"location":"samples/typescript/prisma/index.html#usage","title":"Usage","text":"<pre><code>import { DsqlPrismaClient } from \"./dsql-client\";\n\nconst client = new DsqlPrismaClient();\n\n// Use Prisma as normal\nconst users = await client.user.findMany();\n\n// Clean up\nawait client.dispose();\n</code></pre>"},{"location":"samples/typescript/prisma/index.html#important","title":"\u26a0\ufe0f Important","text":"<ul> <li>Running this code might result in charges to your AWS account.</li> <li>We recommend that you grant your code least privilege. At most, grant only the   minimum permissions required to perform the task. For more information, see   Grant least privilege.</li> <li>This code is not tested in every AWS Region. For more information, see   AWS Regional Services.</li> </ul>"},{"location":"samples/typescript/prisma/index.html#run-the-example","title":"Run the Example","text":""},{"location":"samples/typescript/prisma/index.html#prerequisites","title":"Prerequisites","text":"<ul> <li>You must have an AWS account, and have your default credentials and AWS Region   configured as described in the   Globally configuring AWS SDKs and tools   guide.</li> <li>Node 20.0.0 or later.</li> <li>You must have an Aurora DSQL cluster. For information about creating an Aurora DSQL cluster, see the   Getting started with Aurora DSQL   guide.</li> <li>If connecting as a non-admin user, ensure the user is linked to an IAM role and is granted access to the <code>myschema</code>   schema. See the   Using database roles with IAM roles   guide.</li> </ul>"},{"location":"samples/typescript/prisma/index.html#install-dependencies","title":"Install dependencies","text":"<p>Install all required packages for the Prisma example:</p> <pre><code>npm install\n</code></pre>"},{"location":"samples/typescript/prisma/index.html#set-environment-variables","title":"Set environment variables","text":"<p>Set environment variables for your cluster details:</p> <pre><code># e.g. \"admin\"\nexport CLUSTER_USER=\"&lt;your user&gt;\"\n\n# e.g. \"foo0bar1baz2quux3quuux4.dsql.us-east-1.on.aws\"\nexport CLUSTER_ENDPOINT=\"&lt;your endpoint&gt;\"\n</code></pre>"},{"location":"samples/typescript/prisma/index.html#database-migrations","title":"Database migrations","text":"<p>Before running the example, you need to apply database migrations to create the required tables. Prisma's migration tool requires a <code>DATABASE_URL</code> environment variable with authentication credentials.</p> <p>Generate an authentication token following the instructions in the Aurora DSQL authentication token guide and set it as the <code>CLUSTER_PASSWORD</code> environment variable, then set up the database URL:</p> <pre><code># Set schema based on user type.\nif [ \"$CLUSTER_USER\" = \"admin\" ]; then\n  export SCHEMA=\"public\"\nelse\n  export SCHEMA=\"myschema\"\nfi\n\n# URL-encode password for consumption by Prisma.\nexport ENCODED_PASSWORD=$(python -c \"from urllib.parse import quote; print(quote('$CLUSTER_PASSWORD', safe=''))\")\n\n# Set up DATABASE_URL for Prisma migrations.\nexport DATABASE_URL=\"postgresql://$CLUSTER_USER:$ENCODED_PASSWORD@$CLUSTER_ENDPOINT:5432/postgres?sslmode=verify-full&amp;schema=$SCHEMA\"\n</code></pre> <p>Apply the database migrations:</p> <pre><code># Create the database schema\nnpm run prisma:migrate-up\n</code></pre> <p>To remove the database schema when you're done:</p> <pre><code># Clean up the database schema\nnpm run prisma:migrate-down\n</code></pre>"},{"location":"samples/typescript/prisma/index.html#run-the-example_1","title":"Run the example","text":"<p>Note: running the example will use actual resources in your AWS account and may incur charges.</p> <pre><code>npm run sample\n</code></pre>"},{"location":"samples/typescript/prisma/index.html#run-tests","title":"Run tests","text":"<p>The example includes integration tests that verify the Prisma client functionality with DSQL.</p> <p>Note: running the tests will use actual resources in your AWS account and may incur charges.</p> <pre><code>npm test\n</code></pre>"},{"location":"samples/typescript/prisma/index.html#prisma-considerations-with-aurora-dsql","title":"Prisma Considerations with Aurora DSQL","text":"<p>When using Prisma with Aurora DSQL, be aware of the following considerations and limitations. For full details on DSQL limitations, see Unsupported PostgreSQL features in Aurora DSQL.</p>"},{"location":"samples/typescript/prisma/index.html#configuration-requirements","title":"Configuration Requirements","text":"<ul> <li>Relation mode: Set <code>relationMode = \"prisma\"</code> to handle referential integrity at the application level (DSQL does not support foreign keys).</li> <li>Model IDs: Use <code>gen_random_uuid()</code> to create DSQL-compatible automatic unique IDs (DSQL does not support sequences).</li> </ul>"},{"location":"samples/typescript/prisma/index.html#migration-requirements","title":"Migration Requirements","text":"<ul> <li>Advisory Locks: Disable Prisma's default advisory locks behavior by setting   <code>PRISMA_SCHEMA_DISABLE_ADVISORY_LOCK=1</code>.</li> <li>Use the transformer: Run <code>npm run dsql-transform</code> on Prisma's migration output to automatically apply DSQL-required changes (transaction wrapping, async indexes, foreign key removal).</li> </ul>"},{"location":"samples/typescript/prisma/index.html#additional-resources","title":"Additional Resources","text":"<ul> <li>Amazon Aurora DSQL Documentation</li> <li>Unsupported PostgreSQL Features in DSQL</li> <li>Aurora DSQL Node.js Connector</li> <li>Prisma Documentation</li> <li>Prisma Driver Adapters</li> </ul> <p>Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.</p> <p>SPDX-License-Identifier: MIT-0</p>"},{"location":"samples/typescript/sequelize/index.html","title":"Aurora DSQL with Sequelize","text":""},{"location":"samples/typescript/sequelize/index.html#overview","title":"Overview","text":"<p>This code example demonstrates how to use Sequelize with Amazon Aurora DSQL. The example shows you how to connect to an Aurora DSQL cluster with Sequelize using node-postgres, create entities, and read and write to those entity tables.</p> <p>Aurora DSQL is a distributed SQL database service that provides high availability and scalability for your PostgreSQL-compatible applications. Sequelize is a popular object-relational mapping framework for TypeScript that allows you to persist TypeScript objects to a database while abstracting the database interactions.</p>"},{"location":"samples/typescript/sequelize/index.html#about-the-code-example","title":"About the code example","text":"<p>The example demonstrates a flexible connection approach that works for both admin and non-admin users:</p> <ul> <li>When connecting as an admin user, the example uses the <code>public</code> schema and generates an admin authentication   token.</li> <li>When connecting as a non-admin user, the example uses a custom <code>myschema</code> schema and generates a standard   authentication token.</li> </ul> <p>The code automatically detects the user type and adjusts its behavior accordingly.</p>"},{"location":"samples/typescript/sequelize/index.html#important","title":"\u26a0\ufe0f Important","text":"<ul> <li>Running this code might result in charges to your AWS account.</li> <li>We recommend that you grant your code least privilege. At most, grant only the   minimum permissions required to perform the task. For more information, see   Grant least privilege.</li> <li>This code is not tested in every AWS Region. For more information, see   AWS Regional Services.</li> </ul>"},{"location":"samples/typescript/sequelize/index.html#run-the-example","title":"Run the example","text":""},{"location":"samples/typescript/sequelize/index.html#prerequisites","title":"Prerequisites","text":"<ul> <li>You must have an AWS account, and have your default credentials and AWS Region   configured as described in the   Globally configuring AWS SDKs and tools   guide.</li> <li>TypeScript: Ensure you have TypeScript 5.6+ installed</li> </ul> <p><pre><code>npx tsc --version\n</code></pre> It should output something similar to <code>Version 5.6.x</code> or higher.</p> <ul> <li>You must have an Aurora DSQL cluster. For information about creating an Aurora DSQL cluster, see the   Getting started with Aurora DSQL   guide.</li> <li>If connecting as a non-admin user, ensure the user is linked to an IAM role and is granted access to the <code>myschema</code>   schema. See the   Using database roles with IAM roles   guide.</li> </ul>"},{"location":"samples/typescript/sequelize/index.html#run-the-code","title":"Run the code","text":"<p>The example demonstrates the following operations:</p> <ul> <li>Opening a connection pool to an Aurora DSQL cluster using Sequelize</li> <li>Creating several Sequelize models</li> <li>Creating and querying objects that are persisted in DSQL</li> </ul> <p>The example is designed to work with both admin and non-admin users:</p> <ul> <li>When run as an admin user, it uses the <code>public</code> schema</li> <li>When run as a non-admin user, it uses the <code>myschema</code> schema</li> </ul> <p>Note: running the example will use actual resources in your AWS account and may incur charges.</p> <p>Set environment variables for your cluster details:</p> <pre><code># e.g. \"admin\"\nexport CLUSTER_USER=\"&lt;your user&gt;\"\n\n# e.g. \"foo0bar1baz2quux3quuux4.dsql.us-east-1.on.aws\"\nexport CLUSTER_ENDPOINT=\"&lt;your endpoint&gt;\"\n</code></pre> <p>Run the example:</p> <pre><code>npm install\nnpm run build\nnpm run start\n</code></pre> <p>The example contains comments explaining the code and the operations being performed.</p>"},{"location":"samples/typescript/sequelize/index.html#usage-notes","title":"Usage notes","text":""},{"location":"samples/typescript/sequelize/index.html#connecting-to-dsql","title":"Connecting to DSQL","text":"<p>DSQL is PostgreSQL-compatible, so use the <code>postgres</code> dialect. The Aurora DSQL Connector for node-postgres handles IAM token generation automatically. Inject the connector into Sequelize via the <code>dialectModule</code> option.</p> <pre><code>import { AuroraDSQLClient } from '@aws/aurora-dsql-node-postgres-connector';\nimport * as pg from 'pg';\n\nconst sequelize = new Sequelize({\n  host: clusterEndpoint,\n  username: user,\n  database: 'postgres',\n  dialect: 'postgres',\n  dialectModule: { ...pg, Client: AuroraDSQLClient },\n  // ...\n});\n</code></pre> <p>For non-admin users, set the search path to their granted schema using an <code>afterConnect</code> hook. Non-admin users cannot be granted access to the <code>public</code> schema.</p> <pre><code>hooks: {\n  afterConnect: async (connection) =&gt; {\n    await connection.query('SET search_path TO myschema');\n  }\n}\n</code></pre>"},{"location":"samples/typescript/sequelize/index.html#connection-configuration","title":"Connection configuration","text":"<p>Sequelize sets <code>client_min_messages</code> by default, which causes the error <code>setting configuration parameter \"client_min_messages\" not supported</code>. Disable this by setting <code>clientMinMessages: 'ignore'</code> in dialect options.</p> <pre><code>new Sequelize({\n  // ...\n  dialect: 'postgres',\n  dialectOptions: {\n    clientMinMessages: 'ignore',\n  },\n});\n</code></pre>"},{"location":"samples/typescript/sequelize/index.html#connection-pooling","title":"Connection pooling","text":"<p>Connection pooling can be configured in the Sequelize constructor. The DSQL connector generates a new authentication token for each connection. DSQL connections close after one hour; the pool automatically opens new connections as needed.</p> <pre><code>new Sequelize({\n  // ...\n  pool: {\n    max: 5,\n    min: 0,\n    acquire: 30000,\n    idle: 10000\n  },\n});\n</code></pre>"},{"location":"samples/typescript/sequelize/index.html#table-creation","title":"Table creation","text":"<p><code>Sequelize.sync()</code> and <code>Model.sync()</code> are not supported because DSQL returns index metadata in a format that Sequelize v6 cannot parse (the <code>INCLUDE</code> clause in index definitions causes parsing failures). Use <code>QueryInterface.createTable()</code> to create tables, then initialize models in memory with <code>Model.init()</code>.</p> <pre><code>// Instead of: await Model.sync();\nconst queryInterface = sequelize.getQueryInterface();\nawait queryInterface.createTable('owner', {\n  id: { type: DataTypes.UUID, primaryKey: true, defaultValue: DataTypes.UUIDV4 },\n  name: { type: DataTypes.STRING(30), allowNull: false },\n});\n\n// Then initialize the model in memory\nOwner.init({ /* same attributes */ }, { sequelize, tableName: 'owner' });\n</code></pre>"},{"location":"samples/typescript/sequelize/index.html#primary-keys","title":"Primary keys","text":"<p>SERIAL and identity columns are not supported. Using <code>autoIncrement: true</code> results in a <code>type \"serial\" does not exist</code> error. Use UUID primary keys instead.</p> <pre><code>id: { type: DataTypes.UUID, primaryKey: true, defaultValue: DataTypes.UUIDV4 }\n</code></pre>"},{"location":"samples/typescript/sequelize/index.html#relationships","title":"Relationships","text":"<p>When defining relationships, set <code>constraints: false</code>. Sequelize attempts to create foreign key constraints by default, which are not supported. ORM-level relationships are retained but are not enforced by the database.</p> <pre><code>Pet.belongsTo(Owner, { foreignKey: 'ownerId', constraints: false });\nOwner.hasMany(Pet, { foreignKey: 'ownerId', constraints: false });\n</code></pre>"},{"location":"samples/typescript/sequelize/index.html#unsupported-data-types","title":"Unsupported data types","text":"<p>JSON/JSONB: <code>DataTypes.JSON</code> and <code>DataTypes.JSONB</code> are not supported. Use <code>DataTypes.TEXT</code> in your table definition, with getter/setter in <code>Model.init()</code> for serialization. JSON query operators are not available with this approach.</p> <pre><code>metadata: {\n  type: DataTypes.TEXT,\n  get() {\n    const val = this.getDataValue('metadata');\n    return val ? JSON.parse(val) : null;\n  },\n  set(val: object) {\n    this.setDataValue('metadata', JSON.stringify(val));\n  }\n}\n</code></pre> <p>ENUM: <code>DataTypes.ENUM</code> is not supported. Use <code>DataTypes.STRING</code> in your table definition, with validation in <code>Model.init()</code>. Validation is enforced at the application level, not the database.</p> <pre><code>status: {\n  type: DataTypes.STRING,\n  validate: { isIn: [['pending', 'active', 'completed']] }\n}\n</code></pre>"},{"location":"samples/typescript/sequelize/index.html#unsupported-methods","title":"Unsupported methods","text":"<p>findOrCreate: <code>Model.findOrCreate()</code> internally uses PL/pgSQL, which is not supported. Use <code>upsert</code> or manual <code>findOne</code> + <code>create</code>.</p> <pre><code>// Instead of: await User.findOrCreate({ where: { email }, defaults: { name } });\n\n// Option 1: upsert (overwrites name if record exists)\nconst [user] = await User.upsert({ email, name });\n\n// Option 2: manual approach (preserves existing name)\nlet user = await User.findOne({ where: { email } });\nif (!user) user = await User.create({ email, name });\n</code></pre> <p>truncate: <code>Model.truncate()</code> is not supported. Use <code>destroy</code> with empty where clause.</p> <pre><code>// Instead of: await Model.truncate();\nawait Model.destroy({ where: {} });\n</code></pre>"},{"location":"samples/typescript/sequelize/index.html#locking","title":"Locking","text":"<p>Aurora DSQL uses optimistic concurrency control (OCC), meaning transactions proceed without locks and conflicts are detected at commit time. The <code>SELECT FOR UPDATE</code> clause modifies this behavior by flagging read rows for concurrency checks, which is useful for managing write skew scenarios.</p> <p>In Sequelize, only <code>Transaction.LOCK.UPDATE</code> is supported. The query must include an equality predicate on the primary key. Queries that lock by non-key columns will fail.</p> <pre><code>// Works: lock by primary key\nawait Model.findByPk(id, { lock: Transaction.LOCK.UPDATE, transaction });\n\n// Does not work: lock by non-key column\nawait Model.findOne({ where: { status: 'pending' }, lock: Transaction.LOCK.UPDATE, transaction });\n</code></pre> <p>For more details on concurrency control in Aurora DSQL, see Concurrency control in Amazon Aurora DSQL.</p>"},{"location":"samples/typescript/sequelize/index.html#additional-resources","title":"Additional resources","text":"<ul> <li>Amazon Aurora DSQL Documentation</li> <li>Aurora DSQL Connector for node-postgres</li> <li>Sequelize Documentation</li> <li>AWS SDK for JavaScript Documentation</li> </ul> <p>Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved. </p> <p>SPDX-License-Identifier: MIT-0</p>"},{"location":"samples/typescript/type-orm/index.html","title":"Aurora DSQL with TypeORM","text":""},{"location":"samples/typescript/type-orm/index.html#overview","title":"Overview","text":"<p>This code example demonstrates how to use <code>TypeORM</code> with Amazon Aurora DSQL. The example shows you how to connect to an Aurora DSQL cluster and perform basic database operations.</p> <p>Aurora DSQL is a distributed SQL database service that provides high availability and scalability for your PostgreSQL-compatible applications. <code>TypeORM</code> is a popular ORM for TypeScript and JavaScript that allows you to interact with PostgreSQL databases using object-relational mapping patterns.</p>"},{"location":"samples/typescript/type-orm/index.html#about-the-code-example","title":"About the code example","text":"<p>The example demonstrates a flexible connection approach that works for both admin and non-admin users:</p> <ul> <li>When connecting as an admin user, the example uses the <code>public</code> schema and generates an admin authentication   token.</li> <li>When connecting as a non-admin user, the example uses a custom <code>myschema</code> schema and generates a standard   authentication token.</li> </ul> <p>The code automatically detects the user type and adjusts its behavior accordingly.</p>"},{"location":"samples/typescript/type-orm/index.html#important","title":"\u26a0\ufe0f Important","text":"<ul> <li>Running this code might result in charges to your AWS account.</li> <li>We recommend that you grant your code least privilege. At most, grant only the   minimum permissions required to perform the task. For more information, see   Grant least privilege.</li> <li>This code is not tested in every AWS Region. For more information, see   AWS Regional Services.</li> </ul>"},{"location":"samples/typescript/type-orm/index.html#run-the-example","title":"Run the example","text":""},{"location":"samples/typescript/type-orm/index.html#prerequisites","title":"Prerequisites","text":"<ul> <li>You must have an AWS account, and have your default credentials and AWS Region   configured as described in the   Globally configuring AWS SDKs and tools   guide.</li> <li>Node 18.0.0 or later.</li> <li>You must have an Aurora DSQL cluster. For information about creating an Aurora DSQL cluster, see the   Getting started with Aurora DSQL   guide.</li> <li>If connecting as a non-admin user, ensure the user is linked to an IAM role and is granted access to the <code>myschema</code>   schema. See the   Using database roles with IAM roles   guide.</li> </ul>"},{"location":"samples/typescript/type-orm/index.html#set-up-environment-for-examples","title":"Set up environment for examples","text":"<pre><code>npm install\n</code></pre>"},{"location":"samples/typescript/type-orm/index.html#run-the-code","title":"Run the code","text":"<p>The example demonstrates the following operations:</p> <ul> <li>Opening a connection to an Aurora DSQL cluster</li> <li>Creating a table</li> <li>Inserting and querying data</li> </ul> <p>The example is designed to work with both admin and non-admin users:</p> <ul> <li>When run as an admin user, it uses the <code>public</code> schema</li> <li>When run as a non-admin user, it uses the <code>myschema</code> schema</li> </ul> <p>Note: running the example will use actual resources in your AWS account and may incur charges.</p> <p>Set environment variables for your cluster details:</p> <pre><code># e.g. \"admin\"\nexport CLUSTER_USER=\"&lt;your user&gt;\"\n\n# e.g. \"foo0bar1baz2quux3quuux4.dsql.us-east-1.on.aws\"\nexport CLUSTER_ENDPOINT=\"&lt;your endpoint&gt;\"\n\n# e.g. \"us-east-1\"\nexport REGION=\"&lt;your region&gt;\"\n</code></pre> <p>Run the example:</p> <pre><code># first time running the application\nnpm run build\nnpm run migrations-create-table\nnpm run migrations-run\n\nnpm test\n</code></pre> <p>The example contains comments explaining the code and the operations being performed.</p>"},{"location":"samples/typescript/type-orm/index.html#additional-resources","title":"Additional resources","text":"<ul> <li>Amazon Aurora DSQL Documentation</li> <li>TypeORM Documentation</li> <li>AWS SDK for JavaScript Documentation</li> </ul> <p>Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.</p> <p>SPDX-License-Identifier: MIT-0</p>"}]}