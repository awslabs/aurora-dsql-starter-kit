{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"index.html","title":"Welcome","text":"\ud83d\udccb Copy Page"},{"location":"index.html#aurora-dsql-documentation","title":"Aurora DSQL Documentation","text":"<p>Welcome to the Amazon Aurora DSQL documentation. Aurora DSQL is a serverless, distributed relational database optimized for transactional workloads.</p>"},{"location":"index.html#what-is-aurora-dsql","title":"What is Aurora DSQL?","text":"<p>Amazon Aurora DSQL is a serverless, distributed SQL database that provides:</p> <ul> <li>Serverless Architecture - No infrastructure to manage</li> <li>Distributed Design - Built for high availability and scalability</li> <li>PostgreSQL Compatibility - Use familiar PostgreSQL tools and syntax</li> <li>Multi-Region Support - Deploy across multiple AWS regions</li> <li>ACID Compliance - Full transactional consistency</li> </ul>"},{"location":"index.html#getting-started","title":"Getting Started","text":"<p>Ready to start using Aurora DSQL? Check out our Quickstart Guide to:</p> <ol> <li>Create your first Aurora DSQL cluster</li> <li>Connect to your cluster</li> <li>Run your first queries</li> <li>Set up multi-region clusters</li> </ol>"},{"location":"index.html#key-features","title":"Key Features","text":""},{"location":"index.html#single-region-clusters","title":"Single-Region Clusters","text":"<p>Perfect for getting started or applications that don't require multi-region deployment.</p>"},{"location":"index.html#multi-region-clusters","title":"Multi-Region Clusters","text":"<p>Deploy your database across multiple AWS regions for: - High availability - Disaster recovery - Low-latency access for global users</p>"},{"location":"index.html#query-editor","title":"Query Editor","text":"<p>Built-in query editor in the AWS Console for quick database interactions.</p>"},{"location":"index.html#iam-authentication","title":"IAM Authentication","text":"<p>Secure database access using AWS IAM roles and policies.</p>"},{"location":"index.html#documentation-structure","title":"Documentation Structure","text":"<ul> <li>Getting Started - Quick guides to get you up and running</li> <li>User Guide - Detailed documentation on features and capabilities</li> <li>API Reference - Complete API documentation</li> <li>Examples - Sample code and use cases</li> </ul>"},{"location":"index.html#need-help","title":"Need Help?","text":"<ul> <li>AWS Documentation</li> <li>Troubleshooting Guide</li> <li>AWS Support</li> </ul>"},{"location":"api-reference.html","title":"API Reference Overview","text":"\ud83d\udccb Copy Page"},{"location":"api-reference.html#amazon-aurora-dsql-api-reference","title":"Amazon Aurora DSQL API Reference","text":""},{"location":"api-reference.html#overview","title":"Overview","text":"<p>In addition to the AWS Console and the AWS Command Line Interface (CLI), Aurora DSQL also provides an API interface. You can use the API operations to manage your resources in Aurora DSQL.</p>"},{"location":"api-reference.html#api-documentation-links","title":"API Documentation Links","text":""},{"location":"api-reference.html#actions","title":"Actions","text":"<p>For an alphabetical list of API operations, see Actions.</p>"},{"location":"api-reference.html#data-types","title":"Data Types","text":"<p>For an alphabetical list of data types, see Data types.</p>"},{"location":"api-reference.html#common-parameters","title":"Common Parameters","text":"<p>For a list of common query parameters, see Common parameters.</p>"},{"location":"api-reference.html#error-codes","title":"Error Codes","text":"<p>For descriptions of the error codes, see Common errors.</p>"},{"location":"api-reference.html#cli-reference","title":"CLI Reference","text":"<p>For more information about the AWS CLI, see AWS Command Line Interface reference for Aurora DSQL.</p>"},{"location":"api-reference.html#related-documentation","title":"Related Documentation","text":"<ul> <li>Authentication Guide: Auth &amp; Access Overview</li> <li>Getting Started: Getting Started</li> <li>Troubleshooting: Troubleshooting Overview</li> </ul>"},{"location":"authentication-and-authorization.html","title":"Auth & Access Overview","text":"\ud83d\udccb Copy Page"},{"location":"authentication-and-authorization.html#authentication-and-authorization-for-amazon-aurora-dsql","title":"Authentication and Authorization for Amazon Aurora DSQL","text":""},{"location":"authentication-and-authorization.html#overview","title":"Overview","text":"<p>Aurora DSQL uses IAM roles and policies for cluster authorization. You associate IAM roles with PostgreSQL database roles for database authorization. This approach combines benefits from IAM with PostgreSQL privileges. Aurora DSQL uses these features to provide a comprehensive authorization and access policy for your cluster, database, and data.</p>"},{"location":"authentication-and-authorization.html#managing-your-cluster-using-iam","title":"Managing Your Cluster Using IAM","text":"<p>To manage your cluster, use IAM for authentication and authorization:</p>"},{"location":"authentication-and-authorization.html#iam-authentication","title":"IAM Authentication","text":"<p>To authenticate your IAM identity when you manage Aurora DSQL clusters, you must use IAM. You can provide authentication using: - AWS Management Console - AWS CLI - AWS SDK</p>"},{"location":"authentication-and-authorization.html#iam-authorization","title":"IAM Authorization","text":"<p>To manage Aurora DSQL clusters, grant authorization using IAM actions for Aurora DSQL. For example, to describe a cluster, make sure that your IAM identity has permissions for the IAM action <code>dsql:GetCluster</code>, as in the following sample policy action:</p> <pre><code>{\n  \"Effect\": \"Allow\",\n  \"Action\": \"dsql:GetCluster\",\n  \"Resource\": \"arn:aws:dsql:us-east-1:123456789012:cluster/my-cluster\"\n}\n</code></pre>"},{"location":"authentication-and-authorization.html#connecting-to-your-cluster-using-iam","title":"Connecting to Your Cluster Using IAM","text":"<p>To connect to your cluster, use IAM for authentication and authorization:</p>"},{"location":"authentication-and-authorization.html#iam-authentication_1","title":"IAM Authentication","text":"<p>Generate a temporary authentication token using an IAM identity with authorization to connect to your cluster. To learn more, see Generating an authentication token in Amazon Aurora DSQL.</p>"},{"location":"authentication-and-authorization.html#iam-authorization_1","title":"IAM Authorization","text":"<p>Grant the following IAM policy actions to the IAM identity you're using to establish the connection to your cluster's endpoint:</p>"},{"location":"authentication-and-authorization.html#admin-role-connection","title":"Admin Role Connection","text":"<p>Use <code>dsql:DbConnectAdmin</code> if you're using the <code>admin</code> role. Aurora DSQL creates and manages this role for you. The following sample IAM policy action permits <code>admin</code> to connect to <code>my-cluster</code>:</p> <pre><code>{\n  \"Effect\": \"Allow\",\n  \"Action\": \"dsql:DbConnectAdmin\",\n  \"Resource\": \"arn:aws:dsql:us-east-1:123456789012:cluster/my-cluster\"\n}\n</code></pre>"},{"location":"authentication-and-authorization.html#custom-database-role-connection","title":"Custom Database Role Connection","text":"<p>Use <code>dsql:DbConnect</code> if you're using a custom database role. You create and manage this role by using SQL commands in your database. The following sample IAM policy action permits a custom database role to connect to <code>my-cluster</code> for up to one hour:</p> <pre><code>{\n  \"Effect\": \"Allow\",\n  \"Action\": \"dsql:DbConnect\",\n  \"Resource\": \"arn:aws:dsql:us-east-1:123456789012:cluster/my-cluster\"\n}\n</code></pre> <p>Important: After you establish a connection, your role is authorized for up to one hour for the connection.</p>"},{"location":"authentication-and-authorization.html#postgresql-database-roles-and-iam-roles","title":"PostgreSQL Database Roles and IAM Roles","text":""},{"location":"authentication-and-authorization.html#database-role-management","title":"Database Role Management","text":"<p>PostgreSQL manages database access permissions using the concept of roles. A role can be thought of as either a database user, or a group of database users, depending on how the role is set up. You create PostgreSQL roles using SQL commands. To manage database-level authorization, grant PostgreSQL permissions to your PostgreSQL database roles.</p> <p>Aurora DSQL supports two types of database roles: - Admin role: Aurora DSQL automatically creates a predefined <code>admin</code> role for you in your Aurora DSQL cluster. You can't modify the <code>admin</code> role. - Custom roles: When you connect to your database as <code>admin</code>, you can issue SQL to create new database-level roles to associate with your IAM roles.</p> <p>To let IAM roles connect to your database, associate your custom database roles with your IAM roles.</p>"},{"location":"authentication-and-authorization.html#authentication-process","title":"Authentication Process","text":"<p>Use the <code>admin</code> role to connect to your cluster. After you connect your database, use the command <code>AWS IAM GRANT</code> to associate a custom database role with the IAM identity authorized to connect to the cluster:</p> <pre><code>AWS IAM GRANT custom-db-role TO 'arn:aws:iam::account-id:role/iam-role-name';\n</code></pre> <p>To learn more, see Authorizing database roles to connect to your cluster.</p>"},{"location":"authentication-and-authorization.html#authorization-process","title":"Authorization Process","text":"<p>Use the <code>admin</code> role to connect to your cluster. Run SQL commands to set up custom database roles and grant permissions. To learn more, see: - PostgreSQL database roles - PostgreSQL privileges</p>"},{"location":"authentication-and-authorization.html#using-iam-policy-actions-with-aurora-dsql","title":"Using IAM Policy Actions with Aurora DSQL","text":"<p>The IAM policy action you use depends on the role you use to connect to your cluster: either <code>admin</code> or a custom database role. The policy also depends on the IAM actions required for this role.</p>"},{"location":"authentication-and-authorization.html#using-iam-policy-actions-to-connect-to-clusters","title":"Using IAM Policy Actions to Connect to Clusters","text":""},{"location":"authentication-and-authorization.html#admin-role-connection_1","title":"Admin Role Connection","text":"<p>When you connect to your cluster with the default database role of <code>admin</code>, use an IAM identity with authorization to perform the following IAM policy action:</p> <pre><code>\"dsql:DbConnectAdmin\"\n</code></pre>"},{"location":"authentication-and-authorization.html#custom-database-role-connection_1","title":"Custom Database Role Connection","text":"<p>When you connect to your cluster with a custom database role, first associate the IAM role with the database role. The IAM identity you use to connect to your cluster must have authorization to perform the following IAM policy action:</p> <pre><code>\"dsql:DbConnect\"\n</code></pre> <p>To learn more about custom database roles, see Using database roles and IAM authentication.</p>"},{"location":"authentication-and-authorization.html#using-iam-policy-actions-to-manage-clusters","title":"Using IAM Policy Actions to Manage Clusters","text":"<p>When managing your Aurora DSQL clusters, specify policy actions only for the actions that your role needs to perform. For example, if your role only needs to get cluster information, you might limit role permissions to only the <code>GetCluster</code> and <code>ListClusters</code> permissions, as in the following sample policy:</p> <pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"dsql:GetCluster\",\n        \"dsql:ListClusters\"\n      ],\n      \"Resource\": \"arn:aws:dsql:us-east-1:123456789012:cluster/my-cluster\"\n    }\n  ]\n}\n</code></pre>"},{"location":"authentication-and-authorization.html#all-available-iam-policy-actions-for-managing-clusters","title":"All Available IAM Policy Actions for Managing Clusters","text":"<p>The following example policy shows all available IAM policy actions for managing clusters:</p> <pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"dsql:CreateCluster\",\n        \"dsql:GetCluster\",\n        \"dsql:UpdateCluster\",\n        \"dsql:DeleteCluster\",\n        \"dsql:ListClusters\",\n        \"dsql:TagResource\",\n        \"dsql:ListTagsForResource\",\n        \"dsql:UntagResource\"\n      ],\n      \"Resource\": \"*\"\n    }\n  ]\n}\n</code></pre>"},{"location":"authentication-and-authorization.html#revoking-authorization-using-iam-and-postgresql","title":"Revoking Authorization Using IAM and PostgreSQL","text":"<p>You can revoke permissions for your IAM roles to access your database-level roles:</p>"},{"location":"authentication-and-authorization.html#revoking-admin-authorization-to-connect-to-clusters","title":"Revoking Admin Authorization to Connect to Clusters","text":"<p>To revoke authorization to connect to your cluster with the <code>admin</code> role: 1. Revoke the IAM identity's access to <code>dsql:DbConnectAdmin</code> 2. Either edit the IAM policy or detach the policy from the identity</p> <p>After revoking connection authorization from the IAM identity, Aurora DSQL rejects all new connection attempts from that IAM identity. Any active connections that use the IAM identity might stay authorized for the duration of the connection. For more information on connection durations, see Quotas and limits.</p>"},{"location":"authentication-and-authorization.html#revoking-custom-role-authorization-to-connect-to-clusters","title":"Revoking Custom Role Authorization to Connect to Clusters","text":"<p>To revoke access to database roles other than <code>admin</code>: 1. Revoke the IAM identity's access to <code>dsql:DbConnect</code> 2. Either edit the IAM policy or detach the policy from the identity</p> <p>You can also remove the association between the database role and IAM by using the command <code>AWS IAM REVOKE</code> in your database. To learn more about revoking access from database roles, see Revoking database authorization from an IAM role.</p>"},{"location":"authentication-and-authorization.html#important-notes-on-permission-management","title":"Important Notes on Permission Management","text":"<ul> <li>You can't manage permissions of the predefined <code>admin</code> database role</li> <li>To learn how to manage permissions for custom database roles, see PostgreSQL privileges</li> <li>Modifications to privileges take effect on the next transaction after Aurora DSQL successfully commits the modification transaction</li> </ul>"},{"location":"backup-and-restore.html","title":"Backup and Restore","text":"\ud83d\udccb Copy Page"},{"location":"backup-and-restore.html#backup-and-restore-for-amazon-aurora-dsql","title":"Backup and restore for Amazon Aurora DSQL","text":"<p>Amazon Aurora DSQL helps you meet your regulatory compliance and business continuity requirements through integration with AWS Backup, a fully managed data protection service that makes it easy to centralize and automate backups across AWS services, in the cloud, and on premises. The service streamlines backup creation, management, and restoration for both single-Region and multi-Region Aurora DSQL clusters.</p> <p>Key features include the following:</p> <ul> <li>Centralized backup management through the AWS Management Console, SDK, or AWS CLI</li> <li>Full cluster backups</li> <li>Automated backup schedules and retention policies</li> <li>Cross-Region and cross-account capabilities</li> <li>WORM (write-once, read-many) configuration for all the backups you store</li> </ul> <p>For more information on the features of AWS Backup Vault Lock and an extensive list of available AWS Backup features for Aurora DSQL, see Vault lock benefits and AWS Backup feature availability in the AWS Backup Developer Guide.</p>"},{"location":"backup-and-restore.html#getting-started-with-aws-backup","title":"Getting started with AWS Backup","text":"<p>AWS Backup creates complete copies of your Aurora DSQL clusters. You can get started using AWS Backup for Aurora DSQL by following the steps in Getting started with AWS Backup:</p> <ol> <li>Create on-demand backups for immediate protection.</li> <li>Establish backup plans for automated, scheduled backups.</li> <li>Configure retention periods and cross-Region copying.</li> <li>Set up monitoring and notifications for backup activities.</li> </ol>"},{"location":"backup-and-restore.html#restoring-your-backups","title":"Restoring your backups","text":"<p>When you restore Aurora DSQL clusters, AWS Backup always creates new clusters to preserve your source data.</p>"},{"location":"backup-and-restore.html#restoring-single-region-clusters","title":"Restoring single-Region clusters","text":"<p>To restore an Aurora DSQL single-Region cluster, use the AWS Backup console or CLI to select the recovery point (backup) you wish to restore. Configure the settings for the new cluster that will be created from your backup. For detailed instructions, see Restore a single-Region Aurora DSQL cluster.</p>"},{"location":"backup-and-restore.html#restoring-multi-region-clusters","title":"Restoring multi-Region clusters","text":"<p>Restoring an Aurora DSQL multi-Region cluster is supported through both the AWS Backup console and the AWS CLI. For detailed instructions, see Restore a multi-Region Aurora DSQL cluster.</p> <p>To restore to a multi-Region Aurora DSQL cluster, you can use a backup taken in a single AWS Region. However, before you initiate the restore process, you must ensure there is an identical copy of your backup in all AWS Regions for your multi-Region clusters. If you don't yet have those copies, you must first copy the backup to another AWS Region that supports multi-Region clusters.</p> <p>We recommend creating backup copies in key AWS Regions to enable robust disaster recovery options and meet compliance requirements. To view available AWS Regions for Aurora DSQL, see Region Availability for Amazon Aurora DSQL.</p> <p>For detailed instructions on these steps, see Amazon Aurora DSQL restore documentation.</p>"},{"location":"backup-and-restore.html#monitoring-and-compliance","title":"Monitoring and compliance","text":"<p>AWS Backup provides comprehensive visibility into backup and restore operations with the following resources.</p> <ul> <li>A centralized dashboard for tracking backup and restore jobs</li> <li>Integration with CloudWatch and CloudTrail.</li> <li>AWS Backup Audit Manager for compliance reporting and auditing.</li> </ul> <p>See Logging Aurora DSQL operations using AWS CloudTrail to learn more about logging records of actions taken by a user, role, or an AWS service while using Aurora DSQL.</p>"},{"location":"backup-and-restore.html#additional-resources","title":"Additional resources","text":"<p>To learn more about AWS Backup features and and using it in tandem with Aurora DSQL, see the following resources:</p> <ul> <li>Managed policies for AWS Backup</li> <li>Amazon Aurora DSQL restore</li> <li>Supported services by AWS Region</li> <li>Encryption for backups in AWS Backup</li> </ul> <p>By using AWS Backup for Aurora DSQL, you implement a robust, compliant, and automated backup strategy that protects your critical database resources while minimizing administrative overhead. Whether you manage a single cluster or a complex multi-Region deployment, AWS Backup provides the tools you need to ensure your data remains secure and recoverable.</p>"},{"location":"connectors-overview.html","title":"Connectors Overview","text":"\ud83d\udccb Copy Page"},{"location":"connectors-overview.html#connectors-for-amazon-aurora-dsql","title":"Connectors for Amazon Aurora DSQL","text":""},{"location":"connectors-overview.html#overview","title":"Overview","text":"<p>Aurora DSQL provides specialized connectors that extend existing database drivers to enable seamless IAM authentication and integration with AWS services. These connectors are designed to work with popular programming languages and frameworks while maintaining compatibility with existing PostgreSQL workflows.</p>"},{"location":"connectors-overview.html#available-connectors","title":"Available Connectors","text":""},{"location":"connectors-overview.html#java-jdbc-connector","title":"Java JDBC Connector","text":"<p>Purpose: Extends PostgreSQL JDBC driver functionality for Aurora DSQL</p> <p>Key Features: - Seamless IAM authentication integration - Automatic token generation and refresh - Compatible with existing JDBC workflows - Built on top of PostgreSQL JDBC driver</p> <p>Use Cases: - Java applications using JDBC - Enterprise Java applications - Spring Boot applications - Hibernate ORM integration</p> <p>Repository: Aurora DSQL JDBC Connector</p>"},{"location":"connectors-overview.html#python-connector","title":"Python Connector","text":"<p>Purpose: Extends Python PostgreSQL drivers for Aurora DSQL</p> <p>Key Features: - Works with Psycopg, Psycopg2, and asyncpg - Automatic IAM token handling - Seamless integration with existing Python workflows - Support for both synchronous and asynchronous operations</p> <p>Use Cases: - Python applications using PostgreSQL drivers - Django web applications - SQLAlchemy ORM integration - Data science and analytics applications</p> <p>Supported Libraries: - Psycopg (latest version) - Psycopg2 (legacy support) - asyncpg (asynchronous operations)</p> <p>Repository: Aurora DSQL Python Connector</p>"},{"location":"connectors-overview.html#nodejs-connectors","title":"Node.js Connectors","text":"<p>Purpose: Extends Node.js PostgreSQL drivers for Aurora DSQL</p> <p>Key Features: - Support for node-postgres and Postgres.js - Automatic IAM authentication handling - Compatible with existing Node.js PostgreSQL workflows - TypeScript support available</p> <p>Use Cases: - Node.js web applications - Express.js applications - TypeScript applications - Serverless functions (Lambda)</p> <p>Supported Libraries: - node-postgres (pg) - Postgres.js</p> <p>Repository: Aurora DSQL Node.js Connectors</p>"},{"location":"connectors-overview.html#connector-benefits","title":"Connector Benefits","text":""},{"location":"connectors-overview.html#simplified-authentication","title":"Simplified Authentication","text":"<ul> <li>Automatic token generation: Connectors handle IAM token creation and refresh</li> <li>Seamless integration: No changes required to existing application code</li> <li>Security best practices: Built-in support for AWS security standards</li> </ul>"},{"location":"connectors-overview.html#framework-compatibility","title":"Framework Compatibility","text":"<ul> <li>Existing workflows: Maintain compatibility with current PostgreSQL applications</li> <li>ORM support: Works with popular ORMs like Hibernate, SQLAlchemy, Django</li> <li>Migration friendly: Easy transition from standard PostgreSQL to Aurora DSQL</li> </ul>"},{"location":"connectors-overview.html#aws-integration","title":"AWS Integration","text":"<ul> <li>IAM authentication: Native support for AWS IAM roles and policies</li> <li>AWS SDK integration: Leverages existing AWS SDK configurations</li> <li>Service integration: Designed for AWS service ecosystem</li> </ul>"},{"location":"connectors-overview.html#getting-started-with-connectors","title":"Getting Started with Connectors","text":""},{"location":"connectors-overview.html#installation-process","title":"Installation Process","text":"<ol> <li>Install connector package for your programming language</li> <li>Configure AWS credentials (IAM roles, access keys, or profiles)</li> <li>Update connection strings to use Aurora DSQL endpoints</li> <li>Test connectivity with your Aurora DSQL cluster</li> </ol>"},{"location":"connectors-overview.html#configuration-requirements","title":"Configuration Requirements","text":"<ul> <li>AWS credentials: Valid IAM credentials with Aurora DSQL permissions</li> <li>Cluster endpoint: Aurora DSQL cluster endpoint URL</li> <li>Database role: Admin role or custom database role</li> <li>SSL configuration: SSL/TLS encryption enabled</li> </ul>"},{"location":"connectors-overview.html#best-practices","title":"Best Practices","text":"<ul> <li>Use IAM roles when possible for enhanced security</li> <li>Configure connection pooling appropriately for your workload</li> <li>Handle token expiration gracefully in your applications</li> <li>Monitor connection health and implement retry logic</li> </ul>"},{"location":"connectors-overview.html#future-connector-releases","title":"Future Connector Releases","text":"<p>Additional connectors are planned for future releases. For the latest information on connector availability, see the Aurora DSQL samples repository.</p>"},{"location":"connectors-overview.html#planned-languages-and-frameworks","title":"Planned Languages and Frameworks","text":"<ul> <li>Additional ORM integrations</li> <li>More programming language support</li> <li>Enhanced framework-specific features</li> <li>Improved performance optimizations</li> </ul>"},{"location":"connectors-overview.html#related-documentation","title":"Related Documentation","text":"<ul> <li>Database Drivers: Programming with DSQL Overview</li> <li>Authentication: Generate Authentication Token</li> <li>Database Roles: Database Roles and IAM Authentication</li> <li>Sample Code: Aurora DSQL Samples Repository</li> <li>Getting Started: Getting Started</li> </ul>"},{"location":"considerations.html","title":"Considerations Overview","text":"\ud83d\udccb Copy Page"},{"location":"considerations.html#considerations-for-working-with-amazon-aurora-dsql","title":"Considerations for Working with Amazon Aurora DSQL","text":""},{"location":"considerations.html#overview","title":"Overview","text":"<p>Consider the following behaviors when you work with Amazon Aurora DSQL. For more information about PostgreSQL compatibility and support, see the compatibility documentation. For quotas and limits, see the quotas and limits guide.</p>"},{"location":"considerations.html#key-considerations","title":"Key Considerations","text":""},{"location":"considerations.html#count-operations-on-large-tables","title":"COUNT(*) Operations on Large Tables","text":"<p>Behavior: Aurora DSQL doesn't complete <code>COUNT(*)</code> operations before transaction timeout for large tables.</p> <p>Recommendation: To retrieve table row count from the system catalog, use the systems tables and commands available in Aurora DSQL.</p> <p>Related Documentation: Using systems tables and commands in Aurora DSQL</p>"},{"location":"considerations.html#prepared-statements-behavior","title":"Prepared Statements Behavior","text":"<p>Behavior: Drivers calling <code>PG_PREPARED_STATEMENTS</code> might provide an inconsistent view of cached prepared statements for the cluster.</p> <p>Technical Details:  - You might see more than the expected number of prepared statements per connection for the same cluster and IAM role - Aurora DSQL doesn't preserve statement names that you prepare</p> <p>Impact: This affects statement caching behavior but doesn't impact functionality</p>"},{"location":"considerations.html#multi-region-cluster-recovery","title":"Multi-Region Cluster Recovery","text":"<p>Behavior: In rare multi-Region linked-cluster impairment scenarios, it might take longer than expected for transaction commit availability to resume.</p> <p>Technical Details: - Automated cluster recovery operations can result in transient concurrency control or connection errors - In most cases, you will only see the effects for a percentage of your workload</p> <p>Recommendation: When you see these transient errors, retry your transaction or reconnect with your client.</p>"},{"location":"considerations.html#sql-client-schema-display","title":"SQL Client Schema Display","text":"<p>Behavior: Some SQL clients, such as DataGrip, make expansive calls to system metadata to populate schema information.</p> <p>Technical Details: - Aurora DSQL doesn't support all of this metadata information and returns errors - This issue doesn't affect SQL query functionality - It might affect schema display in certain clients</p> <p>Impact: Query functionality remains unaffected, only visual schema display may be impacted</p>"},{"location":"considerations.html#admin-role-permissions","title":"Admin Role Permissions","text":"<p>Behavior: The admin role has a set of permissions related to database management tasks.</p> <p>Technical Details: - By default, these permissions don't extend to objects that other users create - The admin role can't grant or revoke permissions on user-created objects to other users - The admin user can grant itself any other role to get the necessary permissions on these objects</p> <p>Recommendation: Use role-based access control for managing permissions on user-created objects</p>"},{"location":"considerations.html#general-behavioral-considerations","title":"General Behavioral Considerations","text":""},{"location":"considerations.html#transaction-behavior","title":"Transaction Behavior","text":"<ul> <li>Timeout Limits: Large operations may timeout before completion</li> <li>Retry Logic: Implement retry mechanisms for transient errors</li> <li>Connection Management: Plan for connection lifecycle and recovery scenarios</li> </ul>"},{"location":"considerations.html#client-compatibility","title":"Client Compatibility","text":"<ul> <li>Version Requirements: Use supported PostgreSQL client versions</li> <li>Feature Support: Not all PostgreSQL features are available</li> <li>Error Handling: Implement proper error handling for unsupported operations</li> </ul>"},{"location":"considerations.html#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Large Table Operations: Consider alternative approaches for operations on large datasets</li> <li>Index Usage: Optimize queries to use available indexes effectively</li> <li>Connection Pooling: Implement appropriate connection pooling strategies</li> </ul>"},{"location":"considerations.html#security-considerations","title":"Security Considerations","text":"<ul> <li>Role Management: Understand admin role limitations with user-created objects</li> <li>Permission Inheritance: Plan role hierarchy for proper access control</li> <li>IAM Integration: Leverage IAM roles for secure database access</li> </ul>"},{"location":"considerations.html#best-practices","title":"Best Practices","text":""},{"location":"considerations.html#application-design","title":"Application Design","text":"<ol> <li>Implement retry logic for transient errors during cluster recovery</li> <li>Use appropriate timeouts for large table operations</li> <li>Plan for client compatibility requirements</li> <li>Design role hierarchy to handle admin role limitations</li> </ol>"},{"location":"considerations.html#operational-practices","title":"Operational Practices","text":"<ol> <li>Monitor cluster health and recovery operations</li> <li>Test client compatibility before production deployment</li> <li>Implement proper error handling for unsupported features</li> <li>Plan capacity based on quotas and limits</li> </ol>"},{"location":"considerations.html#development-practices","title":"Development Practices","text":"<ol> <li>Use supported PostgreSQL features only</li> <li>Test with realistic data volumes to understand timeout behavior</li> <li>Implement proper connection management in applications</li> <li>Design for eventual consistency during recovery scenarios</li> </ol>"},{"location":"copy-page-button.html","title":"Copy page button","text":"\ud83d\udccb Copy Page"},{"location":"copy-page-script.html","title":"Copy page script","text":""},{"location":"database-roles-iam-authentication.html","title":"Database Roles and IAM Authentication","text":"\ud83d\udccb Copy Page"},{"location":"database-roles-iam-authentication.html#database-roles-and-iam-authentication","title":"Database Roles and IAM Authentication","text":""},{"location":"database-roles-iam-authentication.html#overview","title":"Overview","text":"<p>Amazon Aurora DSQL supports authentication using both IAM roles and IAM users. You can use either method to authenticate and access Aurora DSQL databases.</p>"},{"location":"database-roles-iam-authentication.html#iam-identity-types","title":"IAM Identity Types","text":""},{"location":"database-roles-iam-authentication.html#iam-roles","title":"IAM Roles","text":"<p>An IAM role is an identity within your AWS account that has specific permissions but is not associated with a specific person. Using IAM roles provide temporary security credentials. You can temporarily assume an IAM role in several ways:</p> <ul> <li>By switching roles in the AWS Console</li> <li>By calling a CLI or AWS API operation</li> <li>By using a custom URL</li> </ul> <p>After assuming a role, you can access Aurora DSQL using the role's temporary credentials. For more information about methods for using roles, see IAM Identities in the IAM user guide.</p>"},{"location":"database-roles-iam-authentication.html#iam-users","title":"IAM Users","text":"<p>An IAM user is an identity within your AWS account that has specific permissions and is associated with a single person or application. IAM users have long-term credentials such as passwords and access keys that can be used to access Aurora DSQL.</p> <p>Note: To run SQL commands with IAM authentication, you can use either IAM role ARNs or IAM user ARNs in the examples below.</p>"},{"location":"database-roles-iam-authentication.html#database-role-types","title":"Database Role Types","text":"<p>Aurora DSQL supports two types of database roles:</p>"},{"location":"database-roles-iam-authentication.html#admin-role","title":"Admin Role","text":"<ul> <li>Aurora DSQL automatically creates a predefined <code>admin</code> role for you in your Aurora DSQL cluster</li> <li>You can't modify the <code>admin</code> role</li> <li>When you connect to your database as <code>admin</code>, you can issue SQL to create new database-level roles</li> </ul>"},{"location":"database-roles-iam-authentication.html#custom-roles","title":"Custom Roles","text":"<ul> <li>You create and manage custom roles using SQL commands in your database</li> <li>Custom roles must be associated with your IAM roles to allow IAM identities to connect to your database</li> </ul>"},{"location":"database-roles-iam-authentication.html#working-with-custom-database-roles","title":"Working with Custom Database Roles","text":""},{"location":"database-roles-iam-authentication.html#authorizing-database-roles-to-connect-to-your-cluster","title":"Authorizing Database Roles to Connect to Your Cluster","text":"<ol> <li> <p>Create an IAM role and grant connection authorization with the IAM policy action: <code>dsql:DbConnect</code></p> </li> <li> <p>Grant cluster access permissions - The IAM policy must also grant permission to access the cluster resources. Use a wildcard (<code>*</code>) or follow the instructions for using IAM condition keys with Aurora DSQL.</p> </li> </ol>"},{"location":"database-roles-iam-authentication.html#authorizing-database-roles-to-use-sql-in-your-database","title":"Authorizing Database Roles to Use SQL in Your Database","text":"<p>You must use an IAM role with authorization to connect to your cluster.</p> <p>Step-by-step process:</p> <ol> <li> <p>Connect to your Aurora DSQL cluster using a SQL utility with the <code>admin</code> database role and an IAM identity that is authorized for IAM action <code>dsql:DbConnectAdmin</code></p> </li> <li> <p>Create a new database role with the <code>WITH LOGIN</code> option:    <pre><code>CREATE ROLE example WITH LOGIN;\n</code></pre></p> </li> <li> <p>Associate the database role with the IAM role ARN:    <pre><code>AWS IAM GRANT example TO 'arn:aws:iam::012345678912:role/example';\n</code></pre></p> </li> <li> <p>Grant database-level permissions to the database role:    <pre><code>GRANT USAGE ON SCHEMA myschema TO example;\nGRANT SELECT, INSERT, UPDATE ON ALL TABLES IN SCHEMA myschema TO example;\n</code></pre></p> </li> </ol> <p>For more information, see PostgreSQL GRANT and PostgreSQL Privileges in the PostgreSQL documentation.</p>"},{"location":"database-roles-iam-authentication.html#viewing-iam-to-database-role-mappings","title":"Viewing IAM to Database Role Mappings","text":"<p>To view the mappings between IAM roles and database roles, query the <code>sys.iam_pg_role_mappings</code> system table:</p> <pre><code>SELECT * FROM sys.iam_pg_role_mappings;\n</code></pre> <p>Example output: <pre><code> iam_oid |                  arn                   | pg_role_oid | pg_role_name | grantor_pg_role_oid | grantor_pg_role_name\n---------+----------------------------------------+-------------+--------------+---------------------+----------------------\n   26398 | arn:aws:iam::012345678912:role/example |       26396 | example      |               15579 | admin\n(1 row)\n</code></pre></p> <p>This table shows all the mappings between IAM roles (identified by their ARN) and PostgreSQL database roles.</p>"},{"location":"database-roles-iam-authentication.html#revoking-database-authorization-from-an-iam-role","title":"Revoking Database Authorization from an IAM Role","text":"<p>To revoke database authorization, use the <code>AWS IAM REVOKE</code> operation:</p> <pre><code>AWS IAM REVOKE example FROM 'arn:aws:iam::012345678912:role/example';\n</code></pre>"},{"location":"database-roles-iam-authentication.html#authentication-process-flow","title":"Authentication Process Flow","text":""},{"location":"database-roles-iam-authentication.html#for-admin-role","title":"For Admin Role","text":"<ol> <li>IAM Authentication: Generate temporary authentication token using IAM identity with <code>dsql:DbConnectAdmin</code> permission</li> <li>Database Connection: Use token as password to connect with <code>admin</code> role</li> <li>Database Operations: Perform administrative tasks, create custom roles</li> </ol>"},{"location":"database-roles-iam-authentication.html#for-custom-role","title":"For Custom Role","text":"<ol> <li>Role Creation: Admin creates custom database role with <code>CREATE ROLE example WITH LOGIN</code></li> <li>IAM Association: Admin associates role with IAM identity using <code>AWS IAM GRANT</code></li> <li>Permission Granting: Admin grants specific database permissions to custom role</li> <li>IAM Authentication: User generates token using IAM identity with <code>dsql:DbConnect</code> permission</li> <li>Database Connection: User connects using custom role and token</li> </ol>"},{"location":"database-roles-iam-authentication.html#authorization-process-flow","title":"Authorization Process Flow","text":""},{"location":"database-roles-iam-authentication.html#database-level-authorization","title":"Database-Level Authorization","text":"<ol> <li>Connect as admin: Use admin role to connect to cluster</li> <li>Create custom roles: Issue SQL commands to create new database-level roles</li> <li>Grant permissions: Use PostgreSQL GRANT commands to assign specific permissions</li> <li>Associate with IAM: Use <code>AWS IAM GRANT</code> to link database roles with IAM identities</li> </ol>"},{"location":"database-roles-iam-authentication.html#cluster-level-authorization","title":"Cluster-Level Authorization","text":"<ul> <li>Managed through IAM policies</li> <li>Use <code>dsql:DbConnectAdmin</code> for admin role access</li> <li>Use <code>dsql:DbConnect</code> for custom role access</li> </ul>"},{"location":"database-roles-iam-authentication.html#best-practices","title":"Best Practices","text":""},{"location":"database-roles-iam-authentication.html#security-recommendations","title":"Security Recommendations","text":"<ul> <li>Use custom database roles for production applications instead of admin role</li> <li>Grant minimal necessary permissions to custom roles</li> <li>Regularly review and audit role mappings using <code>sys.iam_pg_role_mappings</code></li> <li>Use temporary IAM roles when possible for enhanced security</li> </ul>"},{"location":"database-roles-iam-authentication.html#permission-management","title":"Permission Management","text":"<ul> <li>You can't manage permissions of the predefined <code>admin</code> database role</li> <li>Modifications to privileges take effect on the next transaction after Aurora DSQL successfully commits the modification transaction</li> <li>Use PostgreSQL standard commands for managing custom role permissions</li> </ul>"},{"location":"generate-authentication-token.html","title":"Generate Authentication Token","text":"\ud83d\udccb Copy Page"},{"location":"generate-authentication-token.html#generating-an-authentication-token-in-amazon-aurora-dsql","title":"Generating an Authentication Token in Amazon Aurora DSQL","text":""},{"location":"generate-authentication-token.html#overview","title":"Overview","text":"<p>To connect to Amazon Aurora DSQL with a SQL client, generate an authentication token to use as the password. This token is used only for authenticating the connection. After the connection is established, the connection remains valid even if the authentication token expires.</p>"},{"location":"generate-authentication-token.html#token-expiration-and-duration","title":"Token Expiration and Duration","text":"<ul> <li>AWS Console: Token automatically expires in one hour by default</li> <li>CLI or SDKs: Default is 15 minutes</li> <li>Maximum duration: 604,800 seconds (one week)</li> </ul> <p>To connect to Aurora DSQL from your client again, you can use the same authentication token if it hasn't expired, or you can generate a new one.</p>"},{"location":"generate-authentication-token.html#prerequisites","title":"Prerequisites","text":"<p>To get started with generating a token: 1. Create an IAM policy 2. Create a cluster in Aurora DSQL</p> <p>At a minimum, you must have the IAM permissions for connecting to clusters, depending on which database role you use to connect.</p>"},{"location":"generate-authentication-token.html#using-the-aws-console-to-generate-an-authentication-token","title":"Using the AWS Console to Generate an Authentication Token","text":"<p>Aurora DSQL authenticates users with a token rather than a password. You can generate the token from the console.</p> <p>To generate an authentication token:</p> <ol> <li> <p>Sign in to the AWS Console and open the Aurora DSQL console at https://console.aws.amazon.com/dsql</p> </li> <li> <p>Choose the cluster ID of the cluster for which you want to generate an authentication token</p> </li> <li> <p>Choose Connect and then select Get Token</p> </li> <li> <p>Choose whether you want to connect as an <code>admin</code> or with a custom database role</p> </li> <li> <p>Copy the generated authentication token and use it for connecting with SQL clients</p> </li> </ol>"},{"location":"generate-authentication-token.html#using-aws-cloudshell-to-generate-an-authentication-token","title":"Using AWS CloudShell to Generate an Authentication Token","text":"<p>Before you can generate an authentication token using CloudShell, make sure that you have created an Aurora DSQL cluster.</p> <p>To generate an authentication token using CloudShell:</p> <ol> <li> <p>Sign in to the AWS Console and open the Aurora DSQL console at https://console.aws.amazon.com/dsql</p> </li> <li> <p>At the bottom left of the AWS console, choose CloudShell</p> </li> <li> <p>Run the following command to generate an authentication token for the <code>admin</code> role:</p> </li> </ol> <pre><code>\n</code></pre> <p>Note: If you're not connecting as <code>admin</code>, use <code>generate-db-connect-auth-token</code> instead.</p> <ol> <li>Use the following command to use <code>psql</code> to start a connection to your cluster:</li> </ol> <pre><code>\n</code></pre> <ol> <li> <p>When prompted for a password, paste the generated token</p> </li> <li> <p>Press Enter to see the PostgreSQL prompt: <code>postgres=&gt;</code></p> </li> </ol>"},{"location":"generate-authentication-token.html#using-the-aws-cli-to-generate-an-authentication-token","title":"Using the AWS CLI to Generate an Authentication Token","text":"<p>When your cluster is <code>ACTIVE</code>, you can generate an authentication token using the <code>aws dsql</code> command:</p> <ul> <li>Admin role: Use <code>generate-db-connect-admin-auth-token</code></li> <li>Custom database role: Use <code>generate-db-connect-auth-token</code></li> </ul> <p>The following example uses these attributes to generate an authentication token for the <code>admin</code> role: - your_cluster_endpoint: The endpoint of the cluster (format: <code>your_cluster_identifier.dsql.region.on.aws</code>) - region: The AWS Region, such as <code>us-east-2</code> or <code>us-east-1</code></p> <p>Linux and macOS: <pre><code>\n</code></pre></p> <p>Windows: <pre><code>\n</code></pre></p>"},{"location":"generate-authentication-token.html#using-the-sdks-to-generate-a-token","title":"Using the SDKs to Generate a Token","text":"<p>You can generate an authentication token for your cluster when it is in <code>ACTIVE</code> status. The SDK examples use the following attributes to generate an authentication token for the <code>admin</code> role:</p> <ul> <li>your_cluster_endpoint: The endpoint of your Aurora DSQL cluster (format: <code>your_cluster_identifier.dsql.region.on.aws</code>)</li> <li>region: The AWS Region in which your cluster is located</li> </ul>"},{"location":"generate-authentication-token.html#python-sdk","title":"Python SDK","text":"<p>You can generate the token in the following ways: - Admin role: Use <code>generate_db_connect_admin_auth_token</code> - Custom database role: Use <code>generate_connect_auth_token</code></p> <pre><code>\n</code></pre>"},{"location":"generate-authentication-token.html#c-sdk","title":"C++ SDK","text":"<p>You can generate the token in the following ways: - Admin role: Use <code>GenerateDBConnectAdminAuthToken</code> - Custom database role: Use <code>GenerateDBConnectAuthToken</code></p> <pre><code>\n</code></pre>"},{"location":"generate-authentication-token.html#javascript-sdk","title":"JavaScript SDK","text":"<p>You can generate the token in the following ways: - Admin role: Use <code>getDbConnectAdminAuthToken</code> - Custom database role: Use <code>getDbConnectAuthToken</code></p> <pre><code>\n</code></pre>"},{"location":"generate-authentication-token.html#java-sdk","title":"Java SDK","text":"<p>You can generate the token in the following ways: - Admin role: Use <code>generateDbConnectAdminAuthToken</code> - Custom database role: Use <code>generateDbConnectAuthToken</code></p> <pre><code>\n</code></pre>"},{"location":"generate-authentication-token.html#rust-sdk","title":"Rust SDK","text":"<p>You can generate the token in the following ways: - Admin role: Use <code>db_connect_admin_auth_token</code> - Custom database role: Use <code>db_connect_auth_token</code></p> <pre><code>\n</code></pre>"},{"location":"generate-authentication-token.html#ruby-sdk","title":"Ruby SDK","text":"<p>You can generate the token in the following ways: - Admin role: Use <code>generate_db_connect_admin_auth_token</code> - Custom database role: Use <code>generate_db_connect_auth_token</code></p> <pre><code>\n</code></pre>"},{"location":"generate-authentication-token.html#net-sdk","title":".NET SDK","text":"<p>Note: The official SDK for .NET doesn't include a built-in API call to generate an authentication token for Aurora DSQL. Instead, you must use <code>DSQLAuthTokenGenerator</code>, which is a utility class.</p> <p>You can generate the token in the following ways: - Admin role: Use <code>DbConnectAdmin</code> - Custom database role: Use <code>DbConnect</code></p> <pre><code>\n</code></pre>"},{"location":"generate-authentication-token.html#golang-sdk","title":"Golang SDK","text":"<p>Note: The Golang SDK doesn't provide a built-in method for generating a pre-signed token. You must manually construct the signed request.</p> <p>In the following code example, specify the <code>action</code> based on the PostgreSQL user: - Admin role: Use the <code>DbConnectAdmin</code> action - Custom database role: Use the <code>DbConnect</code> action</p> <pre><code>\n</code></pre>"},{"location":"managing-clusters.html","title":"Managing Clusters Overview","text":"\ud83d\udccb Copy Page"},{"location":"managing-clusters.html#managing-amazon-aurora-dsql-clusters","title":"Managing Amazon Aurora DSQL Clusters","text":""},{"location":"managing-clusters.html#overview","title":"Overview","text":"<p>Learn how to set up and optimize performance for your Aurora DSQL deployments. Aurora DSQL provides several configuration options to help you establish the right database infrastructure for your needs.</p>"},{"location":"managing-clusters.html#cluster-management-topics","title":"Cluster Management Topics","text":"<p>The features and functionality discussed in this guide ensure that your Aurora DSQL environment is more resilient, responsive, and capable of supporting your applications as they grow and evolve.</p>"},{"location":"managing-clusters.html#single-region-clusters","title":"Single-Region Clusters","text":"<p>Purpose: Deploy Aurora DSQL clusters within a single AWS Region</p> <p>Key Features: - Simplified deployment and management - Lower latency for regional applications - Cost-effective for single-region workloads - Automatic scaling within the region</p> <p>Use Cases: - Applications with users in a specific geographic region - Development and testing environments - Cost-sensitive workloads - Applications with strict data residency requirements</p>"},{"location":"managing-clusters.html#multi-region-clusters","title":"Multi-Region Clusters","text":"<p>Purpose: Deploy Aurora DSQL clusters across multiple AWS Regions</p> <p>Key Features: - High availability across regions - Disaster recovery capabilities - Global application support - Cross-region data replication</p> <p>Use Cases: - Global applications with worldwide users - Business continuity and disaster recovery - Compliance with data sovereignty requirements - High availability requirements</p>"},{"location":"managing-clusters.html#cloudformation-setup","title":"CloudFormation Setup","text":"<p>Purpose: Infrastructure as Code deployment for Aurora DSQL clusters</p> <p>Key Features: - Automated cluster provisioning - Repeatable deployments - Version-controlled infrastructure - Integration with AWS CloudFormation</p> <p>Use Cases: - Automated deployment pipelines - Consistent environment provisioning - Infrastructure version control - Large-scale deployments</p>"},{"location":"managing-clusters.html#cluster-lifecycle-management","title":"Cluster Lifecycle Management","text":"<p>Purpose: Manage Aurora DSQL clusters throughout their operational lifecycle</p> <p>Key Features: - Cluster creation and deletion - Configuration updates and modifications - Monitoring and maintenance - Performance optimization</p> <p>Use Cases: - Ongoing cluster maintenance - Performance tuning and optimization - Capacity planning and scaling - Operational monitoring</p>"},{"location":"managing-clusters.html#configuration-options","title":"Configuration Options","text":""},{"location":"managing-clusters.html#deployment-strategies","title":"Deployment Strategies","text":"<p>Single-Region Deployment: - Choose appropriate AWS Region based on user location - Configure cluster size based on expected workload - Set up monitoring and alerting - Plan for backup and recovery</p> <p>Multi-Region Deployment: - Select primary and secondary regions - Configure cross-region replication - Set up failover procedures - Plan for data consistency requirements</p>"},{"location":"managing-clusters.html#performance-optimization","title":"Performance Optimization","text":"<p>Cluster Sizing: - Assess application requirements - Plan for peak usage patterns - Consider growth projections - Monitor performance metrics</p> <p>Connection Management: - Implement connection pooling - Plan for connection limits - Configure timeout settings - Monitor connection usage</p>"},{"location":"managing-clusters.html#best-practices","title":"Best Practices","text":""},{"location":"managing-clusters.html#planning-and-design","title":"Planning and Design","text":"<ol> <li>Assess requirements before cluster creation</li> <li>Choose appropriate regions based on user distribution</li> <li>Plan for scalability and future growth</li> <li>Consider compliance and data residency requirements</li> </ol>"},{"location":"managing-clusters.html#operational-excellence","title":"Operational Excellence","text":"<ol> <li>Monitor cluster performance regularly</li> <li>Implement automated backups and recovery procedures</li> <li>Set up alerting for critical metrics</li> <li>Plan for maintenance windows and updates</li> </ol>"},{"location":"managing-clusters.html#security-and-compliance","title":"Security and Compliance","text":"<ol> <li>Configure appropriate access controls using IAM</li> <li>Implement encryption for data at rest and in transit</li> <li>Regular security audits and access reviews</li> <li>Compliance monitoring for regulatory requirements</li> </ol>"},{"location":"managing-clusters.html#related-documentation","title":"Related Documentation","text":"<ul> <li>Getting Started: Getting Started</li> <li>Authentication: Auth &amp; Access Overview</li> <li>Quotas and Limits: Quotas and Limits Overview</li> <li>Troubleshooting: Troubleshooting Overview</li> </ul>"},{"location":"multi-region-clusters.html","title":"Multi-Region Clusters","text":"\ud83d\udccb Copy Page"},{"location":"multi-region-clusters.html#configuring-multi-region-clusters","title":"Configuring Multi-Region Clusters","text":""},{"location":"multi-region-clusters.html#overview","title":"Overview","text":"<p>Learn how to work with clusters that span multiple AWS Regions. Configure and manage clusters across multiple AWS Regions using either the CLI or your preferred programming language including Python, C++, JavaScript, Java, Rust, Ruby, .NET, and Golang.</p>"},{"location":"multi-region-clusters.html#multi-region-cluster-concepts","title":"Multi-Region Cluster Concepts","text":""},{"location":"multi-region-clusters.html#witness-region","title":"Witness Region","text":"<p>A witness region is a third AWS Region that helps maintain consistency and availability for your multi-region cluster. The witness region doesn't host client endpoints but maintains transaction logs to support quorum decisions.</p>"},{"location":"multi-region-clusters.html#cluster-linking","title":"Cluster Linking","text":"<p>Multi-region clusters are created by linking two clusters in different regions. This creates a synchronized, distributed database that spans multiple geographic locations.</p>"},{"location":"multi-region-clusters.html#high-availability","title":"High Availability","text":"<p>Multi-region clusters provide enhanced availability and disaster recovery capabilities by maintaining synchronized data across multiple AWS Regions.</p>"},{"location":"multi-region-clusters.html#using-aws-sdks","title":"Using AWS SDKs","text":"<p>The AWS SDKs provide programmatic access to Aurora DSQL in your preferred programming language. The following sections show how to perform common multi-region cluster operations.</p>"},{"location":"multi-region-clusters.html#create-multi-region-cluster","title":"Create Multi-Region Cluster","text":"<p>The following examples show how to create a multi-Region cluster using different programming languages.</p>"},{"location":"multi-region-clusters.html#python-sdk","title":"Python SDK","text":"<pre><code>import boto3\n\ndef create_multi_region_clusters(region_1, region_2, witness_region):\n    try:\n        client_1 = boto3.client(\"dsql\", region_name=region_1)\n        client_2 = boto3.client(\"dsql\", region_name=region_2)\n\n        # We can only set the witness region for the first cluster\n        cluster_1 = client_1.create_cluster(\n            deletionProtectionEnabled=True,\n            multiRegionProperties={\"witnessRegion\": witness_region},\n            tags={\"Name\": \"Python multi region cluster\"}\n        )\n        print(f\"Created {cluster_1['arn']}\")\n\n        # For the second cluster we can set witness region and designate cluster_1 as a peer\n        cluster_2 = client_2.create_cluster(\n            deletionProtectionEnabled=True,\n            multiRegionProperties={\"witnessRegion\": witness_region, \"clusters\": [cluster_1[\"arn\"]]},\n            tags={\"Name\": \"Python multi region cluster\"}\n        )\n        print(f\"Created {cluster_2['arn']}\")\n\n        # Now that we know the cluster_2 arn we can set it as a peer of cluster_1\n        client_1.update_cluster(\n            identifier=cluster_1[\"identifier\"],\n            multiRegionProperties={\"witnessRegion\": witness_region, \"clusters\": [cluster_2[\"arn\"]]}\n        )\n        print(f\"Added {cluster_2['arn']} as a peer of {cluster_1['arn']}\")\n\n        # Now that multiRegionProperties is fully defined for both clusters they'll begin the transition to ACTIVE\n        print(f\"Waiting for {cluster_1['arn']} to become ACTIVE\")\n        client_1.get_waiter(\"cluster_active\").wait(\n            identifier=cluster_1[\"identifier\"],\n            WaiterConfig={'Delay': 10, 'MaxAttempts': 30}\n        )\n\n        print(f\"Waiting for {cluster_2['arn']} to become ACTIVE\")\n        client_2.get_waiter(\"cluster_active\").wait(\n            identifier=cluster_2[\"identifier\"],\n            WaiterConfig={'Delay': 10, 'MaxAttempts': 30}\n        )\n\n        return (cluster_1, cluster_2)\n    except:\n        print(\"Unable to create cluster\")\n        raise\n\ndef main():\n    region_1 = \"us-east-1\"\n    region_2 = \"us-east-2\"\n    witness_region = \"us-west-2\"\n    (cluster_1, cluster_2) = create_multi_region_clusters(region_1, region_2, witness_region)\n    print(\"Created multi region clusters:\")\n    print(\"Cluster id: \" + cluster_1['arn'])\n    print(\"Cluster id: \" + cluster_2['arn'])\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"multi-region-clusters.html#javascript-sdk","title":"JavaScript SDK","text":"<pre><code>import { DSQLClient, CreateClusterCommand, UpdateClusterCommand, waitUntilClusterActive } from \"@aws-sdk/client-dsql\";\n\nasync function createMultiRegionCluster(region1, region2, witnessRegion) {\n    const client1 = new DSQLClient({ region: region1 });\n    const client2 = new DSQLClient({ region: region2 });\n\n    try {\n        // We can only set the witness region for the first cluster\n        console.log(`Creating cluster in ${region1}`);\n        const createClusterCommand1 = new CreateClusterCommand({\n            deletionProtectionEnabled: true,\n            tags: { Name: \"javascript multi region cluster 1\" },\n            multiRegionProperties: { witnessRegion: witnessRegion }\n        });\n        const response1 = await client1.send(createClusterCommand1);\n        console.log(`Created ${response1.arn}`);\n\n        // For the second cluster we can set witness region and designate the first cluster as a peer\n        console.log(`Creating cluster in ${region2}`);\n        const createClusterCommand2 = new CreateClusterCommand({\n            deletionProtectionEnabled: true,\n            tags: { Name: \"javascript multi region cluster 2\" },\n            multiRegionProperties: {\n                witnessRegion: witnessRegion,\n                clusters: [response1.arn]\n            }\n        });\n        const response2 = await client2.send(createClusterCommand2);\n        console.log(`Created ${response2.arn}`);\n\n        // Now that we know the second cluster arn we can set it as a peer of the first cluster\n        const updateClusterCommand = new UpdateClusterCommand({\n            identifier: response1.identifier,\n            multiRegionProperties: {\n                witnessRegion: witnessRegion,\n                clusters: [response2.arn]\n            }\n        });\n        await client1.send(updateClusterCommand);\n        console.log(`Added ${response2.arn} as a peer of ${response1.arn}`);\n\n        // Now that multiRegionProperties is fully defined for both clusters they'll begin the transition to ACTIVE\n        console.log(`Waiting for cluster ${response1.identifier} to become ACTIVE`);\n        await waitUntilClusterActive(\n            { client: client1, maxWaitTime: 300 },\n            { identifier: response1.identifier }\n        );\n        console.log(`Cluster 1 is now active`);\n\n        console.log(`Waiting for cluster ${response2.identifier} to become ACTIVE`);\n        await waitUntilClusterActive(\n            { client: client2, maxWaitTime: 300 },\n            { identifier: response2.identifier }\n        );\n        console.log(`Cluster 2 is now active`);\n        console.log(\"The multi region clusters are now active\");\n        return;\n    } catch (error) {\n        console.error(\"Failed to create cluster: \", error.message);\n        throw error;\n    }\n}\n\nasync function main() {\n    const region1 = \"us-east-1\";\n    const region2 = \"us-east-2\";\n    const witnessRegion = \"us-west-2\";\n\n    await createMultiRegionCluster(region1, region2, witnessRegion);\n}\n\nmain();\n</code></pre>"},{"location":"multi-region-clusters.html#java-sdk","title":"Java SDK","text":"<pre><code>import software.amazon.awssdk.auth.credentials.DefaultCredentialsProvider;\nimport software.amazon.awssdk.regions.Region;\nimport software.amazon.awssdk.services.dsql.DsqlClient;\nimport software.amazon.awssdk.services.dsql.DsqlClientBuilder;\nimport software.amazon.awssdk.services.dsql.model.CreateClusterRequest;\nimport software.amazon.awssdk.services.dsql.model.CreateClusterResponse;\nimport software.amazon.awssdk.services.dsql.model.GetClusterResponse;\nimport software.amazon.awssdk.services.dsql.model.UpdateClusterRequest;\n\nimport java.time.Duration;\nimport java.util.Map;\n\npublic class CreateMultiRegionCluster {\n    public static void main(String[] args) {\n        Region region1 = Region.US_EAST_1;\n        Region region2 = Region.US_EAST_2;\n        Region witnessRegion = Region.US_WEST_2;\n\n        DsqlClientBuilder clientBuilder = DsqlClient.builder()\n                .credentialsProvider(DefaultCredentialsProvider.create());\n\n        try (DsqlClient client1 = clientBuilder.region(region1).build();\n             DsqlClient client2 = clientBuilder.region(region2).build()) {\n\n            // We can only set the witness region for the first cluster\n            System.out.println(\"Creating cluster in \" + region1);\n            CreateClusterRequest request1 = CreateClusterRequest.builder()\n                    .deletionProtectionEnabled(true)\n                    .multiRegionProperties(mrp -&gt; mrp.witnessRegion(witnessRegion.toString()))\n                    .tags(Map.of(\"Name\", \"java multi region cluster\"))\n                    .build();\n            CreateClusterResponse cluster1 = client1.createCluster(request1);\n            System.out.println(\"Created \" + cluster1.arn());\n\n            // For the second cluster we can set the witness region and designate cluster1 as a peer\n            System.out.println(\"Creating cluster in \" + region2);\n            CreateClusterRequest request2 = CreateClusterRequest.builder()\n                    .deletionProtectionEnabled(true)\n                    .multiRegionProperties(mrp -&gt;\n                            mrp.witnessRegion(witnessRegion.toString()).clusters(cluster1.arn())\n                    )\n                    .tags(Map.of(\"Name\", \"java multi region cluster\"))\n                    .build();\n            CreateClusterResponse cluster2 = client2.createCluster(request2);\n            System.out.println(\"Created \" + cluster2.arn());\n\n            // Now that we know the cluster2 ARN we can set it as a peer of cluster1\n            UpdateClusterRequest updateReq = UpdateClusterRequest.builder()\n                    .identifier(cluster1.identifier())\n                    .multiRegionProperties(mrp -&gt;\n                            mrp.witnessRegion(witnessRegion.toString()).clusters(cluster2.arn())\n                    )\n                    .build();\n            client1.updateCluster(updateReq);\n            System.out.printf(\"Added %s as a peer of %s%n\", cluster2.arn(), cluster1.arn());\n\n            // Wait for both clusters to become ACTIVE\n            System.out.printf(\"Waiting for cluster %s to become ACTIVE%n\", cluster1.arn());\n            GetClusterResponse activeCluster1 = client1.waiter().waitUntilClusterActive(\n                    getCluster -&gt; getCluster.identifier(cluster1.identifier()),\n                    config -&gt; config.waitTimeout(Duration.ofMinutes(5))\n            ).matched().response().orElseThrow();\n\n            System.out.printf(\"Waiting for cluster %s to become ACTIVE%n\", cluster2.arn());\n            GetClusterResponse activeCluster2 = client2.waiter().waitUntilClusterActive(\n                    getCluster -&gt; getCluster.identifier(cluster2.identifier()),\n                    config -&gt; config.waitTimeout(Duration.ofMinutes(5))\n            ).matched().response().orElseThrow();\n\n            System.out.println(\"Created multi region clusters:\");\n            System.out.println(activeCluster1);\n            System.out.println(activeCluster2);\n        }\n    }\n}\n</code></pre>"},{"location":"multi-region-clusters.html#ruby-sdk","title":"Ruby SDK","text":"<pre><code>require \"aws-sdk-dsql\"\nrequire \"pp\"\n\ndef create_multi_region_clusters(region_1, region_2, witness_region)\n  client_1 = Aws::DSQL::Client.new(region: region_1)\n  client_2 = Aws::DSQL::Client.new(region: region_2)\n\n  # We can only set the witness region for the first cluster\n  puts \"Creating cluster in #{region_1}\"\n  cluster_1 = client_1.create_cluster(\n    deletion_protection_enabled: true,\n    multi_region_properties: { witness_region: witness_region },\n    tags: { Name: \"ruby multi region cluster\" }\n  )\n  puts \"Created #{cluster_1.arn}\"\n\n  # For the second cluster we can set witness region and designate cluster_1 as a peer\n  puts \"Creating cluster in #{region_2}\"\n  cluster_2 = client_2.create_cluster(\n    deletion_protection_enabled: true,\n    multi_region_properties: {\n      witness_region: witness_region,\n      clusters: [ cluster_1.arn ]\n    },\n    tags: { Name: \"ruby multi region cluster\" }\n  )\n  puts \"Created #{cluster_2.arn}\"\n\n  # Now that we know the cluster_2 arn we can set it as a peer of cluster_1\n  client_1.update_cluster(\n    identifier: cluster_1.identifier,\n    multi_region_properties: {\n      witness_region: witness_region,\n      clusters: [ cluster_2.arn ]\n    }\n  )\n  puts \"Added #{cluster_2.arn} as a peer of #{cluster_1.arn}\"\n\n  # Now that multi_region_properties is fully defined for both clusters they'll begin the transition to ACTIVE\n  puts \"Waiting for #{cluster_1.arn} to become ACTIVE\"\n  cluster_1 = client_1.wait_until(:cluster_active, identifier: cluster_1.identifier) do |w|\n    w.max_attempts = 30\n    w.delay = 10\n  end\n\n  puts \"Waiting for #{cluster_2.arn} to become ACTIVE\"\n  cluster_2 = client_2.wait_until(:cluster_active, identifier: cluster_2.identifier) do |w|\n    w.max_attempts = 30\n    w.delay = 10\n  end\n\n  [ cluster_1, cluster_2 ]\nrescue Aws::Errors::ServiceError =&gt; e\n  abort \"Failed to create multi-region clusters: #{e.message}\"\nend\n\ndef main\n  region_1 = \"us-east-1\"\n  region_2 = \"us-east-2\"\n  witness_region = \"us-west-2\"\n\n  cluster_1, cluster_2 = create_multi_region_clusters(region_1, region_2, witness_region)\n\n  puts \"Created multi region clusters:\"\n  pp cluster_1\n  pp cluster_2\nend\n\nmain if $PROGRAM_NAME == __FILE__\n</code></pre>"},{"location":"multi-region-clusters.html#get-multi-region-cluster-information","title":"Get Multi-Region Cluster Information","text":""},{"location":"multi-region-clusters.html#python-sdk_1","title":"Python SDK","text":"<pre><code>import boto3\nfrom datetime import datetime\nimport json\n\ndef get_cluster(region, identifier):\n    try:\n        client = boto3.client(\"dsql\", region_name=region)\n        return client.get_cluster(identifier=identifier)\n    except:\n        print(f\"Unable to get cluster {identifier} in region {region}\")\n        raise\n\ndef main():\n    region = \"us-east-1\"\n    cluster_id = \"&lt;your cluster id&gt;\"\n    response = get_cluster(region, cluster_id)\n    print(json.dumps(response, indent=2, default=lambda obj: obj.isoformat() if isinstance(obj, datetime) else None))\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"multi-region-clusters.html#update-multi-region-cluster","title":"Update Multi-Region Cluster","text":""},{"location":"multi-region-clusters.html#python-sdk_2","title":"Python SDK","text":"<pre><code>import boto3\n\ndef update_cluster(region, cluster_id, deletion_protection_enabled):\n    try:\n        client = boto3.client(\"dsql\", region_name=region)\n        return client.update_cluster(identifier=cluster_id, deletionProtectionEnabled=deletion_protection_enabled)\n    except:\n        print(\"Unable to update cluster\")\n        raise\n\ndef main():\n    region = \"us-east-1\"\n    cluster_id = \"&lt;your cluster id&gt;\"\n    deletion_protection_enabled = False\n    response = update_cluster(region, cluster_id, deletion_protection_enabled)\n    print(f\"Updated {response['arn']} with deletion_protection_enabled: {deletion_protection_enabled}\")\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"multi-region-clusters.html#delete-multi-region-clusters","title":"Delete Multi-Region Clusters","text":""},{"location":"multi-region-clusters.html#python-sdk_3","title":"Python SDK","text":"<pre><code>import boto3\n\ndef delete_multi_region_clusters(region_1, cluster_id_1, region_2, cluster_id_2):\n    try:\n        client_1 = boto3.client(\"dsql\", region_name=region_1)\n        client_2 = boto3.client(\"dsql\", region_name=region_2)\n\n        client_1.delete_cluster(identifier=cluster_id_1)\n        print(f\"Deleting cluster {cluster_id_1} in {region_1}\")\n\n        # cluster_1 will stay in PENDING_DELETE state until cluster_2 is deleted\n        client_2.delete_cluster(identifier=cluster_id_2)\n        print(f\"Deleting cluster {cluster_id_2} in {region_2}\")\n\n        # Now that both clusters have been marked for deletion they will transition to DELETING state and finalize deletion\n        print(f\"Waiting for {cluster_id_1} to finish deletion\")\n        client_1.get_waiter(\"cluster_not_exists\").wait(\n            identifier=cluster_id_1,\n            WaiterConfig={'Delay': 10, 'MaxAttempts': 30}\n        )\n\n        print(f\"Waiting for {cluster_id_2} to finish deletion\")\n        client_2.get_waiter(\"cluster_not_exists\").wait(\n            identifier=cluster_id_2,\n            WaiterConfig={'Delay': 10, 'MaxAttempts': 30}\n        )\n    except:\n        print(\"Unable to delete cluster\")\n        raise\n\ndef main():\n    region_1 = \"us-east-1\"\n    cluster_id_1 = \"&lt;cluster 1 id&gt;\"\n    region_2 = \"us-east-2\"\n    cluster_id_2 = \"&lt;cluster 2 id&gt;\"\n\n    delete_multi_region_clusters(region_1, cluster_id_1, region_2, cluster_id_2)\n    print(f\"Deleted {cluster_id_1} in {region_1} and {cluster_id_2} in {region_2}\")\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"multi-region-clusters.html#additional-sdk-examples","title":"Additional SDK Examples","text":"<p>For complete SDK examples in all supported languages (C++, Rust, .NET, Golang), visit the Aurora DSQL Samples GitHub repository.</p>"},{"location":"multi-region-clusters.html#multi-region-cluster-management","title":"Multi-Region Cluster Management","text":""},{"location":"multi-region-clusters.html#key-concepts","title":"Key Concepts","text":"<p>Witness Region: Third region that maintains transaction logs for quorum decisions Cluster Linking: Process of connecting clusters across regions Synchronization: Automatic data replication between linked clusters Failover: Automatic switching between regions during outages</p>"},{"location":"multi-region-clusters.html#best-practices","title":"Best Practices","text":"<p>Region Selection: - Choose regions close to your user base - Consider data residency requirements - Plan for disaster recovery scenarios - Ensure witness region is geographically separate</p> <p>Performance Considerations: - Account for cross-region latency - Plan for eventual consistency during network partitions - Monitor replication lag between regions - Design applications for multi-region architecture</p> <p>Security Considerations: - Configure IAM policies for cross-region access - Ensure encryption in transit between regions - Plan for compliance requirements across regions - Monitor access patterns across regions</p>"},{"location":"multi-region-clusters.html#operational-considerations","title":"Operational Considerations","text":""},{"location":"multi-region-clusters.html#cluster-states","title":"Cluster States","text":"<ul> <li>CREATING: Cluster is being provisioned</li> <li>ACTIVE: Cluster is ready for connections</li> <li>UPDATING: Cluster configuration is being modified</li> <li>DELETING: Cluster is being removed</li> <li>PENDING_DELETE: Multi-region cluster waiting for peer deletion</li> </ul>"},{"location":"multi-region-clusters.html#deletion-process","title":"Deletion Process","text":"<p>For multi-region clusters, both clusters must be deleted. The first cluster will remain in PENDING_DELETE state until the second cluster is also deleted.</p>"},{"location":"multi-region-clusters.html#monitoring","title":"Monitoring","text":"<ul> <li>Monitor cluster status in both regions</li> <li>Track replication metrics</li> <li>Set up alerts for failover events</li> <li>Monitor cross-region network connectivity</li> </ul>"},{"location":"multi-region-clusters.html#related-documentation","title":"Related Documentation","text":"<ul> <li>Single-Region Clusters: Single-Region Clusters</li> <li>Getting Started: Getting Started</li> <li>Authentication: Auth &amp; Access Overview</li> <li>Troubleshooting: Troubleshooting Overview</li> </ul>"},{"location":"programming-with-dsql.html","title":"Programming with DSQL Overview","text":"\ud83d\udccb Copy Page"},{"location":"programming-with-dsql.html#programming-with-amazon-aurora-dsql","title":"Programming with Amazon Aurora DSQL","text":""},{"location":"programming-with-dsql.html#overview","title":"Overview","text":"<p>Aurora DSQL provides you with the following tools to manage your Aurora DSQL resources programmatically and connect to your databases.</p>"},{"location":"programming-with-dsql.html#programmatic-access-tools","title":"Programmatic Access Tools","text":""},{"location":"programming-with-dsql.html#aws-command-line-interface-cli","title":"AWS Command Line Interface (CLI)","text":"<p>You can create and manage your resources by using the CLI in a command-line shell. The CLI provides direct access to the APIs for AWS services, such as Aurora DSQL.</p> <p>Documentation: AWS CLI Command Reference for DSQL</p>"},{"location":"programming-with-dsql.html#aws-software-development-kits-sdks","title":"AWS Software Development Kits (SDKs)","text":"<p>AWS provides SDKs for many popular technologies and programming languages. They make it easier for you to call AWS services from within your applications in that language or technology.</p> <p>Documentation: Tools for developing and managing applications on AWS</p>"},{"location":"programming-with-dsql.html#aurora-dsql-api","title":"Aurora DSQL API","text":"<p>This API is another programming interface for Aurora DSQL. When using this API, you must format every HTTPS request correctly and add a valid digital signature to every request.</p> <p>Documentation: Aurora DSQL API Reference</p>"},{"location":"programming-with-dsql.html#aws-cloudformation","title":"AWS CloudFormation","text":"<p>The AWS::DSQL::Cluster is a CloudFormation resource that enables you to create and manage Aurora DSQL clusters as part of your infrastructure as code.</p> <p>CloudFormation helps you define your entire AWS environment in code, making it easier to provision, update, and replicate your infrastructure in a consistent and reliable way. When you use the AWS::DSQL::Cluster resource in your CloudFormation templates, you can declaratively provision Aurora DSQL clusters alongside your other cloud resources.</p>"},{"location":"programming-with-dsql.html#accessing-aurora-dsql-with-postgresql-compatible-clients","title":"Accessing Aurora DSQL with PostgreSQL-Compatible Clients","text":"<p>Aurora DSQL uses the PostgreSQL wire protocol. You can connect to Aurora DSQL using a variety of tools and clients, such as CloudShell, psql, DBeaver, and DataGrip.</p>"},{"location":"programming-with-dsql.html#connection-parameter-mapping","title":"Connection Parameter Mapping","text":"PostgreSQL Aurora DSQL Notes Role (User or Group) Database Role Aurora DSQL creates a role named <code>admin</code>. Custom database roles must be associated with IAM roles for authentication. Host (hostname) Cluster Endpoint Single-Region clusters provide a single managed endpoint with automatic traffic redirection. Port Default <code>5432</code> Uses the PostgreSQL default port. Database (dbname) <code>postgres</code> Aurora DSQL creates this database when you create the cluster. SSL Mode SSL always enabled Aurora DSQL supports <code>require</code> SSL Mode. Connections without SSL are rejected. Password Authentication Token Aurora DSQL requires temporary authentication tokens instead of long-lived passwords."},{"location":"programming-with-dsql.html#authentication-requirements","title":"Authentication Requirements","text":"<p>When connecting, Aurora DSQL requires a signed IAM authentication token in place of a traditional password. These temporary tokens are generated using AWS Signature Version 4 and are used only during connection establishment. Once connected, the session remains active until it ends or the client disconnects.</p> <p>If you attempt to open a new session with an expired token, the connection request fails and a new token must be generated.</p>"},{"location":"programming-with-dsql.html#sql-client-access","title":"SQL Client Access","text":"<p>Aurora DSQL supports multiple PostgreSQL-compatible clients for connecting to your cluster. Each client requires a valid authentication token.</p>"},{"location":"programming-with-dsql.html#using-cloudshell-with-psql","title":"Using CloudShell with psql","text":"<p>Use the following procedure to access Aurora DSQL with the PostgreSQL interactive terminal from CloudShell.</p> <p>Steps: 1. Sign in to the Aurora DSQL console 2. Choose the cluster you want to connect to 3. Choose Connect with Query Editor and then Connect with CloudShell 4. Choose whether to connect as admin or with a custom database role 5. Choose Launch in CloudShell and Run in the CloudShell dialog</p>"},{"location":"programming-with-dsql.html#using-local-cli-with-psql","title":"Using Local CLI with psql","text":"<p>Use <code>psql</code>, a terminal-based front-end to PostgreSQL utility, to interactively enter queries and view results.</p> <p>Note: To improve query response times, use PostgreSQL version 17 client. Ensure you have Python version 3.8+ and psql version 14+.</p> <p>Connection Example: <pre><code># Aurora DSQL requires a valid IAM token as the password when connecting\n# Generate authentication token using AWS CLI\nexport PGPASSWORD=$(aws dsql generate-db-connect-admin-auth-token \\\n  --region us-east-1 \\\n  --expires-in 3600 \\\n  --hostname your_cluster_endpoint)\n\n# Aurora DSQL requires SSL and will reject connections without it\nexport PGSSLMODE=require\n\n# Connect with psql using the environment variables\npsql --quiet \\\n  --username admin \\\n  --dbname postgres \\\n  --host your_cluster_endpoint\n</code></pre></p>"},{"location":"programming-with-dsql.html#using-dbeaver","title":"Using DBeaver","text":"<p>DBeaver is an open-source, GUI-based database tool for connecting to and managing your database.</p> <p>Setup Steps: 1. Choose New Database Connection 2. Select PostgreSQL 3. In Connection settings/Main tab:    - Host: Your cluster endpoint    - Database: <code>postgres</code>    - Authentication: <code>Database Native</code>    - Username: <code>admin</code>    - Password: Generate authentication token 4. Configure SSL mode (<code>PGSSLMODE=require</code> or <code>PGSSLMODE=verify-full</code>) 5. Test connection and begin running SQL statements</p> <p>Important: Administrative features like Session Manager and Lock Manager don't apply to Aurora DSQL due to its unique architecture.</p>"},{"location":"programming-with-dsql.html#using-jetbrains-datagrip","title":"Using JetBrains DataGrip","text":"<p>DataGrip is a cross-platform IDE for working with SQL and databases, including PostgreSQL.</p> <p>Setup Steps: 1. Choose New Data Source and select PostgreSQL 2. In Data Sources/General tab:    - Host: Your cluster endpoint    - Port: <code>5432</code>    - Database: <code>postgres</code>    - Authentication: <code>User &amp; Password</code>    - Username: <code>admin</code>    - Password: Generate authentication token 3. Configure SSL mode in connection settings 4. Test connection and start running SQL statements</p> <p>Important: Some views like Sessions don't apply to Aurora DSQL due to its unique architecture.</p>"},{"location":"programming-with-dsql.html#database-connectivity-tools","title":"Database Connectivity Tools","text":"<p>AWS provides various tools for connecting to and working with Aurora DSQL databases, including database drivers, ORM libraries, and specialized adapters.</p>"},{"location":"programming-with-dsql.html#database-drivers","title":"Database Drivers","text":"<p>Low-level libraries that directly connect to the database:</p> Programming Language Driver Sample Repository C++ libpq C++ libpq samples C# (.NET) Npgsql .NET Npgsql samples Go pgx Go pgx samples Java pgJDBC Java pgJDBC samples Java Aurora DSQL Connector for JDBC JDBC Connector JavaScript node-postgres Node.js postgres samples JavaScript Postgres.js Postgres.js samples Python Psycopg Python Psycopg samples Python Psycopg2 Python Psycopg2 samples Ruby pg Ruby pg samples Rust SQLx Rust SQLx samples"},{"location":"programming-with-dsql.html#object-relational-mapping-orm-libraries","title":"Object-Relational Mapping (ORM) Libraries","text":"<p>Standalone libraries that provide object-relational mapping functionality:</p> Programming Language ORM Library Sample Repository Java Hibernate Hibernate Pet Clinic App Python SQLAlchemy SQLAlchemy Pet Clinic App TypeScript Sequelize TypeScript Sequelize samples TypeScript TypeORM TypeScript TypeORM samples"},{"location":"programming-with-dsql.html#aurora-dsql-adapters-and-dialects","title":"Aurora DSQL Adapters and Dialects","text":"<p>Specific extensions that make existing ORMs work with Aurora DSQL:</p> Programming Language ORM/Framework Repository Java Hibernate Aurora DSQL Hibernate Adapter Python Django Aurora DSQL Django Adapter Python SQLAlchemy Aurora DSQL SQLAlchemy Adapter"},{"location":"programming-with-dsql.html#connection-troubleshooting","title":"Connection Troubleshooting","text":""},{"location":"programming-with-dsql.html#authentication-token-expiration","title":"Authentication Token Expiration","text":"<p>Behavior: Established sessions remain authenticated for a maximum of 1 hour or until an explicit disconnect or client-side timeout occurs.</p> <p>Important Considerations: - If new connections need to be established, a new authentication token must be generated - Opening a new session (listing tables, new SQL console) forces a new authentication attempt - If the authentication token is no longer valid, new sessions will fail and all previously opened sessions become invalid - Plan token duration carefully using the <code>expires-in</code> option (15 minutes default, maximum 7 days)</p>"},{"location":"programming-with-dsql.html#ssl-requirements","title":"SSL Requirements","text":"<p>SSL Mode Support: - <code>PGSSLMODE=require</code>: Basic SSL encryption - <code>PGSSLMODE=verify-full</code>: SSL with certificate verification</p> <p>Important: Aurora DSQL enforces SSL communication on the server side and rejects non-SSL connections. For <code>verify-full</code> option, you need to install SSL certificates locally.</p>"},{"location":"programming-with-dsql.html#related-documentation","title":"Related Documentation","text":"<ul> <li>Authentication Tokens: Generate Authentication Token</li> <li>Database Roles: Database Roles and IAM Authentication</li> <li>Getting Started: Getting Started</li> <li>Troubleshooting: Troubleshooting Overview</li> <li>SSL Certificates: SSL/TLS certificates configuration</li> </ul>"},{"location":"quotas-and-limits.html","title":"Quotas and Limits Overview","text":"\ud83d\udccb Copy Page"},{"location":"quotas-and-limits.html#cluster-quotas-and-database-limits-in-amazon-aurora-dsql","title":"Cluster Quotas and Database Limits in Amazon Aurora DSQL","text":""},{"location":"quotas-and-limits.html#overview","title":"Overview","text":"<p>This guide describes the cluster quotas and database limits for Amazon Aurora DSQL. Understanding these limits is essential for planning your Aurora DSQL deployment and application design.</p>"},{"location":"quotas-and-limits.html#cluster-quotas","title":"Cluster Quotas","text":"<p>Your AWS account has the following cluster quotas in Aurora DSQL. To request an increase to the service quotas for single-Region and multi-Region clusters within a specific AWS Region, use the Service Quotas console page. For other quota increases, contact AWS Support.</p>"},{"location":"quotas-and-limits.html#single-region-clusters","title":"Single-Region Clusters","text":"Quota Default Limit Configurable Error Code Error Message Maximum single-Region clusters per AWS account 20 clusters Yes <code>ServiceQuotaExceededException : 402</code> <code>You have reached the cluster limit.</code>"},{"location":"quotas-and-limits.html#multi-region-clusters","title":"Multi-Region Clusters","text":"Quota Default Limit Configurable Error Code Error Message Maximum multi-Region clusters per AWS account 5 clusters Yes <code>ServiceQuotaExceededException : 402</code> <code>You have reached the cluster limit.</code>"},{"location":"quotas-and-limits.html#storage-quotas","title":"Storage Quotas","text":"Quota Default Limit Configurable Error Code Error Message Maximum storage per cluster 10 TiB (up to 256 TiB with approved increase) Yes <code>DISK_FULL(53100)</code> <code>Current cluster size exceeds cluster size limit.</code>"},{"location":"quotas-and-limits.html#connection-quotas","title":"Connection Quotas","text":"Quota Default Limit Configurable Error Code Error Message Maximum connections per cluster 10,000 connections Yes <code>TOO_MANY_CONNECTIONS(53300)</code> <code>Unable to accept connection, too many open connections.</code> Maximum connection rate per cluster 100 connections per second No <code>CONFIGURED_LIMIT_EXCEEDED(53400)</code> <code>Unable to accept connection, rate exceeded.</code> Maximum connection burst capacity per cluster 1,000 connections No No error code No error message Connection refill rate 100 connections per second No No error code No error message Maximum connection duration 60 minutes No No error code No error message"},{"location":"quotas-and-limits.html#operational-quotas","title":"Operational Quotas","text":"Quota Default Limit Configurable Error Code Error Message Maximum concurrent restore jobs 4 No No error code No error message"},{"location":"quotas-and-limits.html#database-limits","title":"Database Limits","text":"<p>The following table describes the database limits in Aurora DSQL.</p>"},{"location":"quotas-and-limits.html#table-and-column-limits","title":"Table and Column Limits","text":"Limit Default Value Configurable Error Code Error Message Maximum combined size of columns in primary key 1 KiB No <code>54000</code> <code>ERROR: key size too large</code> Maximum combined size of columns in secondary index 1 KiB No <code>54000</code> <code>ERROR: key size too large</code> Maximum size of a row in a table 2 MiB No <code>54000</code> <code>ERROR: maximum row size exceeded</code> Maximum size of a column (not part of index) 1 MiB No <code>54000</code> <code>ERROR: maximum column size exceeded</code> Maximum number of columns in primary key or secondary index 8 No <code>54011</code> <code>ERROR: more than 8 column keys in an index are not supported</code> Maximum number of columns in a table 255 No <code>54011</code> <code>ERROR: tables can have at most 255 columns</code> Maximum number of indexes in a table 24 No <code>54000</code> <code>ERROR: more than 24 indexes per table are not allowed</code>"},{"location":"quotas-and-limits.html#database-structure-limits","title":"Database Structure Limits","text":"Limit Default Value Configurable Error Code Error Message Maximum number of schemas in a database 10 No <code>54000</code> <code>ERROR: more than 10 schemas not allowed</code> Maximum number of tables in a database 1,000 tables No <code>54000</code> <code>ERROR: creating more than 1000 tables not allowed</code> Maximum number of databases in a cluster 1 No No error code <code>ERROR: unsupported statement</code> Maximum number of views in a database 5,000 No <code>54000</code> <code>ERROR: creating more than 5000 views not allowed</code> Maximum view definition size 2 MiB No <code>54000</code> <code>ERROR: view definition too large</code>"},{"location":"quotas-and-limits.html#transaction-limits","title":"Transaction Limits","text":"Limit Default Value Configurable Error Code Error Message Maximum size of all data modified in write transaction 10 MiB No <code>54000</code> <code>ERROR: transaction size limit 10mb exceeded DETAIL: Current transaction size {sizemb} 10mb</code> Maximum number of rows mutated per transaction 3,000 rows No <code>54000</code> <code>ERROR: transaction row limit exceeded</code> Maximum transaction time 5 minutes No <code>54000</code> <code>ERROR: transaction age limit of 300s exceeded</code>"},{"location":"quotas-and-limits.html#memory-limits","title":"Memory Limits","text":"Limit Default Value Configurable Error Code Error Message Maximum base memory per query operation 128 MiB per transaction No <code>53200</code> <code>ERROR: query requires too much temp space, out of memory.</code>"},{"location":"quotas-and-limits.html#quota-management","title":"Quota Management","text":""},{"location":"quotas-and-limits.html#requesting-quota-increases","title":"Requesting Quota Increases","text":"<p>Service Quotas Console: Use the Service Quotas console to request increases for: - Single-Region cluster limits - Multi-Region cluster limits - Storage limits per cluster - Connection limits per cluster</p> <p>AWS Support: Contact AWS Support for other quota increases not available through Service Quotas console.</p>"},{"location":"quotas-and-limits.html#monitoring-quota-usage","title":"Monitoring Quota Usage","text":"<p>Best Practices: 1. Monitor cluster count against account limits 2. Track storage usage per cluster 3. Monitor connection patterns to avoid rate limits 4. Plan capacity based on application requirements</p>"},{"location":"quotas-and-limits.html#planning-considerations","title":"Planning Considerations","text":""},{"location":"quotas-and-limits.html#application-design","title":"Application Design","text":"<ul> <li>Transaction Size: Keep transactions under 10 MiB data modification limit</li> <li>Row Mutations: Limit to 3,000 rows per transaction</li> <li>Connection Management: Plan for 10,000 connection limit per cluster</li> <li>Schema Design: Consider 10 schema limit per database</li> </ul>"},{"location":"quotas-and-limits.html#performance-planning","title":"Performance Planning","text":"<ul> <li>Query Memory: Design queries to stay within 128 MiB memory limit</li> <li>Transaction Duration: Keep transactions under 5-minute limit</li> <li>Index Strategy: Plan for maximum 24 indexes per table</li> <li>Table Structure: Consider 255 column limit per table</li> </ul>"},{"location":"quotas-and-limits.html#scalability-planning","title":"Scalability Planning","text":"<ul> <li>Multi-Region Strategy: Plan for 5 multi-Region cluster limit</li> <li>Storage Growth: Plan for 10 TiB default storage limit</li> <li>Connection Scaling: Consider connection rate limits (100/second)</li> <li>View Management: Plan for 5,000 view limit per database</li> </ul>"},{"location":"quotas-and-limits.html#error-code-reference","title":"Error Code Reference","text":""},{"location":"quotas-and-limits.html#connection-error-codes","title":"Connection Error Codes","text":"<ul> <li>53300: <code>TOO_MANY_CONNECTIONS</code> - Exceeded connection limit</li> <li>53400: <code>CONFIGURED_LIMIT_EXCEEDED</code> - Exceeded connection rate</li> <li>53100: <code>DISK_FULL</code> - Exceeded storage limit</li> </ul>"},{"location":"quotas-and-limits.html#database-error-codes","title":"Database Error Codes","text":"<ul> <li>54000: General database limit exceeded</li> <li>54011: Column or key limit exceeded</li> <li>53200: Memory limit exceeded</li> </ul>"},{"location":"quotas-and-limits.html#api-error-codes","title":"API Error Codes","text":"<ul> <li>402: <code>ServiceQuotaExceededException</code> - Service quota exceeded</li> </ul>"},{"location":"quotas-and-limits.html#related-documentation","title":"Related Documentation","text":"<ul> <li>PostgreSQL Compatibility: Supported PostgreSQL features</li> <li>Data Types: Supported data types</li> <li>Systems Tables: Using systems tables and commands</li> </ul>"},{"location":"single-region-clusters.html","title":"Single-Region Clusters","text":"\ud83d\udccb Copy Page"},{"location":"single-region-clusters.html#configuring-single-region-clusters","title":"Configuring Single-Region Clusters","text":""},{"location":"single-region-clusters.html#overview","title":"Overview","text":"<p>Learn how to manage your clusters using the AWS SDKs and CLI. Configure and manage clusters for an AWS Region using either the CLI or your preferred programming language including Python, C++, JavaScript, Java, Rust, Ruby, .NET, and Golang.</p> <p>The CLI provides quick access through shell commands, while AWS Software Development Kits (SDKs) enable programmatic control through native language support.</p>"},{"location":"single-region-clusters.html#using-aws-sdks","title":"Using AWS SDKs","text":"<p>The AWS SDKs provide programmatic access to Aurora DSQL in your preferred programming language. The following sections show how to perform common cluster operations using different programming languages.</p>"},{"location":"single-region-clusters.html#create-cluster","title":"Create Cluster","text":"<p>The following examples show how to create a single-Region cluster using different programming languages.</p>"},{"location":"single-region-clusters.html#python-sdk","title":"Python SDK","text":"<pre><code>import boto3\n\ndef create_cluster(region):\n    try:\n        client = boto3.client(\"dsql\", region_name=region)\n        tags = {\"Name\": \"Python single region cluster\"}\n        cluster = client.create_cluster(tags=tags, deletionProtectionEnabled=True)\n        print(f\"Initiated creation of cluster: {cluster['identifier']}\")\n\n        print(f\"Waiting for {cluster['arn']} to become ACTIVE\")\n        client.get_waiter(\"cluster_active\").wait(\n            identifier=cluster[\"identifier\"],\n            WaiterConfig={\n                'Delay': 10,\n                'MaxAttempts': 30\n            }\n        )\n\n        return cluster\n    except:\n        print(\"Unable to create cluster\")\n        raise\n\ndef main():\n    region = \"us-east-1\"\n    response = create_cluster(region)\n    print(f\"Created cluster: {response['arn']}\")\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"single-region-clusters.html#javascript-sdk","title":"JavaScript SDK","text":"<pre><code>import { DSQLClient, CreateClusterCommand, waitUntilClusterActive } from \"@aws-sdk/client-dsql\";\n\nasync function createCluster(region) {\n    const client = new DSQLClient({ region });\n\n    try {\n        const createClusterCommand = new CreateClusterCommand({\n            deletionProtectionEnabled: true,\n            tags: {\n                Name: \"javascript single region cluster\"\n            },\n        });\n        const response = await client.send(createClusterCommand);\n\n        console.log(`Waiting for cluster ${response.identifier} to become ACTIVE`);\n        await waitUntilClusterActive(\n            {\n                client: client,\n                maxWaitTime: 300 // Wait for 5 minutes\n            },\n            {\n                identifier: response.identifier\n            }\n        );\n        console.log(`Cluster Id ${response.identifier} is now active`);\n        return;\n    } catch (error) {\n        console.error(`Unable to create cluster in ${region}: `, error.message);\n        throw error;\n    }\n}\n\nasync function main() {\n    const region = \"us-east-1\";\n    await createCluster(region);\n}\n\nmain();\n</code></pre>"},{"location":"single-region-clusters.html#java-sdk","title":"Java SDK","text":"<pre><code>import software.amazon.awssdk.auth.credentials.DefaultCredentialsProvider;\nimport software.amazon.awssdk.core.waiters.WaiterResponse;\nimport software.amazon.awssdk.regions.Region;\nimport software.amazon.awssdk.services.dsql.DsqlClient;\nimport software.amazon.awssdk.services.dsql.model.CreateClusterRequest;\nimport software.amazon.awssdk.services.dsql.model.CreateClusterResponse;\nimport software.amazon.awssdk.services.dsql.model.GetClusterResponse;\n\nimport java.time.Duration;\nimport java.util.Map;\n\npublic class CreateCluster {\n    public static void main(String[] args) {\n        Region region = Region.US_EAST_1;\n\n        try (DsqlClient client = DsqlClient.builder()\n                .region(region)\n                .credentialsProvider(DefaultCredentialsProvider.create())\n                .build()) {\n\n            CreateClusterRequest request = CreateClusterRequest.builder()\n                    .deletionProtectionEnabled(true)\n                    .tags(Map.of(\"Name\", \"java single region cluster\"))\n                    .build();\n            CreateClusterResponse cluster = client.createCluster(request);\n            System.out.println(\"Created \" + cluster.arn());\n\n            System.out.println(\"Waiting for cluster to become ACTIVE\");\n            WaiterResponse&lt;GetClusterResponse&gt; waiterResponse = client.waiter().waitUntilClusterActive(\n                    getCluster -&gt; getCluster.identifier(cluster.identifier()),\n                    config -&gt; config.waitTimeout(Duration.ofMinutes(5))\n            );\n            waiterResponse.matched().response().ifPresent(System.out::println);\n        }\n    }\n}\n</code></pre>"},{"location":"single-region-clusters.html#get-cluster-information","title":"Get Cluster Information","text":"<p>The following examples show how to get information about a single-Region cluster.</p>"},{"location":"single-region-clusters.html#python-sdk_1","title":"Python SDK","text":"<pre><code>import boto3\nfrom datetime import datetime\nimport json\n\ndef get_cluster(region, identifier):\n    try:\n        client = boto3.client(\"dsql\", region_name=region)\n        return client.get_cluster(identifier=identifier)\n    except:\n        print(f\"Unable to get cluster {identifier} in region {region}\")\n        raise\n\ndef main():\n    region = \"us-east-1\"\n    cluster_id = \"&lt;your cluster id&gt;\"\n    response = get_cluster(region, cluster_id)\n    print(json.dumps(response, indent=2, default=lambda obj: obj.isoformat() if isinstance(obj, datetime) else None))\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"single-region-clusters.html#javascript-sdk_1","title":"JavaScript SDK","text":"<pre><code>import { DSQLClient, GetClusterCommand } from \"@aws-sdk/client-dsql\";\n\nasync function getCluster(region, clusterId) {\n    const client = new DSQLClient({ region });\n\n    const getClusterCommand = new GetClusterCommand({\n        identifier: clusterId,\n    });\n\n    try {\n        return await client.send(getClusterCommand);\n    } catch (error) {\n        if (error.name === \"ResourceNotFoundException\") {\n            console.log(\"Cluster ID not found or deleted\");\n        }\n        throw error;\n    }\n}\n\nasync function main() {\n    const region = \"us-east-1\";\n    const clusterId = \"&lt;CLUSTER_ID&gt;\";\n\n    const response = await getCluster(region, clusterId);\n    console.log(\"Cluster: \", response);\n}\n\nmain();\n</code></pre>"},{"location":"single-region-clusters.html#update-cluster","title":"Update Cluster","text":"<p>The following examples show how to update a single-Region cluster.</p>"},{"location":"single-region-clusters.html#python-sdk_2","title":"Python SDK","text":"<pre><code>import boto3\n\ndef update_cluster(region, cluster_id, deletion_protection_enabled):\n    try:\n        client = boto3.client(\"dsql\", region_name=region)\n        return client.update_cluster(identifier=cluster_id, deletionProtectionEnabled=deletion_protection_enabled)\n    except:\n        print(\"Unable to update cluster\")\n        raise\n\ndef main():\n    region = \"us-east-1\"\n    cluster_id = \"&lt;your cluster id&gt;\"\n    deletion_protection_enabled = False\n    response = update_cluster(region, cluster_id, deletion_protection_enabled)\n    print(f\"Updated {response['arn']} with deletion_protection_enabled: {deletion_protection_enabled}\")\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"single-region-clusters.html#delete-cluster","title":"Delete Cluster","text":"<p>The following examples show how to delete a single-Region cluster.</p>"},{"location":"single-region-clusters.html#python-sdk_3","title":"Python SDK","text":"<pre><code>import boto3\n\ndef delete_cluster(region, identifier):\n    try:\n        client = boto3.client(\"dsql\", region_name=region)\n        cluster = client.delete_cluster(identifier=identifier)\n        print(f\"Initiated delete of {cluster['arn']}\")\n\n        print(\"Waiting for cluster to finish deletion\")\n        client.get_waiter(\"cluster_not_exists\").wait(\n            identifier=cluster[\"identifier\"],\n            WaiterConfig={\n                'Delay': 10,\n                'MaxAttempts': 30\n            }\n        )\n    except:\n        print(\"Unable to delete cluster \" + identifier)\n        raise\n\ndef main():\n    region = \"us-east-1\"\n    cluster_id = \"&lt;cluster id&gt;\"\n    delete_cluster(region, cluster_id)\n    print(f\"Deleted {cluster_id}\")\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"single-region-clusters.html#using-aws-cli","title":"Using AWS CLI","text":"<p>The AWS CLI provides a command-line interface for managing your Aurora DSQL clusters. The following examples demonstrate common cluster management operations.</p>"},{"location":"single-region-clusters.html#create-cluster_1","title":"Create Cluster","text":"<p>Create a cluster using the <code>create-cluster</code> command.</p> <p>Note: Cluster creation is an asynchronous operation. Call the <code>GetCluster</code> API until the status changes to <code>ACTIVE</code>. You can connect to your cluster after it becomes active.</p> <p>Command: <pre><code>aws dsql create-cluster --region us-east-1\n</code></pre></p> <p>Note: To disable deletion protection during creation, include the <code>--no-deletion-protection-enabled</code> flag.</p> <p>Response: <pre><code>{\n    \"identifier\": \"abc0def1baz2quux3quuux4\",\n    \"arn\": \"arn:aws:dsql:us-east-1:111122223333:cluster/abc0def1baz2quux3quuux4\",\n    \"status\": \"CREATING\",\n    \"creationTime\": \"2024-05-25T16:56:49.784000-07:00\",\n    \"deletionProtectionEnabled\": true,\n    \"tag\": {},\n    \"encryptionDetails\": {\n        \"encryptionType\": \"AWS_OWNED_KMS_KEY\",\n        \"encryptionStatus\": \"ENABLED\"\n    }\n}\n</code></pre></p>"},{"location":"single-region-clusters.html#get-cluster-information_1","title":"Get Cluster Information","text":"<p>Get information about a cluster using the <code>get-cluster</code> command.</p> <p>Command: <pre><code>aws dsql get-cluster \\\n  --region us-east-1 \\\n  --identifier your_cluster_id\n</code></pre></p> <p>Response: <pre><code>{\n    \"identifier\": \"abc0def1baz2quux3quuux4\",\n    \"arn\": \"arn:aws:dsql:us-east-1:111122223333:cluster/abc0def1baz2quux3quuux4\",\n    \"status\": \"ACTIVE\",\n    \"creationTime\": \"2024-11-27T00:32:14.434000-08:00\",\n    \"deletionProtectionEnabled\": false,\n    \"encryptionDetails\": {\n        \"encryptionType\": \"CUSTOMER_MANAGED_KMS_KEY\",\n        \"kmsKeyArn\": \"arn:aws:kms:us-east-1:111122223333:key/123a456b-c789-01de-2f34-g5hi6j7k8lm9\",\n        \"encryptionStatus\": \"ENABLED\"\n    }\n}\n</code></pre></p>"},{"location":"single-region-clusters.html#update-cluster_1","title":"Update Cluster","text":"<p>Update an existing cluster using the <code>update-cluster</code> command.</p> <p>Note: Updates are asynchronous operations. Call the <code>GetCluster</code> API until the status changes to <code>ACTIVE</code> to see your changes.</p> <p>Command: <pre><code>aws dsql update-cluster \\\n  --region us-east-1 \\\n  --no-deletion-protection-enabled \\\n  --identifier your_cluster_id\n</code></pre></p> <p>Response: <pre><code>{\n    \"identifier\": \"abc0def1baz2quux3quuux4\",\n    \"arn\": \"arn:aws:dsql:us-east-1:111122223333:cluster/abc0def1baz2quux3quuux4\",\n    \"status\": \"UPDATING\",\n    \"creationTime\": \"2024-05-24T09:15:32.708000-07:00\"\n}\n</code></pre></p>"},{"location":"single-region-clusters.html#delete-cluster_1","title":"Delete Cluster","text":"<p>Delete an existing cluster using the <code>delete-cluster</code> command.</p> <p>Note: You can only delete clusters that have deletion protection disabled. By default, deletion protection is enabled when you create new clusters.</p> <p>Command: <pre><code>aws dsql delete-cluster \\\n  --region us-east-1 \\\n  --identifier your_cluster_id\n</code></pre></p> <p>Response: <pre><code>{\n    \"identifier\": \"abc0def1baz2quux3quuux4\",\n    \"arn\": \"arn:aws:dsql:us-east-1:111122223333:cluster/abc0def1baz2quux3quuux4\",\n    \"status\": \"DELETING\",\n    \"creationTime\": \"2024-05-24T09:16:43.778000-07:00\"\n}\n</code></pre></p>"},{"location":"single-region-clusters.html#list-clusters","title":"List Clusters","text":"<p>List your clusters using the <code>list-clusters</code> command.</p> <p>Command: <pre><code>aws dsql list-clusters --region us-east-1\n</code></pre></p> <p>Response: <pre><code>{\n    \"clusters\": [\n        {\n            \"identifier\": \"abc0def1baz2quux3quux4quuux\",\n            \"arn\": \"arn:aws:dsql:us-east-1:111122223333:cluster/abc0def1baz2quux3quux4quuux\"\n        },\n        {\n            \"identifier\": \"abc0def1baz2quux3quux5quuuux\",\n            \"arn\": \"arn:aws:dsql:us-east-1:111122223333:cluster/abc0def1baz2quux3quux5quuuux\"\n        }\n    ]\n}\n</code></pre></p>"},{"location":"single-region-clusters.html#additional-resources","title":"Additional Resources","text":"<p>For more code samples and examples, visit the Aurora DSQL Samples GitHub repository.</p>"},{"location":"single-region-clusters.html#related-documentation","title":"Related Documentation","text":"<ul> <li>Getting Started: Getting Started</li> <li>Multi-Region Clusters: Multi-Region cluster management</li> <li>Authentication: Auth &amp; Access Overview</li> <li>Troubleshooting: Troubleshooting Overview</li> </ul>"},{"location":"troubleshooting.html","title":"Troubleshooting Overview","text":"\ud83d\udccb Copy Page"},{"location":"troubleshooting.html#troubleshooting-amazon-aurora-dsql","title":"Troubleshooting Amazon Aurora DSQL","text":""},{"location":"troubleshooting.html#overview","title":"Overview","text":"<p>This guide provides troubleshooting advice for common errors and issues when using Amazon Aurora DSQL. If you encounter an issue not listed here, contact AWS support.</p>"},{"location":"troubleshooting.html#connection-errors","title":"Connection Errors","text":""},{"location":"troubleshooting.html#ssl-error-code-6","title":"SSL Error Code 6","text":"<p>Error Message: <code>error: unrecognized SSL error code: 6</code> or <code>unable to accept connection, sni was not received</code></p> <p>Root Cause: PostgreSQL client version earlier than 14 lacks Server Name Indication (SNI) support, which is required for Aurora DSQL connections.</p> <p>Resolution: 1. Check your client version: <code>psql --version</code> 2. Upgrade PostgreSQL client to version 14 or later 3. Retry the connection</p>"},{"location":"troubleshooting.html#network-unreachable-error","title":"Network Unreachable Error","text":"<p>Error Message: <code>error: NetworkUnreachable</code></p> <p>Root Cause: Client doesn't support IPv6 connections on dual-stack server configuration.</p> <p>Technical Details: When a server supports dual-stack mode, clients first resolve hostnames to both IPv4 and IPv6 addresses. They attempt IPv4 connection first, then IPv6 if initial connection fails. IPv4-only systems show generic NetworkUnreachable error instead of clear \"IPv6 not supported\" message.</p> <p>Resolution: Ensure IPv6 support is available or use IPv4-only endpoint if provided.</p>"},{"location":"troubleshooting.html#authentication-errors","title":"Authentication Errors","text":""},{"location":"troubleshooting.html#iam-authentication-failed","title":"IAM Authentication Failed","text":"<p>Error Message: <code>IAM authentication failed for user \"...\"</code></p> <p>Root Cause: Authentication token or IAM role has expired.</p> <p>Common Scenarios: - Authentication token exceeded maximum duration (1 week) - Temporary IAM role expired before connection attempt - IAM role credentials no longer valid</p> <p>Resolution: 1. Generate new authentication token 2. Verify IAM role is still valid and accessible 3. Check IAM role expiration time 4. Retry connection with fresh credentials</p> <p>Related Documentation: Authentication and authorization guide</p>"},{"location":"troubleshooting.html#invalid-access-key-id","title":"Invalid Access Key ID","text":"<p>Error Message: <code>An error occurred (InvalidAccessKeyId) when calling the GetObject operation: The AWS Access Key ID you provided does not exist in our records</code></p> <p>Root Cause: IAM credential validation failure.</p> <p>Resolution: 1. Verify AWS Access Key ID is correct 2. Check if credentials have been rotated or deleted 3. Ensure credentials are properly configured in your environment</p> <p>Related Documentation: Why requests are signed</p>"},{"location":"troubleshooting.html#iam-role-not-found","title":"IAM Role Not Found","text":"<p>Error Message: <code>IAM role &lt;role&gt; does not exist</code></p> <p>Root Cause: Aurora DSQL cannot locate the specified IAM role.</p> <p>Resolution: 1. Verify IAM role name and ARN are correct 2. Check if role exists in the correct AWS account 3. Confirm role hasn't been deleted or renamed</p> <p>Related Documentation: IAM roles</p>"},{"location":"troubleshooting.html#invalid-iam-arn-format","title":"Invalid IAM ARN Format","text":"<p>Error Message: <code>IAM role must look like an IAM ARN</code></p> <p>Root Cause: IAM role ARN format is incorrect.</p> <p>Resolution: 1. Verify ARN follows correct format: <code>arn:aws:iam::account-id:role/role-name</code> 2. Check for typos in ARN string 3. Ensure proper ARN structure and syntax</p> <p>Related Documentation: IAM ARN format</p>"},{"location":"troubleshooting.html#authorization-errors","title":"Authorization Errors","text":""},{"location":"troubleshooting.html#role-not-supported","title":"Role Not Supported","text":"<p>Error Message: <code>Role &lt;role&gt; not supported</code></p> <p>Root Cause: Aurora DSQL doesn't support certain PostgreSQL GRANT operations.</p> <p>Resolution: Review supported PostgreSQL commands and use alternative approaches.</p> <p>Related Documentation: Supported PostgreSQL commands</p>"},{"location":"troubleshooting.html#cannot-establish-trust-with-role","title":"Cannot Establish Trust with Role","text":"<p>Error Message: <code>Cannot establish trust with role &lt;role&gt;</code></p> <p>Root Cause: Aurora DSQL doesn't support certain PostgreSQL GRANT operations.</p> <p>Resolution: Use Aurora DSQL-specific role management commands instead of standard PostgreSQL GRANT operations.</p> <p>Related Documentation: Supported PostgreSQL commands</p>"},{"location":"troubleshooting.html#database-role-does-not-exist","title":"Database Role Does Not Exist","text":"<p>Error Message: <code>Role &lt;role&gt; does not exist</code></p> <p>Root Cause: Aurora DSQL cannot find the specified database user role.</p> <p>Resolution: 1. Verify the database role was created properly 2. Check role name spelling and case sensitivity 3. Ensure role was created with proper permissions</p> <p>Related Documentation: Custom database roles</p>"},{"location":"troubleshooting.html#permission-denied-for-iam-trust","title":"Permission Denied for IAM Trust","text":"<p>Error Message: <code>ERROR: permission denied to grant IAM trust with role &lt;role&gt;</code></p> <p>Root Cause: Must be connected with admin role to grant access to database roles.</p> <p>Resolution: 1. Connect to cluster using admin role 2. Verify you have <code>dsql:DbConnectAdmin</code> permission 3. Retry the grant operation</p> <p>Related Documentation: Database role authorization</p>"},{"location":"troubleshooting.html#role-missing-login-attribute","title":"Role Missing LOGIN Attribute","text":"<p>Error Message: <code>ERROR: role &lt;role&gt; must have the LOGIN attribute</code></p> <p>Root Cause: Database roles must have LOGIN permission to be used for connections.</p> <p>Resolution: 1. Create role with LOGIN permission: <code>CREATE ROLE example WITH LOGIN;</code> 2. Or modify existing role: <code>ALTER ROLE example WITH LOGIN;</code></p> <p>Related Documentation:  - CREATE ROLE - ALTER ROLE</p>"},{"location":"troubleshooting.html#cannot-drop-role-with-dependencies","title":"Cannot Drop Role with Dependencies","text":"<p>Error Message: <code>ERROR: role &lt;role&gt; cannot be dropped because some objects depend on it</code></p> <p>Root Cause: Database role has active IAM relationship that must be revoked first.</p> <p>Resolution: 1. Revoke IAM relationship: <code>AWS IAM REVOKE example FROM 'arn:aws:iam::account:role/role-name';</code> 2. Then drop the database role 3. Verify no other dependencies exist</p> <p>Related Documentation: Revoking authorization</p>"},{"location":"troubleshooting.html#sql-errors","title":"SQL Errors","text":""},{"location":"troubleshooting.html#feature-not-supported","title":"Feature Not Supported","text":"<p>Error Message: <code>Error: Not supported</code></p> <p>Root Cause: Attempted to use PostgreSQL feature not supported in Aurora DSQL.</p> <p>Resolution: 1. Check Aurora DSQL feature compatibility documentation 2. Use supported alternative commands or approaches 3. Review PostgreSQL compatibility guide</p> <p>Related Documentation: Supported PostgreSQL features</p>"},{"location":"troubleshooting.html#index-creation-error","title":"Index Creation Error","text":"<p>Error Message: <code>Error: use CREATE INDEX ASYNC instead</code></p> <p>Root Cause: Creating indexes on tables with existing data requires asynchronous operation.</p> <p>Resolution: 1. Use <code>CREATE INDEX ASYNC</code> command instead of <code>CREATE INDEX</code> 2. Monitor index creation progress 3. Wait for completion before using index</p> <p>Related Documentation: Asynchronous index creation</p>"},{"location":"troubleshooting.html#concurrency-control-errors","title":"Concurrency Control Errors","text":""},{"location":"troubleshooting.html#mutation-conflicts","title":"Mutation Conflicts","text":"<p>Error Message: <code>OC000 \"ERROR: mutation conflicts with another transaction, retry as needed\"</code></p> <p>Root Cause: Transaction attempted to modify same data as concurrent transaction.</p> <p>Technical Details: Indicates contention on modified tuples between concurrent transactions.</p> <p>Resolution: 1. Implement retry logic in application 2. Add exponential backoff for retries 3. Consider reducing transaction scope to minimize conflicts</p> <p>Related Documentation: Concurrency control</p>"},{"location":"troubleshooting.html#schema-update-conflicts","title":"Schema Update Conflicts","text":"<p>Error Message: <code>OC001 \"ERROR: schema has been updated by another transaction, retry as needed\"</code></p> <p>Root Cause: Session catalog cache became outdated due to concurrent schema changes.</p> <p>Technical Process: 1. Session loaded catalog version V1 at time T1 2. Another transaction updated catalog to V2 at time T2 3. Original session attempted storage read with outdated V1 catalog 4. Storage layer rejected request due to version mismatch</p> <p>Resolution: 1. Retry the transaction (Aurora DSQL will refresh catalog cache) 2. New transaction will use updated catalog version 3. Ensure no additional schema changes occur during retry</p>"},{"location":"troubleshooting.html#ssltls-connection-errors","title":"SSL/TLS Connection Errors","text":""},{"location":"troubleshooting.html#certificate-verification-failed","title":"Certificate Verification Failed","text":"<p>Error Message: <code>SSL error: certificate verify failed</code></p> <p>Root Cause: Client cannot verify server certificate.</p> <p>Resolution: 1. Install Amazon Root CA 1 certificate properly 2. Set <code>PGSSLROOTCERT</code> environment variable to correct certificate file 3. Verify certificate file has correct permissions 4. Retry connection</p>"},{"location":"troubleshooting.html#unrecognized-ssl-error-code","title":"Unrecognized SSL Error Code","text":"<p>Error Message: <code>Unrecognized SSL error code: 6</code></p> <p>Root Cause: PostgreSQL client version below 14 lacks proper SSL support.</p> <p>Resolution: Upgrade PostgreSQL client to version 17 or later.</p>"},{"location":"troubleshooting.html#ssl-unregistered-scheme-windows","title":"SSL Unregistered Scheme (Windows)","text":"<p>Error Message: <code>SSL error: unregistered scheme (Windows)</code></p> <p>Root Cause: Known issue with Windows psql client using system certificates.</p> <p>Resolution: Use downloaded certificate file method for Windows connections instead of system certificates.</p>"},{"location":"what-is-amazon-aurora-dsql.html","title":"What is Amazon Aurora DSQL?","text":"\ud83d\udccb Copy Page"},{"location":"what-is-amazon-aurora-dsql.html#what-is-amazon-aurora-dsql","title":"What is Amazon Aurora DSQL?","text":""},{"location":"what-is-amazon-aurora-dsql.html#overview","title":"Overview","text":"<p>Amazon Aurora DSQL is a serverless, distributed relational database service optimized for transactional workloads. Amazon Aurora DSQL offers virtually unlimited scale and doesn't require you to manage infrastructure. The active-active highly available architecture provides 99.99% single-Region and 99.999% multi-Region availability.</p>"},{"location":"what-is-amazon-aurora-dsql.html#when-to-use-amazon-aurora-dsql","title":"When to Use Amazon Aurora DSQL","text":"<p>Aurora DSQL is optimized for transactional workloads that benefit from ACID transactions and a relational data model. Because it's serverless, Aurora DSQL is ideal for application patterns of microservice, serverless, and event-driven architectures. Aurora DSQL is PostgreSQL-compatible, so you can use familiar drivers, object-relational mappings (ORMs), frameworks, and SQL features.</p> <p>Aurora DSQL automatically manages system infrastructure and scales compute, I/O, and storage based on your workload. Because you have no servers to provision or manage, you don't have to worry about maintenance downtime related to provisioning, patching, or infrastructure upgrades.</p> <p>Aurora DSQL helps you to build and maintain enterprise applications that are always available at any scale. The active-active serverless design automates failure recovery, so you don't need to worry about traditional database failover. Your applications benefit from Multi-AZ and multi-Region availability, and you don't have to be concerned about eventual consistency or missing data related to failovers.</p>"},{"location":"what-is-amazon-aurora-dsql.html#key-features-in-amazon-aurora-dsql","title":"Key Features in Amazon Aurora DSQL","text":""},{"location":"what-is-amazon-aurora-dsql.html#distributed-architecture","title":"Distributed Architecture","text":"<p>Amazon Aurora DSQL is composed of the following multi-tenant components:</p> <ol> <li>Relay and connectivity</li> <li>Compute and databases</li> <li>Transaction log, concurrency control, and isolation</li> <li>Storage</li> </ol> <p>A control plane coordinates these components. Each component provides redundancy across three Availability Zones (AZs), with: - Automatic cluster scaling - Self-healing in case of component failures</p>"},{"location":"what-is-amazon-aurora-dsql.html#single-region-and-multi-region-clusters","title":"Single-Region and Multi-Region Clusters","text":"<p>Amazon Aurora DSQL clusters provide the following benefits:</p> <ul> <li>Synchronous data replication</li> <li>Consistent read operations</li> <li>Automatic failure recovery</li> <li>Data consistency across multiple AZs or Regions</li> </ul>"},{"location":"what-is-amazon-aurora-dsql.html#failure-recovery","title":"Failure Recovery","text":"<p>If an infrastructure component fails, Amazon Aurora DSQL automatically routes requests to healthy infrastructure without manual intervention. Amazon Aurora DSQL provides atomicity, consistency, isolation, and durability (ACID) transactions with: - Strong consistency - Snapshot isolation - Atomicity - Cross-AZ and cross-Region durability</p>"},{"location":"what-is-amazon-aurora-dsql.html#multi-region-capabilities","title":"Multi-Region Capabilities","text":"<p>Multi-Region peered clusters provide the same resilience and connectivity as single-Region clusters. But they improve availability by offering: - Two Regional endpoints (one in each peered cluster Region) - Both endpoints present a single logical database - Available for concurrent read and write operations - Strong data consistency</p> <p>You can build applications that run in multiple Regions at the same time for performance and resilience\u2014and know that readers always see the same data.</p>"},{"location":"what-is-amazon-aurora-dsql.html#compatibility-with-postgresql-databases","title":"Compatibility with PostgreSQL Databases","text":"<p>The distributed database layer (compute) in Amazon Aurora DSQL is based on a current major version of PostgreSQL. You can connect to Amazon Aurora DSQL with familiar PostgreSQL drivers and tools, such as <code>psql</code>.</p>"},{"location":"what-is-amazon-aurora-dsql.html#version-compatibility","title":"Version Compatibility","text":"<ul> <li>Amazon Aurora DSQL is currently compatible with PostgreSQL version 16</li> <li>Supports a subset of PostgreSQL features, expressions, and data types</li> </ul>"},{"location":"what-is-amazon-aurora-dsql.html#technical-specifications","title":"Technical Specifications","text":""},{"location":"what-is-amazon-aurora-dsql.html#availability-guarantees","title":"Availability Guarantees","text":"<ul> <li>Single-Region: 99.99% availability</li> <li>Multi-Region: 99.999% availability</li> </ul>"},{"location":"what-is-amazon-aurora-dsql.html#architecture-benefits","title":"Architecture Benefits","text":"<ul> <li>Serverless: No infrastructure management required</li> <li>Distributed: Built for high availability and scalability</li> <li>Active-Active: Highly available architecture</li> <li>ACID Compliant: Full transactional consistency</li> <li>PostgreSQL Compatible: Use familiar tools and syntax</li> </ul>"},{"location":"what-is-amazon-aurora-dsql.html#scaling-characteristics","title":"Scaling Characteristics","text":"<ul> <li>Virtually unlimited scale</li> <li>Automatic scaling of compute, I/O, and storage</li> <li>Multi-AZ redundancy</li> <li>Multi-Region support</li> </ul>"},{"location":"what-is-amazon-aurora-dsql.html#region-availability-for-amazon-aurora-dsql","title":"Region Availability for Amazon Aurora DSQL","text":"<p>With Amazon Aurora DSQL, you can deploy database instances across multiple AWS Regions to support global applications and meet data residency requirements. Region availability determines where you can create and manage Aurora DSQL database clusters. Database administrators and application architects who need to design highly available, globally distributed database systems often need to understand Region support for their workloads. Common use cases include setting up cross-Region disaster recovery, serving users from geographically closer database instances to reduce latency, and maintaining data copies in specific locations for compliance.</p> <p>The following table shows the AWS Regions where Aurora DSQL is currently available and the endpoint for each AWS Region:</p>"},{"location":"what-is-amazon-aurora-dsql.html#supported-aws-regions","title":"Supported AWS Regions","text":"Region Name Region Code Endpoint Protocol US East (Ohio) us-east-2 dsql.us-east-2.api.awsdsql-fips.us-east-2.api.aws HTTPSHTTPS US East (N. Virginia) us-east-1 dsql.us-east-1.api.awsdsql-fips.us-east-1.api.aws HTTPSHTTPS US West (Oregon) us-west-2 dsql.us-west-2.api.awsdsql-fips.us-west-2.api.aws HTTPSHTTPS Asia Pacific (Osaka) ap-northeast-3 dsql.ap-northeast-3.api.aws HTTPS Asia Pacific (Seoul) ap-northeast-2 dsql.ap-northeast-2.api.aws HTTPS Asia Pacific (Tokyo) ap-northeast-1 dsql.ap-northeast-1.api.aws HTTPS Europe (Frankfurt) eu-central-1 dsql.eu-central-1.api.aws HTTPS Europe (Ireland) eu-west-1 dsql.eu-west-1.api.aws HTTPS Europe (London) eu-west-2 dsql.eu-west-2.api.aws HTTPS Europe (Paris) eu-west-3 dsql.eu-west-3.api.aws HTTPS"},{"location":"what-is-amazon-aurora-dsql.html#multi-region-cluster-availability-for-amazon-aurora-dsql","title":"Multi-Region Cluster Availability for Amazon Aurora DSQL","text":"<p>You can create Aurora DSQL multi-Region clusters within specific AWS Region sets. Each Region set groups geographically related Regions that can work together in a multi-Region cluster.</p>"},{"location":"what-is-amazon-aurora-dsql.html#us-regions","title":"US Regions","text":"<ul> <li>US East (N. Virginia)</li> <li>US East (Ohio)</li> <li>US West (Oregon)</li> </ul>"},{"location":"what-is-amazon-aurora-dsql.html#asia-pacific-regions","title":"Asia Pacific Regions","text":"<ul> <li>Asia Pacific (Osaka)</li> <li>Asia Pacific (Seoul)</li> <li>Asia Pacific (Tokyo)</li> </ul>"},{"location":"what-is-amazon-aurora-dsql.html#european-regions","title":"European Regions","text":"<ul> <li>Europe (Frankfurt)</li> <li>Europe (Ireland)</li> <li>Europe (London)</li> <li>Europe (Paris)</li> </ul>"},{"location":"what-is-amazon-aurora-dsql.html#important-limitations","title":"Important Limitations","text":"<p>Multi-Region clusters must be created within a single Region set. For example, you can't create a cluster that includes both US East (N. Virginia) and Europe (Ireland) Regions.</p> <p>Important: Aurora DSQL currently doesn't support cross-continent multi-Region clusters.</p>"},{"location":"what-is-amazon-aurora-dsql.html#pricing","title":"Pricing","text":"<p>For cost information, see Amazon Aurora DSQL pricing.</p>"},{"location":"what-is-amazon-aurora-dsql.html#next-steps","title":"Next Steps","text":"<p>For information about the core components in Amazon Aurora DSQL and to get started with the service, see the following:</p> <ol> <li>Getting Started - Complete guide to creating your first Aurora DSQL cluster</li> <li>PostgreSQL Compatibility - Detailed information about supported SQL features</li> <li>Accessing Aurora DSQL - Methods for connecting to your clusters with PostgreSQL-compatible clients</li> <li>Working with Aurora DSQL - Advanced cluster management and operations</li> </ol>"},{"location":"what-is-amazon-aurora-dsql.html#cross-references","title":"Cross-References","text":"<ul> <li>High Availability Architecture: Learn more about how the distributed architecture supports high availability</li> <li>SQL Feature Compatibility: Understand the subset of PostgreSQL features, expressions, and data types supported</li> </ul>"},{"location":"generative-ai/jupyterlab.html","title":"Using JupyterLab with Aurora DSQL","text":"\ud83d\udccb Copy Page"},{"location":"generative-ai/jupyterlab.html#query-editors-using-jupyterlab-with-aurora-dsql","title":"Query Editors: Using JupyterLab with Aurora DSQL","text":"<p>This guide provides step-by-step instructions on how to connect and query Amazon Aurora DSQL using JupyterLab with Python. JupyterLab is a popular interactive computing environment that combines code, text, and visualizations in a single document. It's widely used for data science and research applications.</p> <p>The instructions below will cover the basics of Aurora DSQL usage in both a local installation of JupyterLab as well as using Amazon SageMaker AI, a fully-managed machine learning service that provides a hosted environment with a UI for data workflows.</p>"},{"location":"generative-ai/jupyterlab.html#getting-started","title":"Getting started","text":""},{"location":"generative-ai/jupyterlab.html#requirements","title":"Requirements","text":"<ul> <li>An Aurora DSQL cluster</li> <li>AWS credentials configured (local installation only)</li> <li>Python version 3.9 or greater (local installation only)</li> </ul>"},{"location":"generative-ai/jupyterlab.html#using-local-jupyterlab","title":"Using local JupyterLab","text":"<p>To get started with JupyterLab, users must first install the application using Python's pip:</p> <pre><code>pip install jupyterlab\n</code></pre> <p>JupyterLab can then be opened by running <code>jupyter lab</code>. This will open the JupyterLab application at localhost:8888, accessible in a browser. Ensure you have AWS credentials configured in your local environment before proceeding.</p>"},{"location":"generative-ai/jupyterlab.html#using-amazon-sagemaker-ai","title":"Using Amazon SageMaker AI","text":"<p>In the AWS console, proceed to the Amazon SageMaker AI console page and then to the Notebooks section under Applications and IDEs. From there you can select Create notebook instance to begin creating a SageMaker environment. Select an instance type and platform before clicking Create notebook instance.</p> <p>See Amazon SageMaker AI setup documentation for more information on setup and instance options.</p> <p>Note</p> <p>Warning: Using Amazon SageMaker AI may result in charges to your AWS account.</p> <p>Once the SageMaker instance becomes active, you can open it from the Notebook instances section with Open JupyterLab. Before getting started with Aurora DSQL in your notebook you must provide access to your DSQL cluster in the SageMaker instance's IAM role. The simplest way to do so is to follow the link to the IAM role in the notebook instance page. From there you can edit the Policies attached to your SageMaker IAM role. See Authentication and authorization for more information on configuring an IAM policy to allow access to Aurora DSQL.</p>"},{"location":"generative-ai/jupyterlab.html#connecting-to-aurora-dsql-using-jupyterlab","title":"Connecting to Aurora DSQL using JupyterLab","text":"<p>After you have set up a JupyterLab instance, the steps to connect to Aurora DSQL are the same locally and in SageMaker AI. Create an empty Python 3 notebook, in which you can add cells with Python code.</p> <p>In a Python cell, download the Amazon root certificate from the official trust store:</p> <pre><code>import urllib.request\nurllib.request.urlretrieve('https://www.amazontrust.com/repository/AmazonRootCA1.pem', 'root.pem')\n</code></pre> <p>To connect to Aurora DSQL, first install the Aurora DSQL Connector for Python and the Psycopg driver in a Python cell, and then import it:</p> <pre><code>pip install aurora_dsql_python_connector psycopg\n</code></pre> <pre><code>import aurora_dsql_psycopg as dsql\n</code></pre> <p>With the connector imported, you can then create a DSQL configuration and connect. The Aurora DSQL Python Connector will automatically handle creation of an authentication token on each connection.</p> <pre><code>config = {\n    'host': \"your-cluster.dsql.us-east-1.on.aws\",\n    'region': \"us-east-1\",\n    'user': \"admin\"\n}\n\nconn = dsql.connect(**config)\n</code></pre> <p>Upon running your code you should now have a Psycopg connection to Aurora DSQL. You can then run queries using the Psycopg cursor and providing your SQL query. See the Psycopg documentation for more information on using Psycopg with a Postgres-compatible database. This query will result in a list of tuples in <code>results_list</code>.</p> <pre><code>with conn:\n    with conn.cursor() as cur:\n        cur.execute(\"SELECT * FROM table\")\n        results_list = cur.fetchall()\n</code></pre> <p>You can then use Python frameworks like Pandas to analyze or visualize your query results, for example:</p> <pre><code>pip install pandas\n</code></pre> <pre><code>import pandas as pd\n\ndf = pd.DataFrame(tuples_list)\nprint(df)\nprint(f\"Total records: {len(df)}\")\n</code></pre>"},{"location":"generative-ai/jupyterlab.html#example-notebook","title":"Example notebook","text":"<p>A sample notebook using Aurora DSQL is available in the Aurora DSQL samples repository.</p>"},{"location":"generative-ai/jupyterlab.html#further-reading","title":"Further reading","text":"<ul> <li>Amazon SageMaker AI setup documentation</li> <li>Aurora DSQL Connector for Python</li> <li>Pandas documentation</li> </ul>"},{"location":"generative-ai/mcp-server.html","title":"AWS Labs Aurora DSQL MCP Server","text":"\ud83d\udccb Copy Page"},{"location":"generative-ai/mcp-server.html#aws-labs-aurora-dsql-mcp-server","title":"AWS Labs Aurora DSQL MCP Server","text":"<p>An AWS Labs Model Context Protocol (MCP) server for Aurora DSQL.</p>"},{"location":"generative-ai/mcp-server.html#features","title":"Features","text":"<ul> <li>Converting human-readable questions and commands into structured Postgres-compatible SQL queries and executing them against the configured Aurora DSQL database.</li> <li>Read-only by default, transactions enabled with <code>--allow-writes</code></li> <li>Connection reuse between requests for improved performance</li> <li>Built-in access to Aurora DSQL documentation, search, and best practice recommendations</li> </ul>"},{"location":"generative-ai/mcp-server.html#available-tools","title":"Available Tools","text":""},{"location":"generative-ai/mcp-server.html#database-operations","title":"Database Operations","text":"<ul> <li>readonly_query - Execute read-only SQL queries against your DSQL cluster</li> <li>transact - Execute write operations in a transaction (requires <code>--allow-writes</code>)</li> <li>get_schema - Retrieve table schema information</li> </ul>"},{"location":"generative-ai/mcp-server.html#documentation-and-recommendations","title":"Documentation and Recommendations","text":"<ul> <li>dsql_search_documentation - Search Aurora DSQL documentation</li> <li>Parameters: <code>search_phrase</code> (required), <code>limit</code> (optional)</li> <li>dsql_read_documentation - Read specific DSQL documentation pages</li> <li>Parameters: <code>url</code> (required), <code>start_index</code> (optional), <code>max_length</code> (optional)</li> <li>dsql_recommend - Get recommendations for DSQL best practices</li> <li>Parameters: <code>url</code> (required)</li> </ul>"},{"location":"generative-ai/mcp-server.html#prerequisites","title":"Prerequisites","text":"<ol> <li>An AWS account with an Aurora DSQL Cluster</li> <li>This MCP server can only be run locally on the same host as your LLM client.</li> <li>Set up AWS credentials with access to AWS services</li> <li>You need an AWS account with a role including these permissions:<ul> <li><code>dsql:DbConnectAdmin</code> - Connect to DSQL clusters as the admin user</li> <li><code>dsql:DbConnect</code> - Connect to DSQL clusters with custom database roles (only needed if using non-admin users)</li> </ul> </li> <li>Configure AWS credentials with <code>aws configure</code> or environment variables</li> </ol>"},{"location":"generative-ai/mcp-server.html#installation","title":"Installation","text":""},{"location":"generative-ai/mcp-server.html#using-uv","title":"Using <code>uv</code>","text":"<ol> <li>Install <code>uv</code> from Astral or the GitHub README</li> <li>Install Python using <code>uv python install 3.10</code></li> </ol> <p>Configure the MCP server in your MCP client configuration (e.g., for Amazon Q Developer CLI, edit <code>~/.aws/amazonq/mcp.json</code>):</p> <pre><code>{\n  \"mcpServers\": {\n    \"awslabs.aurora-dsql-mcp-server\": {\n      \"command\": \"uvx\",\n      \"args\": [\n        \"awslabs.aurora-dsql-mcp-server@latest\",\n        \"--cluster_endpoint\",\n        \"[your dsql cluster endpoint, e.g. abcdefghijklmnopqrst234567.dsql.us-east-1.on.aws]\",\n        \"--region\",\n        \"[your dsql cluster region, e.g. us-east-1]\",\n        \"--database_user\",\n        \"[your dsql username, e.g. admin]\",\n        \"--profile\",\n        \"default\"\n      ],\n      \"env\": {\n        \"FASTMCP_LOG_LEVEL\": \"ERROR\"\n      },\n      \"disabled\": false,\n      \"autoApprove\": []\n    }\n  }\n}\n</code></pre>"},{"location":"generative-ai/mcp-server.html#windows-installation","title":"Windows Installation","text":"<p>For Windows users, the MCP server configuration format is slightly different:</p> <pre><code>{\n  \"mcpServers\": {\n    \"awslabs.aurora-dsql-mcp-server\": {\n      \"disabled\": false,\n      \"timeout\": 60,\n      \"type\": \"stdio\",\n      \"command\": \"uv\",\n      \"args\": [\n        \"tool\",\n        \"run\",\n        \"--from\",\n        \"awslabs.aurora-dsql-mcp-server@latest\",\n        \"awslabs.aurora-dsql-mcp-server.exe\"\n      ],\n      \"env\": {\n        \"FASTMCP_LOG_LEVEL\": \"ERROR\",\n        \"AWS_PROFILE\": \"your-aws-profile\",\n        \"AWS_REGION\": \"us-east-1\"\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"generative-ai/mcp-server.html#verifying-installation","title":"Verifying Installation","text":"<p>For Amazon Q Developer CLI, run <code>/mcp</code> to see the status of the MCP server.</p>"},{"location":"generative-ai/mcp-server.html#server-configuration-options","title":"Server Configuration Options","text":""},{"location":"generative-ai/mcp-server.html#-allow-writes","title":"<code>--allow-writes</code>","text":"<p>By default, the dsql mcp server does not allow write operations (\"read-only mode\"). Any invocations of transact tool will fail in this mode. To use transact tool, allow writes by passing <code>--allow-writes</code> parameter.</p> <p>We recommend using least-privilege access when connecting to DSQL. For example, users should use a role that is read-only when possible. The read-only mode has a best-effort client-side enforcement to reject mutations.</p>"},{"location":"generative-ai/mcp-server.html#-cluster_endpoint","title":"<code>--cluster_endpoint</code>","text":"<p>This is mandatory parameter to specify the cluster to connect to. This should be the full endpoint of your cluster, e.g., <code>01abc2ldefg3hijklmnopqurstu.dsql.us-east-1.on.aws</code></p>"},{"location":"generative-ai/mcp-server.html#-database_user","title":"<code>--database_user</code>","text":"<p>This is a mandatory parameter to specify the user to connect as. For example <code>admin</code>, or <code>my_user</code>. Note that the AWS credentials you are using must have permission to login as that user. For more information on setting up and using database roles in DSQL, see Using database roles with IAM roles.</p>"},{"location":"generative-ai/mcp-server.html#-profile","title":"<code>--profile</code>","text":"<p>You can specify the aws profile to use for your credentials. Note that this is not supported for docker installation.</p> <p>Using the <code>AWS_PROFILE</code> environment variable in your MCP configuration is also supported:</p> <pre><code>\"env\": {\n  \"AWS_PROFILE\": \"your-aws-profile\"\n}\n</code></pre> <p>If neither is provided, the MCP server defaults to using the \"default\" profile in your AWS configuration file.</p>"},{"location":"generative-ai/mcp-server.html#-region","title":"<code>--region</code>","text":"<p>This is a mandatory parameter to specify the region of your DSQL database.</p>"},{"location":"generative-ai/mcp-server.html#-knowledge-server","title":"<code>--knowledge-server</code>","text":"<p>Optional parameter to specify the remote MCP server endpoint for DSQL knowledge tools (documentation search, reading, and recommendations). By default it is pre-configured.</p> <p>Example:</p> <pre><code>--knowledge-server https://custom-knowledge-server.example.com\n</code></pre> <p>Note: For security, only use trusted knowledge server endpoints. The server should be an HTTPS endpoint.</p>"},{"location":"generative-ai/mcp-server.html#-knowledge-timeout","title":"<code>--knowledge-timeout</code>","text":"<p>Optional parameter to specify the timeout in seconds for requests to the knowledge server.</p> <p>Default: <code>30.0</code></p> <p>Example:</p> <pre><code>--knowledge-timeout 60.0\n</code></pre> <p>Increase this value if you experience timeouts when accessing documentation on slow networks.</p>"},{"location":"generative-ai/query-editor.html","title":"Aurora DSQL Query Editor","text":"\ud83d\udccb Copy Page"},{"location":"generative-ai/query-editor.html#get-started-with-the-aurora-dsql-query-editor","title":"Get started with the Aurora DSQL Query Editor","text":"<p>With the Aurora DSQL Query Editor, you can securely connect to your Aurora DSQL clusters and run SQL queries directly from the AWS Management Console without installing or configuring external clients. It provides an intuitive workspace with built-in syntax highlighting, auto-completion, and intelligent code assistance. You can quickly explore schema objects, develop and execute SQL queries, and view results, all within a single interface.</p> <p>This topic walks you through the steps to connect to a cluster, run queries, view results, and explore advanced capabilities such as execution plans.</p> <p>Note</p> <p>The Query Editor is available in all Regions where Aurora DSQL is supported. For more information, see AWS Regional Services.</p>"},{"location":"generative-ai/query-editor.html#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure that you meet the following requirements:</p> <ul> <li>You have at least one Aurora DSQL cluster available. For more information, see Step 1: Create a Single-Region Cluster.</li> <li>Your cluster endpoint is publicly accessible. The Query Editor does not currently support clusters that have public access blocked by resource-based policies or clusters managed through VPC endpoints. For more information, see Blocking public access with resource-based policies in Aurora DSQL and Managing and connecting to Amazon Aurora DSQL clusters using AWS PrivateLink.</li> <li>Your IAM user or role has the required permissions to access and connect to the cluster. For more information, see Using database roles and IAM authentication.</li> </ul>"},{"location":"generative-ai/query-editor.html#working-with-the-query-editor","title":"Working with the Query Editor","text":""},{"location":"generative-ai/query-editor.html#open-the-query-editor","title":"Open the Query Editor","text":"<p>To open the Query Editor:</p> <ol> <li>Open the Aurora DSQL console.</li> <li>In the navigation pane, choose Query Editor.</li> </ol> <p>Alternatively, from the Clusters page, select the cluster you want to query and choose Connect with Query editor to launch the editor directly.</p> <p>Note</p> <p>Work and connection state are not saved. If you navigate away from the Aurora DSQL console, close the browser tab, or sign out, your connections, query text, and results are lost.</p>"},{"location":"generative-ai/query-editor.html#connect-to-a-cluster","title":"Connect to a cluster","text":"<p>To connect to a cluster:</p> <ol> <li>If no cluster connection exists, the editor displays No cluster has been connected. Choose Connect or select + (Add) in the Cluster Explorer pane to connect to an existing cluster.</li> <li>(Optional) Connect to multiple clusters or to the same cluster using different roles.</li> </ol>"},{"location":"generative-ai/query-editor.html#explore-cluster-objects","title":"Explore cluster objects","text":"<p>The Cluster Explorer displays all available cluster connections and lets you browse objects such as databases, schemas, tables, and views. It also provides common actions like Refresh, Create table, and other context-specific options.</p>"},{"location":"generative-ai/query-editor.html#run-queries","title":"Run queries","text":"<p>To run a query:</p> <ol> <li> <p>In the query editor tab pane, enter your SQL statement. For example:    <pre><code>SELECT * FROM public.orders LIMIT 10;\n</code></pre></p> </li> <li> <p>Verify the Active Cluster Context displayed on the upper right of the query tab. This indicates the cluster connection associated with the current query tab.</p> </li> <li> <p>(Optional) Use the connection dropdown to review all available connections or switch to a different cluster. Changing the connection updates where your queries in that tab are executed.</p> </li> <li> <p>Choose Run to execute the query.</p> </li> </ol> <p>Note</p> <p>Each query can return up to 10,000 rows in the results pane. For larger datasets, refine your query with filters or limits.</p>"},{"location":"generative-ai/query-editor.html#review-results-and-execution-plans","title":"Review results and execution plans","text":"<p>After the query runs, review the output in the Results panel at the bottom of the editor. By default, each query execution displays the Results (Table) tab, showing tabular query output.</p> <p>To get the query execution plan, run <code>EXPLAIN ANALYZE</code> or <code>EXPLAIN ANALYZE VERBOSE</code> to get additional insights into query performance. For more information, see Reading Aurora DSQL EXPLAIN plans.</p> <p>Tip</p> <p>The <code>EXPLAIN ANALYZE VERBOSE</code> command surfaces DPU usage estimates, including Compute, Read, Write, and Total DPU values, providing immediate visibility into the resources consumed by individual SQL statements.</p>"},{"location":"guides/getting-started/quickstart.html","title":"Getting Started","text":"\ud83d\udccb Copy Page"},{"location":"guides/getting-started/quickstart.html#getting-started-with-aurora-dsql","title":"Getting Started with Aurora DSQL","text":"<p>Learn how to create an Aurora DSQL cluster, connect to it, and run your first queries. This guide walks you through creating single-Region and multi-Region Aurora DSQL clusters, connecting to them, and running sample SQL commands using the AWS Console and PostgreSQL-compatible tools.</p>"},{"location":"guides/getting-started/quickstart.html#prerequisites","title":"Prerequisites","text":"<p>Before you begin using Aurora DSQL, ensure you meet the following prerequisites:</p> <ul> <li>Your IAM identity must have permission to sign in to the console</li> <li>Your IAM identity must meet the following criteria:</li> <li>Access to perform any action on any resource in your AWS account</li> <li><code>AmazonAuroraDSQLConsoleFullAccess</code> AWS managed policy is attached</li> </ul>"},{"location":"guides/getting-started/quickstart.html#step-1-create-a-single-region-cluster","title":"Step 1: Create a Single-Region Cluster","text":"<p>The basic unit of Aurora DSQL is the cluster, which is where you store your data. In this step, you create a cluster in a single AWS Region.</p>"},{"location":"guides/getting-started/quickstart.html#create-a-single-region-cluster","title":"Create a single-Region cluster","text":"<ol> <li> <p>Sign in to the AWS Console and open the Aurora DSQL console at https://console.aws.amazon.com/dsql</p> </li> <li> <p>Choose Create cluster and then Single-Region</p> </li> <li> <p>(Optional) Change the value of the default Name tag</p> </li> <li> <p>(Optional) Add additional Tags for this cluster</p> </li> <li> <p>(Optional) In Cluster settings, select any of the following options:</p> </li> <li>Select Customize encryption settings (advanced) to choose or create an AWS KMS key</li> <li>Select Enable deletion protection to prevent a delete operation from removing your cluster. By default, deletion protection is selected</li> <li> <p>Select Resource-based policy (advanced) to specify access control policies for this cluster</p> </li> <li> <p>Choose Create cluster</p> </li> <li> <p>The console returns you to the Clusters page. A notification banner appears indicating that the cluster is being created. Select the Cluster ID to open the cluster details view</p> </li> </ol>"},{"location":"guides/getting-started/quickstart.html#step-2-connect-to-your-cluster","title":"Step 2: Connect to Your Cluster","text":"<p>Aurora DSQL supports multiple ways to connect to your cluster, including the DSQL Query Editor, AWS CloudShell, the local psql client, and other PostgreSQL-compatible tools. In this step, you connect using the Aurora DSQL Query Editor, which provides a quick way to begin interacting with your new cluster.</p>"},{"location":"guides/getting-started/quickstart.html#connect-using-the-query-editor","title":"Connect using the Query Editor","text":"<ol> <li> <p>In the Aurora DSQL Console (https://console.aws.amazon.com/dsql), open the Clusters page and confirm that your cluster creation has completed and its status is Active</p> </li> <li> <p>Select your cluster from the list, or choose the Cluster ID to open the Cluster details page</p> </li> <li> <p>Choose Connect with Query editor</p> </li> <li> <p>Choose Connect as admin for the cluster that was just created</p> </li> <li>Optionally you can connect with a custom role see Using database roles and IAM authentication</li> </ol>"},{"location":"guides/getting-started/quickstart.html#step-3-run-sample-sql-commands","title":"Step 3: Run Sample SQL Commands","text":"<p>Test your Aurora DSQL cluster by running SQL statements. After opening the cluster in the Query Editor, select and run each sample query step by step.</p>"},{"location":"guides/getting-started/quickstart.html#create-a-schema","title":"Create a schema","text":"<p>Create a schema named <code>test</code>:</p> <pre><code>CREATE SCHEMA IF NOT EXISTS test;\n</code></pre>"},{"location":"guides/getting-started/quickstart.html#create-a-table","title":"Create a table","text":"<p>Create a hello_world table that uses an automatically generated UUID as the primary key:</p> <pre><code>CREATE TABLE IF NOT EXISTS test.hello_world (\n    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n    message VARCHAR(255) NOT NULL,\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n);\n</code></pre>"},{"location":"guides/getting-started/quickstart.html#insert-sample-data","title":"Insert sample data","text":"<p>Insert a sample row:</p> <pre><code>INSERT INTO test.hello_world (message)\nVALUES ('Hello, World!');\n</code></pre>"},{"location":"guides/getting-started/quickstart.html#query-the-data","title":"Query the data","text":"<p>Read the inserted values:</p> <pre><code>SELECT * FROM test.hello_world;\n</code></pre>"},{"location":"guides/getting-started/quickstart.html#clean-up-optional","title":"Clean up (Optional)","text":"<p>Optionally clean up the test resources:</p> <pre><code>DROP TABLE test.hello_world;\nDROP SCHEMA test;\n</code></pre>"},{"location":"guides/getting-started/quickstart.html#step-4-optional-create-a-multi-region-cluster","title":"Step 4 (Optional): Create a Multi-Region Cluster","text":"<p>When you create a multi-Region cluster, you specify the following Regions:</p>"},{"location":"guides/getting-started/quickstart.html#remote-region","title":"Remote Region","text":"<p>This is the Region in which you create a second cluster. You create a second cluster in this Region and peer it to your initial cluster. Aurora DSQL replicates all writes on the initial cluster to the remote cluster. You can read and write on any cluster.</p>"},{"location":"guides/getting-started/quickstart.html#witness-region","title":"Witness Region","text":"<p>This Region receives all data that is written to the multi-Region cluster. However, witness Regions don't host client endpoints and don't provide user data access. A limited window of the encrypted transaction log is maintained in witness Regions. This log facilitates recovery and supports transactional quorum if a Region becomes unavailable.</p>"},{"location":"guides/getting-started/quickstart.html#create-a-multi-region-cluster","title":"Create a multi-Region cluster","text":"<p>Use the following procedure to create an initial cluster, create a second cluster in a different Region, and then peer the two clusters to create a multi-Region cluster. It also demonstrates cross-Region write replication and consistent reads from both Regional endpoints.</p> <ol> <li> <p>Sign in to the Aurora DSQL console</p> </li> <li> <p>In the navigation pane, choose Clusters</p> </li> <li> <p>Choose Create cluster and then Multi-Region</p> </li> <li> <p>(Optional) Change the value of the default Name tag</p> </li> <li> <p>(Optional) Add additional Tags for this cluster</p> </li> <li> <p>In Multi-Region settings, choose the following options for your initial cluster:</p> </li> <li>In Witness Region, choose a Region. Currently, only US-based Regions are supported for witness Regions in multi-Region clusters</li> <li> <p>(Optional) In Remote Region cluster ARN, enter an ARN for an existing cluster in another Region. If no cluster exists to serve as the second cluster in your multi-Region cluster, complete setup after you create the initial cluster</p> </li> <li> <p>(Optional) In Cluster settings, select any of the following options for your initial cluster:</p> </li> <li>Select Customize encryption settings (advanced) to choose or create an AWS KMS key</li> <li>Select Enable deletion protection to prevent a delete operation from removing your cluster. By default, deletion protection is selected</li> <li> <p>Select Resource-based policy (advanced) to specify access control policies for this cluster</p> </li> <li> <p>Choose Create cluster to create your initial cluster. If you didn't enter an ARN in the previous step, the console shows the Cluster setup pending notification</p> </li> <li> <p>In the Cluster setup pending notification, choose Complete multi-Region cluster setup. This action initiates creation of a second cluster in another Region</p> </li> <li> <p>Choose one of the following options for your second cluster:</p> <ul> <li>Add remote Region cluster ARN \u2013 Choose this option if a cluster exists, and you want it to be the second cluster in your multi-Region cluster</li> <li>Create cluster in another Region \u2013 Choose this option to create a second cluster. In Remote Region, choose the Region for this second cluster</li> </ul> </li> <li> <p>Choose Create cluster in your-second-region, where <code>your-second-region</code> is the location of your second cluster. The console opens in your second Region</p> </li> <li> <p>(Optional) Choose cluster settings for your second cluster. For example, you can choose an AWS KMS key</p> </li> <li> <p>Choose Create cluster to create your second cluster</p> </li> <li> <p>Choose Peer in initial-cluster-region, where <code>initial-cluster-region</code> is the Region that hosts the first cluster that you created</p> </li> <li> <p>When prompted, choose Confirm. This step completes the creation of your multi-Region cluster</p> </li> </ol>"},{"location":"guides/getting-started/quickstart.html#connect-to-your-second-cluster","title":"Connect to your second cluster","text":"<ol> <li> <p>Open the Aurora DSQL console and choose the Region for your second cluster</p> </li> <li> <p>Choose Clusters</p> </li> <li> <p>Select the row for the second cluster in your multi-Region cluster</p> </li> <li> <p>Choose Connect with Query editor</p> </li> <li> <p>Choose Connect as admin</p> </li> <li> <p>Create a sample schema and table, and insert data by following the steps in Step 3: Run Sample SQL Commands</p> </li> </ol>"},{"location":"guides/getting-started/quickstart.html#query-data-across-regions","title":"Query data across Regions","text":"<p>To query data in the second cluster from the Region hosting your initial cluster:</p> <ol> <li> <p>In the Aurora DSQL console, choose the Region for your initial cluster</p> </li> <li> <p>Choose Clusters</p> </li> <li> <p>Select the row for the second cluster in your multi-Region cluster</p> </li> <li> <p>Choose Connect with Query editor</p> </li> <li> <p>Choose Connect as admin</p> </li> <li> <p>Query the data that you inserted into the second cluster:</p> </li> </ol> <pre><code>SELECT * FROM test.hello_world;\n</code></pre>"},{"location":"guides/getting-started/quickstart.html#troubleshooting","title":"Troubleshooting","text":"<p>See the Troubleshooting section of the Aurora DSQL documentation.</p>"},{"location":"guides/getting-started/quickstart.html#next-steps","title":"Next Steps","text":"<ul> <li>Learn about authentication and authorization</li> <li>Explore programming with Aurora DSQL</li> <li>Understand multi-Region clusters</li> <li>Review security best practices</li> </ul>"}]}